<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-12-14T15:01:17-08:00</updated><id>/feed.xml</id><title type="html">Asher Mancinelli</title><subtitle>Mostly C++ and Compilers</subtitle><entry><title type="html">Debugging Performance in Compilers</title><link href="/comp-debug-perf" rel="alternate" type="text/html" title="Debugging Performance in Compilers" /><published>2022-12-12T00:00:00-08:00</published><updated>2022-12-12T00:00:00-08:00</updated><id>/Compiler-Perf-Debugging</id><content type="html" xml:base="/comp-debug-perf"><![CDATA[<p>Overview of how I debug performance regressions when developing a compiler.
I don‚Äôt claim this is the best way to do it, email me or tweet at me if you‚Äôve got better ideasüòâ</p>

<font size="-1">
  <em>
    These views do not in any way represent those of NVIDIA or any other organization or institution that I am professionally associated with.
    These views are entirely my own.
  </em>
</font>

<h2 id="starting-point">Starting Point</h2>

<p>Compilers are very complicated and the results can be surprising.
Sometimes performance issues only show up in large scale real-world applications.
How do you go about debugging such an issue?</p>

<p>As you might expect, narrowing down the issue to be minimal and reproducible is the first task.
Ideally, we narrow the performance regression down to a single translation unit, though sometimes this isn‚Äôt enough.
For this post, we‚Äôll assume that the bulk of the performance regression you see in your application is coming from one translation unit, and that you know which patch is causing the regression (if you don‚Äôt know which patch is causing the regression‚Ä¶ well you can bisect the recent patches tooüòÅ).</p>

<h2 id="bisecting-the-object-files">Bisecting the Object Files</h2>

<p>Assume we have two compilers: compiler A which doesn‚Äôt have the ‚Äúbad‚Äù changes (the ‚Äúgood‚Äù compiler), and compiler B which does (the ‚Äúbad‚Äù compiler).
We‚Äôll start by building the application with both compilers, building half of the object files with compiler A and half with compiler B.
Say we have 100 object files that are linked into the application; we‚Äôd build the first 50 with compiler A and the second 50 with compiler B.</p>

<p>If the perf regression isn‚Äôt observed after you re-link all the object files into the application, then we know the bulk of the issue is in the object files that were just built with compiler A.
We can then rebuild all the object files in the second 50 with compiler A and build object files 26-50 or 1-25 with compiler B.
In this way, we bisect all the translation units until we find the single TU with the largest impact on performance.</p>

<p>This can be really tedious and manual, but it‚Äôs not too hard to scriptüòâ.</p>

<h2 id="bisecting-the-source-file">Bisecting the Source File</h2>

<p>Now that we‚Äôve narrowed our regression down to a single TU, our work gets a little more complicated.
We can use the same bisection process as before, but this time we‚Äôll have to do it on a single file.
To acomplish this, we‚Äôll have to figure out which parts of the source file depend on each other so we can break it into two new source files, one to be built with compiler A and one to be built with compiler B (all other TUs being built with the ‚Äúgood‚Äù compiler).</p>

<p>Depending on the situation you may create two source files, each with half of the content of the original, or maybe you‚Äôll use the same source file but use macro guards so each compiler only builds half of the source, eg:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* includes, declarations, and global defs up here */</span>

<span class="cp">#ifdef COMPILERA
</span><span class="c1">// stuff built with the good compiler...</span>
<span class="cp">#else </span><span class="cm">/* COMPILERB */</span><span class="cp">
</span><span class="c1">// stuff built with the bad compiler...</span>
<span class="cp">#endif
</span></code></pre></div></div>

<p>You may then add <code class="language-plaintext highlighter-rouge">-DCOMPILERA</code> to the invokation of compiler A so each compiler only builds half of the TU in question.
Again, if we don‚Äôt see the perf regression, we swap the macro guards and try again.
We then have compiler B build a quarter of the original TU and have compiler A build the other 3/4ths, and see if we observe the regression, etc etc.
Ideally, at the end of this process we know exactly which function(s) are causing the regression.</p>

<h2 id="what-next">What Next?</h2>

<p>After we‚Äôve narrowed the regression down to a function or two (ü§û) things can get tricky, and very much depends on the nature of the changes that caused the regression.</p>

<p>At this point I think it‚Äôs best to ask some questions:</p>

<ul>
  <li>Was the patch in question related to a specific pass?
    <ul>
      <li>Can the effects of that pass be seen in the function(s) we found to be causing the regression?</li>
      <li>Is the regression observed when the pass is disabled?</li>
    </ul>
  </li>
  <li>Do you notice any obvious differences between the IR the compilers generate for the identified functions?
    <ul>
      <li>Can you use those differences to work backwards to the code that generated that IR?</li>
    </ul>
  </li>
  <li>If you enable lots of debugging output (like dumping all the <code class="language-plaintext highlighter-rouge">opt</code> pass remarks) and build with compilers A and B and then diff the output, are there any glaring differences? Maybe an earlier change allowed another pass (uninvolved in the patch) to perform some transformations it otherwise would not, or maybe vice-versa.</li>
</ul>

<h2 id="why-might-this-not-work">Why Might This Not Work?</h2>

<p>Sometimes the effects only occur in a short function that is always inlined, in which case you might not find a specific TU or set of functions at the root of the regression; for this reason, you might want to crank the inlining pass down as low as it goes to help you narrow down the issue.
It‚Äôs often best to use the fewest optimizations possible when debugging this sort of thing (so long as you still observe the behavior).
<!--
--></p>]]></content><author><name></name></author><category term="c++, llvm, compilers" /><summary type="html"><![CDATA[Overview of how I debug performance regressions when developing a compiler. I don‚Äôt claim this is the best way to do it, email me or tweet at me if you‚Äôve got better ideasüòâ]]></summary></entry><entry><title type="html">TBAA in LLVM IR</title><link href="/llvm-tbaa" rel="alternate" type="text/html" title="TBAA in LLVM IR" /><published>2022-12-02T00:00:00-08:00</published><updated>2022-12-02T00:00:00-08:00</updated><id>/LLVM-TBAA</id><content type="html" xml:base="/llvm-tbaa"><![CDATA[<p>Overview of type-based alias analysis in LLMV IR.</p>

<font size="-1">
  <em>
    These views do not in any way represent those of NVIDIA or any other organization or institution that I am professionally associated with.
    These views are entirely my own.
  </em>
</font>

<h2 id="llvm-alias-analysis">LLVM Alias Analysis</h2>

<p>Alias analysis answers the question ‚Äúdo these two addresses alias each other?‚Äù with three possible responses.
Two addresses either <code class="language-plaintext highlighter-rouge">MayAlias</code>, <code class="language-plaintext highlighter-rouge">MustAlias</code>, or they do <code class="language-plaintext highlighter-rouge">NotAlias</code> (which is how they are spelled in the LLVM alias analysis api).</p>

<p>Alias analyses can take into account lots of factors (control flow, field analysis (when analyzing structures), etc), but we‚Äôre only talking about <em>type-based</em> alias analysis (TBAA).</p>

<p>TBAA leverages the rules in the type system of the given programming language to prove something about the aliasing relationship between two addresses.</p>

<h2 id="llvm-tbaa-metadata">LLVM TBAA Metadata</h2>

<div class="language-llvm highlighter-rouge"><div class="highlight"><pre class="highlight"><code></code></pre></div></div>

<h2 id="what-is-tbaa">What is TBAA</h2>

<h2 id="links">Links</h2>

<ul>
  <li><a target="blank" href="https://llvm.org/docs/LangRef.html#tbaa-metadata">LLVM TBAA metadata docs</a></li>
  <li><a target="blank" href="https://releases.llvm.org/8.0.0/docs/AliasAnalysis.html">LLVM alias analysis docs</a></li>
  <li><a href="https://www.cs.cornell.edu/courses/cs6120/2020fa/lesson/9/" target="blank">Cornell alias analysis slides</a></li>
  <li><a target="blank" href="https://www.researchgate.net/publication/234027199_A_Formally-Verified_Alias_Analysis">Formally Verified Alias Analysis</a></li>
  <li><a href="https://stefansf.de/c-quiz/" target="blank">Type-Based Alias Analysis in C by Stephan</a></li>
  <li><a href="https://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html" target="blank">LLMV blog: What Every C Programmer Should Know by Chris Lattner</a></li>
</ul>]]></content><author><name></name></author><category term="c++, llvm, compilers" /><summary type="html"><![CDATA[Overview of type-based alias analysis in LLMV IR.]]></summary></entry><entry><title type="html">Provable Optimizations in Coq</title><link href="/prov-opt-coq" rel="alternate" type="text/html" title="Provable Optimizations in Coq" /><published>2022-10-06T00:00:00-07:00</published><updated>2022-10-06T00:00:00-07:00</updated><id>/Provable-Optimizations</id><content type="html" xml:base="/prov-opt-coq"><![CDATA[<p>Proving that optimization passes are correct with the formal verification
assistant Coq.</p>

<font size="-1">
  <em>
    These views do not in any way represent those of NVIDIA or any other organization or institution that I am professionally associated with.
    These views are entirely my own.
  </em>
</font>

<h1 id="what-is-coq">What is Coq?</h1>

<p>It‚Äôs a programming language to help the programmer prove things.</p>]]></content><author><name></name></author><category term="c++, metaprogramming" /><summary type="html"><![CDATA[Proving that optimization passes are correct with the formal verification assistant Coq.]]></summary></entry><entry><title type="html">Slaughterhouse Five</title><link href="/sl5" rel="alternate" type="text/html" title="Slaughterhouse Five" /><published>2022-05-14T00:00:00-07:00</published><updated>2022-05-14T00:00:00-07:00</updated><id>/Slaughterhouse-5</id><content type="html" xml:base="/sl5"><![CDATA[<p>Significant quotes and reflections from reading Slaughterhouse 5 by Kurt Vonnegut.</p>

<h1 id="slaughterhouse-five">Slaughterhouse Five</h1>]]></content><author><name></name></author><summary type="html"><![CDATA[Significant quotes and reflections from reading Slaughterhouse 5 by Kurt Vonnegut.]]></summary></entry><entry><title type="html">Worth Being Wrong</title><link href="/rel-wbw" rel="alternate" type="text/html" title="Worth Being Wrong" /><published>2022-05-14T00:00:00-07:00</published><updated>2022-05-14T00:00:00-07:00</updated><id>/Worth-Being-Wrong</id><content type="html" xml:base="/rel-wbw"><![CDATA[<p>What is worth living for? How much of my life should be spent to figuring out what‚Äôs technically correct when I could be using my time to help the material conditions of my neighbor?</p>

<h1 id="whats-worth-being-wrong-about">What‚Äôs Worth Being Wrong About?</h1>

<p>I was given a set of ideas to see the world through, a toolbox for interacting with ideas and experiences.
This toolbox fell short a long time ago for many reasons which won‚Äôt be enumerated here.</p>

<p>The castle of ideas was built up in my mind over many years, and in a few short years I tore many of those walls down.
Since then, I‚Äôve been rebuilding, reconstructing, tearing down again, and rebuilding some more.
I guess this is just a dialectic - not all that special.</p>

<p>I only have so much time and energy in this life, and I can keep building and tearing down again, or I can lift my eyes to the material conditions of my neighbors.
At some point, sorting out my philosophy became less of a priority.</p>

<p>There must come a point where I‚Äôm comfortable working with the castle I‚Äôve built, otherwise I‚Äôll keep adding a brick here, breaking another there, and one day I‚Äôll look around me and realize I‚Äôm in a half-built castle in a field of half-built castles, and I could have joined my neighbor in their half-built castle years ago.</p>

<blockquote>
  <p>If I know enough to know what I‚Äôm willing to risk being wrong about, what else do I need to figure out?</p>
</blockquote>

<h1 id="a-process-god">A Process God?</h1>

<p>Tripp Fuller has been instrumental in my philisophical and religious development, and is likely a large reason I still consider myself christian.</p>

<p>He gives me hope.</p>

<p>Tyson does too.</p>

<p>There are few of them, but their words go so far.</p>

<p>When I consider the God I encounter in Jesus, I think of the christian god of process theology, at least as I understand it.
If all experience that has ever been experienced were rolled up into one being, what would that being have to say?
Let‚Äôs call that being god.</p>

<blockquote>
  <p>What would God say to you?</p>
</blockquote>

<blockquote>
  <p>If God were to walk among us, who would God become?</p>
</blockquote>

<blockquote>
  <p>Who would that God shape us to be?</p>
</blockquote>

<h1 id="could-he-have-been-wrong">Could He Have Been Wrong?</h1>

<p>John Dominic Crosson‚Äôs picture of Jesus is compelling to me - we can‚Äôt look back at how 1st c people talked about Ceasar and not apply those same rules to Jesus.
If we talk about Ceasar being a god, we can talk about Jesus being God.
If we look at the Romans and consider their notion of Ceasar‚Äôs godhood as mythology, we must apply the same reasoning to Jesus.</p>

<p>This doesn‚Äôt discount the story of Jesus however.</p>

<h2 id="mythology-as-reality">Mythology as Reality</h2>

<p>The myth of Jesus has shaped all of reality to a staggering degree.
Myths shape reality more than factual information, and if that doesn‚Äôt ring true to you, you‚Äôre not paying attention.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[What is worth living for? How much of my life should be spent to figuring out what‚Äôs technically correct when I could be using my time to help the material conditions of my neighbor?]]></summary></entry><entry><title type="html">Common Platform on Abortion Rights</title><link href="/common-plat" rel="alternate" type="text/html" title="Common Platform on Abortion Rights" /><published>2022-05-07T00:00:00-07:00</published><updated>2022-05-07T00:00:00-07:00</updated><id>/Common-Platform</id><content type="html" xml:base="/common-plat"><![CDATA[<p>Reflections on the common ground between several organiztions with respect to the class struggle for abortion rights.</p>

<h2 id="-organized-labor-for-abortion-rights-">üõ† Organized Labor for Abortion Rights ‚úä</h2>

<p>It‚Äôs my impression that the common ground between all involved organizations is as follows:</p>

<blockquote>
  <p>We must organize our labor to win abortion rights and bodily autonomy.</p>
</blockquote>

<p>This involves organized labor in some form (likely a labor union with directly democratic principles) demanding abortion and healthcare rights to their constituency through the withholding of labor and class solidarity.</p>

<h2 id="script-draft">Script Draft</h2>

<p><em>Draft/disorganized thoughts about what I would say at a theoretical rally</em></p>

<h3 id="identity">Identity</h3>

<p>Hello everyone, my name is Asher and I‚Äôm with Boise Mutual Aid.
We‚Äôre a group aiming to support the community based on the principles of autonomy and mutual aid, as our namesake would suggest.</p>

<p>The recent legal developments around abortion oppress all of us uniquely.</p>

<p>The women among us are oppressed by it.</p>

<p>Those with uteruses are uniquely oppressed by it.</p>

<p>People of color, as well as our trans, neurodivergent, and less able-bodied neighbors are all uniquely oppressed by this, all to different degrees and by different means.</p>

<p>I am none of those things, but I‚Äôm here to tell you why it still matters to <em>you</em>.</p>

<h2 id="demonstration-of-common-ground">Demonstration of Common Ground</h2>

<p>The following sections demonstrate the support for this common platform based on the official or unofficial programs of each (potentially) involved organization.</p>

<h3 id="iar">IAR</h3>

<p>The IAR position is as follows:</p>

<blockquote>
  <p>Idaho Abortions Rights is a community of people who are seeking social change around the topic of abortion access in the state of Idaho.</p>

  <p>We believe that all people deserve access to legal, safe abortions including people who are BIPOC LGBTQ+ Disabled and live in rural areas. 
We believe that healthcare is a human right
We believe all people need comprehensive sex education and birth control access.</p>

  <p>Our mission to be a resource to the community and to speak up as an advocate for people seeking legal, safe abortions</p>

  <p>Restrictions on abortions cause death to people and loss of bodily autonomy.</p>

  <p>We know that our diverse perspectives make us stronger and we will continue to host demonstrations to bring this issue to the forefront and into the minds of our Idaho neighbors.</p>
</blockquote>

<p>IAR is a nonprofit, and thus has some restrictions on what positions it is allowed to take on.
IAR shares the position that this struggle for abortion rights is a class struggle, and that organized labor is the most effective strategy currently at our disposal to win abortion rights.</p>

<h3 id="rr">RR</h3>

<p>Point II of the Red Republican‚Äôs Program is as follows[1]:</p>

<blockquote>
  <p>We push for uncompromising commitments to a bill of rights, economic rights, workers‚Äô rights, and the privacy and security of working people today.</p>
</blockquote>

<p>This includes free and universal healthcare (subpoint 1a of [1]).
There is no official statement in their program related to abortion rights, however from discussing this with RR representatives, abortions rights seems to fall directly into their program under 1a of [1].</p>

<h3>üè¥</h3>

<p>One of the first groups to grow out of philosophical anarchism was <em>anarcho-syndicalism</em>, and was particularly popular and influencial in the mid to late 19th century around the time of the First International:</p>

<blockquote>
  <p>Most anarchist collectivists and many communists during the nineteenth century were syndicalists by implication, and this was particularly true of the anarchists in the First International.</p>

  <ul>
    <li>About Anarchism[4], p30</li>
  </ul>
</blockquote>

<p>Syndicalism may be summarized as <em>trade unionism</em>, which when translated to French, reads <em>syndicalisme</em>.</p>

<p>The WSM[6], a more contemporary Irish anarchist organization took the following position in their abortion rights position paper[5]:</p>

<blockquote>
  <p>The WSM stands for people having control over their own bodies: bodily autonomy. Part of that is control over our own reproductive health, including the ability to end a pregnancy if we choose to.</p>
</blockquote>

<h3 id="psl">PSL</h3>

<p>Bullet 2 of the <em>Socialism</em> section of part II of the DSA‚Äôs program[3] reads as follows (emphasis added):</p>

<blockquote>
  <p>It will be a right of every person in the United States to have a job with <strong><em>guaranteed union representation</em></strong> and full social benefits provided by the socialist government, including a pension, <strong><em>health care</em></strong>, workers‚Äô compensation, paid parental and family leave for up to two years, paid sick and disability leave, a minimum of one month‚Äôs paid vacation, and at least 12 paid holidays.</p>
</blockquote>

<p>The PSL‚Äôs petition for the legalization of abortion rights reads as follows[2]:</p>

<blockquote>
  <p>Rich women have always had access to abortion; it is poor and working class women who stand to lose this fundamental right‚Ä¶ 
Abortion bans are an attack on the rights of working class women‚Ä¶ History shows that progressive legislation is won through struggle. We need a miltant national women‚Äôs rights movement.</p>
</blockquote>

<p>Although the <em>national</em> component of the labor struggle for abortion rights does not fall into the common platform established by this document, the PSL official does support <em>a</em> labor struggle for abortion rights.
The labor struggle for abortion rights in this document is not specifically national or local or non-heirarchical for this reason.</p>

<h2 id="references">References</h2>

<ol>
  <li><a href="https://www.redrepublicans.org/our-program" target="blank">Red Republicans National Program</a></li>
  <li><a href="https://www2.pslweb.org/abortion_rights_petition" target="blank">PSL Petition for the Legalization of Abortion Rights</a></li>
  <li><a href="https://pslweb.org/program/" target="blank">PSL Program</a></li>
  <li><em>About Anarchism</em> by Nicolas Walter</li>
  <li><a href="https://theanarchistlibrary.org/library/workers-solidarity-movement-abortion-rights-position-paper" target="blank"> Abortion Rights: A Workers Solidarity Movement Position Paper </a></li>
  <li><a href="http://www.wsm.ie/" target="blank">Workers Solidarity Movement</a></li>
</ol>]]></content><author><name></name></author><summary type="html"><![CDATA[Reflections on the common ground between several organiztions with respect to the class struggle for abortion rights.]]></summary></entry><entry><title type="html">Authority and Organization</title><link href="/pol-auth-org" rel="alternate" type="text/html" title="Authority and Organization" /><published>2022-05-05T00:00:00-07:00</published><updated>2022-05-05T00:00:00-07:00</updated><id>/Authority-and-Organization</id><content type="html" xml:base="/pol-auth-org"><![CDATA[<!--
pol: true
-->

<p>Reflections on authority, particularly contrasting implicit and explicit authority.</p>

<h2 id="-setting-the-stage">üçΩ Setting the Stage</h2>

<p>Let us consider <em>Organization X</em>, a mutual aid collective and with the goal of improving the material conditions of the working class in their context.
The primary form of organization is through informal and unofficial communication between members, inspired by 19th century Anarchism, particularly Mutualism, Bakuninists, and the First International.</p>

<p>X may reflect organisational beliefs such as:</p>

<blockquote>
  <p>Instead of relying on the state, society should be organised by individuals entering into voluntary agreements with each other on a basis of equality and reciprocity.</p>

  <ul>
    <li>Nicolas Walter, <em>About Anarchism</em>[6], p25</li>
  </ul>
</blockquote>

<p>As expanded upon in libcom.org‚Äôs article on <em>Anarcho-Communism</em>[7], this mix of beliefs may be summarized as <em>the free association of producers and consumers</em>.</p>

<p>The key difference between X and the visions laid out by Bakunin, Malatesta, and their colleagues in the First International is that X <strong><em>does not have capacity for collective decision making</em></strong>.</p>

<p>A key feature of this organization is that many decisions are made in subgroups of X and disseminated ad-hoc to the rest of the members.
These subgroups are unelected and not subject to application nor accountability.</p>

<h2 id="-through-a-cybernetic-lens">üëì Through a Cybernetic Lens</h2>

<p>In examining the prevalence of implicit authority and decision making through the lens of Stafford Beer‚Äôs Viable System Model (VSM), I came away with one key insight:</p>

<blockquote>
  <p>Absence of explicit decision-making is not the <strong><em>absence of response</em></strong> to environmental stimuli, but the absence of a <strong><em>coordinated</em></strong> and <strong><em>effective</em></strong> response.</p>
</blockquote>

<center>
<img src="/images/politics/stafford-beer-vsm.jpeg" alt="Depiction of Stafford Beer's Viable System Model" />
</center>

<h3 id="-problem-formulation">üîé Problem Formulation</h3>

<p>For any organization to perpetuate its own existence, it must react to its environment.
By virtue of X‚Äôs continued existence, it is, by definition, responding to its environment in some manner.
Should there be no collectively agreed-upon actions in response to stimuli, by Ashby‚Äôs Law of Requisite Variety[10], a variety of responses equivalent to the stimuli are <em>still produced by the organization</em>.
Organization X, being relatively large, has a large degree of incoming stimuli and therefor a large degree of response stimuli.
Where, then, do the variety of responses come from if not from an official stance (whether voted on or handed down from leadership)?</p>

<p>The only source of response variety is the individual facing the variety.</p>

<p>Say a member of X is approached by a representative of organization Y, whose platform has about a 50% overlap with the (implicit) platform of X.
Members of Y would like to collaborate with members of X on a particular event.
This introduces environmental variety to the viable system of organization X.
The system 1 of X is now functionally the individual member approached by the member of Y, as this member is the only one able to respond to stimuli, given that voting and collective decision-making are off the table.
Because there is no organizational capacity to perform operations of systems 2 and 3, the individual must also take on these functions and respond according to their own perceptions of organization Y.</p>

<p>System 3 coordinates between systems 1 and 2 and directions reactions to the environment.
In this case, the individual member of X is performing operations of systems 1, 2, and 3 for organization X whether other members of X would support or consent to those actions if they knew about them.
This introduces a sort of tyranny over all members of X - they are cut off from any decision-making about the direction of X.
Over time, many of these small interactions accumulate to form the long-term personality and character of X, meaning that individual members of X are performing system 4 and 5 operations as well - all without the consent and support of the majority of the members of X.</p>

<h3 id="-subgroups">üôã Subgroups</h3>

<p>Another drawback of the decision-making power of the unelected subgroups of X is the disconnect between the entirety of the membership of X and the subgroups, which are able to perform functions of systems 3-5.
Because the subgroups are unofficial and not accountable to the entirety of the membership of X, they cannot effectively take into account the stimuli being fed into X by the environment.</p>

<p>The members of X act as systems 1 whenever they perform mutual aid or talk with non-members: they intake stimuli from the environment and elicit some response.
However because the subgroups are making decisions as systems 3 without direct input from the various systems 1 (the members), organization X is <em>provably</em> unable to make informed decisions in response to the environment - they are not accounting for all of the input variety, and therefore are not responding in like variety.</p>

<h3 id="-collective-decision-making">üè´ Collective Decision-Making</h3>

<p>Without a mechanism for X to make decisions about its members behavior and the organization‚Äôs platform, total control of the organization is given not to the individuals the comprise organization, but to the circumstances in which members of X are presented with environmental stimuli.
In those circumstances, without the consent or support of the organization, functions of systems 1 through 5 are all performed - the character of X is left to chance.</p>

<p>Let the reader note that this is not in the spirit of the 19th century political theorists that inspire X - in the free association of producers and consumers, the most common organization of decision making is by direct democracy and occasional representation subject to instant recall:</p>

<blockquote>
  <p>‚Ä¶[non anarchists] are sure that anarchy means order which arises spontaneously and that anarchists do not want organisation.
This is the reverse of the truth.
Anarchists actually want much more organisation, though organisation without authority‚Ä¶
A moment‚Äôs thought will show that when compulsion is replaced by consent there will hav eto be more discussion and planning, not less.
Everyone who is involved in a decision will be able to take part in making it‚Ä¶ every decision will be made afresh.</p>

  <ul>
    <li>Nicolas Walter, <em>About Anarchism</em>[6], p12</li>
  </ul>
</blockquote>

<p>It is intrinsic to the spirit of anarchism that members of an organization partake consensually, something not possible in X the way it is presently structured.
The history of anarchist organization is rich with examples of communes, collectives, councils, and other structures.
The common thread is consensual participation and individual representation.</p>

<p>What historical examples can be applied to X?</p>

<h2 id="-differences-between-x-and-successful-predecessors">üìñ Differences Between X and Successful Predecessors</h2>

<p>Organizational strategies of some societies in which 19th century Anarchism was most successfully applied are contrasted with that of organization X below.</p>

<h3 id="rojava">Rojava</h3>

<p>In Rojava, citizens are reprented directly in neighborhood councils, and vote for instantly-recallable representatives to attend city and regional councils to form policy in larger health, defence, economic, and education committees (<em>The Communes of Rojava</em>[12] at 1:30).</p>

<p>The smallest unit of democracy is direct, and higher-level policy is formed through representative democracy.</p>

<h3 id="spanish-revolution">Spanish Revolution</h3>

<h3 id="paris-commune">Paris Commune</h3>

<h2 id="references-and-links">References and Links</h2>

<ol>
  <li><a href="https://www.researchgate.net/publication/318993399_The_Ladder_of_Cyber-Subsidiarity_as_a_Mediation_between_the_Autonomous_Citizens_and_the_Commons" target="blank"> The Ladder of Cyber-Subsidiarity as a Mediation between the Autonomous Citizens and the Commons by Jos√© Mar√≠a D√≠az Nafr√≠a </a></li>
  <li><a href="https://www.researchgate.net/publication/200025911_The_Brain_of_the_Firm" target="blank">Brain of the Firm by Stafford Beer</a></li>
  <li><a href="https://plato.stanford.edu/entries/anarchism/#PoliAnar" target="blank">Anarchism entry in Stanford Encycolpedia of Philosophy</a></li>
  <li><a href="https://www.marxists.org/archive/lenin/works/1917/apr/09.htm" target="blank">The Dual Power by Lenin</a></li>
  <li><a href="https://libcom.org/article/1936-1939-spanish-civil-war-and-revolution" target="blank">libcom.org: The Spanish Civil War and Revolution</a></li>
  <li><em>About Anarchism</em> by Nicolas Walter</li>
  <li><a href="https://libcom.org/library/anarcho-communism" target="blank">libcom.org: Anarcho-Communism</a></li>
  <li><a href="https://isreview.org/issue/108/examing-revolution-rojava/index.html" target="blank">Examing the revolution in Rojava</a></li>
  <li><a href="http://new-compass.net/articles/rojavas-communes-and-councils" target="blank">Rojava‚Äôs Communes and Councils</a></li>
  <li><a href="https://www.businessballs.com/strategy-innovation/ashbys-law-of-requisite-variety/" target="blank">Ashby‚Äôs Law of Requisite Variety</a></li>
  <li><a href="https://www.marxists.org/archive/marx/works/1872/10/authority.htm" target="blank"><em>On Authority</em> by Karl Marx</a></li>
  <li><a href="https://www.youtube.com/watch?v=cDnenjIdnnE&amp;ab_channel=NeighborDemocracy" target="blank">The Communes of Rojava: A Model In Societal Self Direction</a></li>
</ol>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">BQN and Reflections on the Joy of Programming</title><link href="/bqn-reflections" rel="alternate" type="text/html" title="BQN and Reflections on the Joy of Programming" /><published>2022-05-02T00:00:00-07:00</published><updated>2022-05-02T00:00:00-07:00</updated><id>/BQN-reflections</id><content type="html" xml:base="/bqn-reflections"><![CDATA[<p>Solve a leetcode problem in BQN and I rant about the joy of programming.</p>

<h2 id="leetcode">Leetcode</h2>

<p><a href="https://leetcode.com/problems/set-matrix-zeroes" target="blank">
  The Leetcode problem is ‚ÄúSet Matrix Zeroes‚Äù
</a>
where we‚Äôre tasked with setting rows and columns of a matrix that contain zero to be all zeroes.</p>

<h2 id="bqn-solution">BQN Solution</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   i‚Üê‚ü®
     3‚Äø4‚•ä‚ü®0,1,3,0,3,4,5,2,1,3,1,5‚ü©
     3‚Äø3‚•ä‚ü®1,1,1,1,0,1,1,1,1‚ü©
   ‚ü©

   Z ‚Üê {
     bm‚Üê0=ùï©
     a‚Üê‚à®` ‚à®`‚åæ‚åΩ bm
     b‚Üê(‚à®`Àò) ((‚à®`Àò)‚åæ(‚åΩÀò)) bm
     ùï©√óa¬¨‚àò‚à®b
   }
   
   ‚ü®"#1","#2"‚ü©‚àæi‚âçZ¬®i
‚îå‚îÄ                       
‚ïµ "#1"        "#2"       
  ‚îå‚îÄ          ‚îå‚îÄ         
  ‚ïµ 0 1 3 0   ‚ïµ 1 1 1    
    3 4 5 2     1 0 1    
    1 3 1 5     1 1 1    
            ‚îò         ‚îò  
  ‚îå‚îÄ          ‚îå‚îÄ         
  ‚ïµ 0 0 0 0   ‚ïµ 1 0 1    
    0 4 5 0     0 0 0    
    0 3 1 0     1 0 1    
            ‚îò         ‚îò  
                        ‚îò
</code></pre></div></div>

<p>Some other solutions from the BQN Matrix chat room:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   ‚ä¢√ó0‚â†‚àßÀùÀò‚àß‚åú‚àßÀù           # Marshall &amp; Dzaima (tacit!)
   (‚â†‚•ä‚àß¬¥)Àò{ùï©√ó(ùîΩ‚åæ‚çâ‚àßùîΩ)0‚â†ùï©} # Dzaima &amp; Rampoina
   {ùï©√ó(‚àßÀùÀò‚àß‚â¢‚•ä‚àßÀù)0‚â†ùï©}     # Dzaima
</code></pre></div></div>

<h2 id="on-the-joy-of-programming">On the Joy of Programming</h2>

<p>It‚Äôs been a few months since I‚Äôve written BQN or APL, so I feel like I‚Äôm looking at the language family with fresh eyes.</p>

<p>I was struck by the resemblance between solving this leetcode problem and creating art:</p>

<ol>
  <li>I know I‚Äôm not the best at either, and many, <em>many</em> people can write more elegant BQN and more elegant poetry than I can (for example)</li>
  <li>I can thoroughly enjoy both when detached from any performance metrics - the process is far more valuable to me than the end-product</li>
  <li>both can be deeply social actions - sharing your painting with someone and discussing my solution with the BQN chat room are both social and exciting. Even if someone comes back with a more wonderful painting or more terse solution, I enjoy the social interaction just as much.</li>
</ol>

<p>I stumbled upon this thread on twitter describing how Kurt Vonnegut responded to a letter from a high school English student asking for life advice.
In short, his response was to do art and enjoy the process of becoming who you are.</p>

<center>
  <blockquote class="twitter-tweet"><p lang="en" dir="ltr">Tear it up into teeny-weeny pieces, and discard them into widely separated trash receptacles. You will find that you have already been gloriously rewarded for your poem. You have experienced becoming, learned a lot more about what‚Äôs inside you, and you have made your soul grow.</p>&mdash; Gabe Hudson (@gabehudson) <a href="https://twitter.com/gabehudson/status/1521139749322477569?ref_src=twsrc%5Etfw">May 2, 2022</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

<p>Creating art seems to be central to the importance of life as far as I can tell.</p>

<p><a href="https://www.arraycast.com/episodes/episode26-stevan-apter" target="blank">
  The most recent episode of ArrayCast with Stevan Apter dipped into this as well when the panelists discussed the aesthetic of writing APL.
</a>
In some ways they were a little reserved about saying they enjoy APL at least in part due to the aesthetic of the language.
I don‚Äôt think this is something to shy away from - if we can‚Äôt appreciate the beauty of what we do, why are we doing it at all?</p>

<p>I loved working through this rather simple problem.</p>

<p>I loved the process of visualizing the inputs, of thinking through possible solutions while going about my day.</p>

<p>I loved taking my solution to the BQN forum for more gifted and practiced BQN-ers to find far simpler and more elegant solutions than mine.</p>

<p>The whole process felt like writing a poem, and at the end I‚Äôm rewarded by sharing this poem with others, seeing what they come up with, and comparing their thoughts with mine.</p>

<p>There is a unique joy and beauty I find in BQN (and APL more broadly), and that‚Äôs what keeps me coming back.</p>

<p>As Kurt Vonnegut pointed out, what else could be a more worthwhile way to spend my time?</p>

<p>Please, give it a try, and fall in love with the community while you‚Äôre at it.</p>

<!---
## C++ Solution

I'll also include my C++ solution for kicks and giggles:

```cpp
void setZeroes(vector<vector<int>>& m) {
  const auto rs = m.size(), cs = m[0].size();
  vector<int> rows, cols;
  for (int i=0; i<rs; i++)
    for (int j=0; j<cs; j++)
      if (0 == m[i][j]) {
        rows.push_back(i);
        cols.push_back(j);
      }
  for (const auto r : rows)
    std::fill(m[r].begin(), m[r].end(), 0);
  for (const auto c : cols)
    for (auto& r : m)
      r[c] = 0;
}
```
-->]]></content><author><name></name></author><category term="BQN, c++" /><summary type="html"><![CDATA[Solve a leetcode problem in BQN and I rant about the joy of programming.]]></summary></entry><entry><title type="html">JAABIBP (Just Another ABI Blog Post)</title><link href="/jaabibp" rel="alternate" type="text/html" title="JAABIBP (Just Another ABI Blog Post)" /><published>2022-03-14T00:00:00-07:00</published><updated>2022-03-14T00:00:00-07:00</updated><id>/JAABIBP</id><content type="html" xml:base="/jaabibp"><![CDATA[<p>ABI breakage is a hot topic. Let‚Äôs look at some ways C++ handles it, and how that compares to the WG14 <code class="language-plaintext highlighter-rouge">_Alias</code> proposal.</p>

<p><em>NOTE: This is probably the least educated article about ABI breakage yet. You should really watch <a href="https://youtu.be/By7b19YIv8Q">Jason Turner‚Äôs youtube video</a> or read <a href="https://thephd.dev/to-save-c-we-must-save-abi-fixing-c-function-abi">JeanHeyd Meneide‚Äôs blog posts</a>üôÉ</em></p>

<font size="-1">
  <em>
    These views do not in any way represent those of NVIDIA or any other organization or institution that I am professionally associated with.
    These views are entirely my own.
  </em>
</font>

<p>After reading about transparent aliasing in <a href="https://thephd.dev/to-save-c-we-must-save-abi-fixing-c-function-abi">this blog post from JeanHeyd Meneide</a>, I had to play around with it in Godbolt and rave about its coolness on the Cursed Bird Site.
Sean Parent rightly pointed out that this super neat proposal from JeanHeyd acomplishes pretty much the same thing as <code class="language-plaintext highlighter-rouge">inline namespace</code> in C++.</p>

<p><br /></p>

<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Isn't this what inline namespaces are for in C++? (prior, they were \_\_strong namespaces). I'm surprised these aren't even mentioned in the proposal. Maybe an abbreviated form of inline namespaces for C could be used. ;-)</p>&mdash; Sean Parent (@SeanParent) <a href="https://twitter.com/SeanParent/status/1503471201833738240?ref_src=twsrc%5Etfw">March 14, 2022</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

<p><br /></p>

<p>Let‚Äôs talk about that.</p>

<h2 id="problem-formulation">Problem Formulation</h2>

<p>Let‚Äôs say you build executable A which dynamically links against library B.</p>

<p>Library B might look like this:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// B.hpp</span>
<span class="k">namespace</span> <span class="n">B</span> <span class="p">{</span>
<span class="kt">int</span> <span class="n">answer</span><span class="p">();</span>
<span class="p">}</span>

<span class="c1">// B.cpp</span>
<span class="cp">#include</span> <span class="cpf">&lt;B.hpp&gt;</span><span class="cp">
</span><span class="kt">int</span> <span class="n">B</span><span class="o">::</span><span class="n">answer</span><span class="p">()</span> <span class="p">{</span>
  <span class="k">return</span> <span class="mi">42</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>while executable A might look like this:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;B.hpp&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">B</span><span class="o">::</span><span class="n">answer</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">;</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p><br /></p>

<p>Some time has passed since library B was released, and now the authors have decided that <code class="language-plaintext highlighter-rouge">answer</code> can be 30% faster if it uses <code class="language-plaintext highlighter-rouge">long double</code> instead of <code class="language-plaintext highlighter-rouge">int</code>s.
How cool!</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// B.hpp</span>
<span class="k">namespace</span> <span class="n">B</span> <span class="p">{</span>
<span class="kt">long</span> <span class="kt">double</span> <span class="n">answer</span><span class="p">();</span>
<span class="p">}</span>

<span class="c1">// B.cpp</span>
<span class="cp">#include</span> <span class="cpf">&lt;B.hpp&gt;</span><span class="cp">
</span><span class="kt">long</span> <span class="kt">double</span> <span class="n">B</span><span class="o">::</span><span class="n">answer</span><span class="p">()</span> <span class="p">{</span>
  <span class="k">return</span> <span class="mi">42</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Wow, so fast! üöÄ</p>

<p>If you rebuild B without rebuilding A however, A will be expecting the <code class="language-plaintext highlighter-rouge">answer</code>‚Äôs return value to be an int (4 bytes on my system) even though <code class="language-plaintext highlighter-rouge">B::answer</code> now returns a <code class="language-plaintext highlighter-rouge">long double</code> (8 bytes on my system).
When dynamically linking to the original B library, A unsurprisingly prints <code class="language-plaintext highlighter-rouge">42</code>.
When dynamically linking to the updated B library however, A prints the following:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>./A
<span class="go">83575344
</span><span class="gp">$</span><span class="w"> </span><span class="c"># üò® uh oh...</span>
</code></pre></div></div>

<p>This disagreement between the program and the library at the binary level wreaks all sorts of havoc.</p>

<p><a href="https://thephd.dev/binary-banshees-digital-demons-abi-c-c++-help-me-god-please#abi-even-simpler">This section of JeanHeyd‚Äôs post gives a much better illustration.</a></p>

<h2 id="comparing-inline-namespace-with-_alias-and-co">Comparing <code class="language-plaintext highlighter-rouge">inline namespace</code> with <code class="language-plaintext highlighter-rouge">_Alias</code> and co</h2>

<h2 id="why-do-some-want-to-break-it">Why do some want to break it?</h2>

<blockquote>
  <p>Q: Why don‚Äôt you just rebuild after an ABI change?
A1: Are you rebuilding the standard library too?
Many people will recommend not passing standard library types around, and not throwing exceptions across shared library boundaries. They often forget that at least one very commonly used shared library does exactly that‚Ä¶ your C++ standard library.</p>

  <p>On many platforms, there is usually a system C++ standard library. If you want to use that, then you need to deal with standard library types and exceptions going across shared library boundaries. If OS version N+1 breaks ABI in the system C++ standard library, the program you shipped and tested with for OS version N will not work on the upgraded OS until you rebuild.</p>
</blockquote>

<h2 id="links">Links</h2>

<ol>
  <li><a href="https://thephd.dev/binary-banshees-digital-demons-abi-c-c++-help-me-god-please">Binary Banshees and Digital Demons (JeanHeyd Meneide)</a></li>
  <li><a href="https://thephd.dev/to-save-c-we-must-save-abi-fixing-c-function-abi">To Save C, We Must Save ABI (JeanHeyd Meneide)</a></li>
  <li><a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2020/p2028r0.pdf">Titus Winters paper on ABI</a></li>
  <li><a href="https://www.reddit.com/r/cpp/comments/fc2qqv/abi_breaks_not_just_about_rebuilding/">Ben Craig‚Äôs Reddit post on ABI breakage</a></li>
  <li><a href="https://www.reddit.com/r/cpp/comments/fc2qqv/abi_breaks_not_just_about_rebuilding/fj9dfg1/">Johnathan Wakely‚Äôs comment about ABI on Reddit</a></li>
  <li><a href="https://cor3ntin.github.io/posts/abi/">Corentin‚Äôs blog post on ABI</a></li>
  <li><a href="https://www.youtube.com/watch?v=By7b19YIv8Q&amp;ab_channel=C%E1%90%A9%E1%90%A9WeeklyWithJasonTurner">C++ Weekly - Ep 270 - Break ABI to Save C++</a></li>
</ol>]]></content><author><name></name></author><category term="c, c++" /><summary type="html"><![CDATA[ABI breakage is a hot topic. Let‚Äôs look at some ways C++ handles it, and how that compares to the WG14 _Alias proposal.]]></summary></entry><entry><title type="html">CUDA 101: Matrix-Vector Product</title><link href="/cuda-matvec" rel="alternate" type="text/html" title="CUDA 101: Matrix-Vector Product" /><published>2022-02-10T00:00:00-08:00</published><updated>2022-02-10T00:00:00-08:00</updated><id>/CUDA-101-Matvec</id><content type="html" xml:base="/cuda-matvec"><![CDATA[<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>

<script async="" src="https://unpkg.com/mermaid@8.2.3/dist/mermaid.min.js"></script>

<!---

  To use:
  <div class="mermaid">
    graph LR
      ...
  </div>

  --->

<p>The foundation of GPU programming is linear algebra.
This post takes you from a basic linear algebra example problem to a GPU-accelerated example that calculates a matrix-vector product.</p>

<font size="-1">
  <em>
    These views do not in any way represent those of NVIDIA or any other organization or institution that I am professionally associated with.
    These views are entirely my own.
  </em>
</font>

<h1 id="key-takeaways">Key Takeaways</h1>

<ol>
  <li>Correctness precedes parallelism and performance</li>
  <li>Identify and understand the underlying algorithms at play</li>
  <li>Speed is not the same as efficiency</li>
  <li>Use sane defaults, only optimize after profiling and testing</li>
  <li>Use the most specialized tool for the job</li>
</ol>

<p><strong><em>NOTE: This post is geared towards those without significant experience in linear algebra, high performance computing, or GPU programming.</em></strong></p>

<h1 id="outline">Outline</h1>

<ol>
  <li>Mathematical Understanding of Algorithm</li>
  <li>Algorithm Analysis</li>
  <li>C on the Host</li>
  <li>CUDA C
    <ol>
      <li>Core Concepts
        <ol>
          <li>Host and Device</li>
          <li>Using the CUDA Runtime</li>
          <li>Shared Memory</li>
          <li>CUDA Mat-Vec Multiply</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>CUDA C++
    <ol>
      <li>Naive Approach</li>
      <li>More Sophisticated Approach</li>
      <li>The Best Tool for the Job</li>
    </ol>
  </li>
  <li>Conclusion</li>
  <li>BQN Example</li>
  <li>Links, References, Additional Reading</li>
</ol>

<h1 id="mathematical-understanding-of-algorithm">Mathematical Understanding of Algorithm</h1>

<p>We‚Äôll be performing a matrix-vector dot product several ways in this post.</p>

<p>The operation is depicted below:</p>

<center>
<img src="/images/hpc-101-matvec/matvec.png" alt="Matvec dot product, credit this post: https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.2-Multiplying-Matrices-and-Vectors/" />
</center>

<p>Let <code class="language-plaintext highlighter-rouge">p</code> be the result of the dot product of matrix <code class="language-plaintext highlighter-rouge">Mat</code> and vector <code class="language-plaintext highlighter-rouge">v</code>.
The dot product is calculated like so:</p>

<center>
\$\$
\\
  p \gets Mat \cdot v

  \\

  =

  v_0 \cdot
  \left[ {\begin{array}{c}
    Mat_{0, 0} \\
    Mat_{1, 0} \\
    Mat_{2, 0} \\
  \end{array} } \right]
  +
  v_1 \cdot
  \left[ {\begin{array}{c}
    Mat_{0, 1} \\
    Mat_{1, 1} \\
    Mat_{2, 1} \\
  \end{array} } \right]
  +
  v_2 \cdot
  \left[ {\begin{array}{c}
    Mat_{0, 2} \\
    Mat_{1, 2} \\
    Mat_{2, 2} \\
  \end{array} } \right]

  \\

  =

  \left[ {\begin{array}{cc}
    (Mat_{0,0} \cdot v_0) + (Mat_{0,1} \cdot v_1) + (Mat_{0,2} \cdot v_2) \\
    (Mat_{1,0} \cdot v_0) + (Mat_{1,1} \cdot v_1) + (Mat_{1,2} \cdot v_2) \\
    (Mat_{2,0} \cdot v_0) + (Mat_{2,1} \cdot v_1) + (Mat_{2,2} \cdot v_2) \\
  \end{array} } \right]
\\
\$\$
<!---
  =   \left[ {\begin{array}{cc}
    6 \\ 24 \\ 42
  \end{array} } \right]
  --->
</center>

<p>Notice how values of <code class="language-plaintext highlighter-rouge">v</code> are broadcast to match the shape of <code class="language-plaintext highlighter-rouge">Mat</code>:</p>

<center>
\$\$
\\
  \left[ {\begin{array}{c}
    v_{0} &amp; v_{1} &amp; \cdots &amp; v_{n}\\
    v_{0} &amp; v_{1} &amp; \cdots &amp; v_{n}\\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    v_{0} &amp; v_{1} &amp; \cdots &amp; v_{n}\\
  \end{array} } \right]
\\
\$\$
</center>

<p>We can broadcast values of <code class="language-plaintext highlighter-rouge">v</code> into columns of a matrix with the same shape as the matrix <code class="language-plaintext highlighter-rouge">Mat</code>, and then pair the <code class="language-plaintext highlighter-rouge">Mat</code> and <code class="language-plaintext highlighter-rouge">v</code> element-wise, creating a matrix of tuples (or a 3d matrix if you prefer):</p>

<center>
\$\$
\\
  tuplespace \gets
  \left[ {\begin{array}{cc}
    (Mat_{0,0}, v_0) &amp; (Mat_{0,1}, v_1) &amp; (Mat_{0,2}, v_2) \\
    (Mat_{1,0}, v_0) &amp; (Mat_{1,1}, v_1) &amp; (Mat_{1,2}, v_2) \\
    (Mat_{2,0}, v_0) &amp; (Mat_{2,1}, v_1) &amp; (Mat_{2,2}, v_2) \\
  \end{array} } \right]
\\
\$\$
</center>

<p>This is sometimes called a <em>tuple space</em>, or the <em>domain</em> of our algorithm.
The book <a href="https://www.worldcat.org/title/how-to-write-parallel-programs-a-first-course/oclc/912171709&amp;referer=brief_results" target="blank"><em>How to Write Parallel Programs: A First Course</em></a> covers tuple spaces in great detail.</p>

<p>Now that we have constructed our tuple space, we might group our computations into self-contained units of work along each row.</p>

<p>Let <em>tuplespace</em> be the 2 dimensional matrix tuple space given above.
We then may form a vector with units of work yielding indices of the output vector:</p>

<center>
\$\$
\\
  \left[ {\begin{array}{cccc}
    w(0) \gets \sum_{i \gets 0}^{N} tuplespace_{0, i, 0} \cdot tuplespace_{0, i, 1} \\
    w(1) \gets \sum_{i \gets 0}^{N} tuplespace_{1, i, 0} \cdot tuplespace_{1, i, 1} \\
    \vdots \\
    w(M) \gets \sum_{i \gets 0}^{N} tuplespace_{M, i, 0} \cdot tuplespace_{M, i, 1} \\
  \end{array} } \right]
\\
\$\$
</center>

<p>Equivalently:</p>

<center>
\$\$
\\
  \left[ {\begin{array}{cccc}
    w(0) \gets \sum_{i \gets 0}^{N} Mat_{0,i} \cdot v_{i} \\
    w(1) \gets \sum_{i \gets 0}^{N} Mat_{1,i} \cdot v_{i} \\
    \vdots \\
    w(M) \gets \sum_{i \gets 0}^{N} Mat_{M,i} \cdot v_{i} \\
  \end{array} } \right]
\\
\$\$
</center>

<p>Our units of work may independently operate on subsets (rows) of our tuple space.</p>

<h1 id="algorithm-analysis">Algorithm Analysis</h1>

<p>The first question we must ask ourselves when parallelizing code is this: <em>are any iterations of the algorithm dependent on values calculated in other iterations? Is iteration <code class="language-plaintext highlighter-rouge">N</code> dependent on calculations in iteration <code class="language-plaintext highlighter-rouge">N-1</code>?</em>
In other words, <em>are the loop bodies entirely</em> <strong><em>independent</em></strong> <em>of each other?</em></p>

<p>If so, our algorithm is <em>loop independent</em> and <em>trivially parallelizable</em>.
<a href="https://www.cs.utexas.edu/~lin/cs380c/handout27.pdf" target="blank">This slidedeck from a UT Austin lecture</a> are helpful additional reading on this topic.</p>

<p>The fundamental algorithm at play here is a <em>reduction</em> or a <em>fold</em>.
If you see these terms elsewhere in literature, documentation, or algorithms in libraries or programming languages, they almost certainly mean the same thing.
Some collection of values are <em>reduced</em> or <em>folded</em> into a single value.</p>

<p>You might be thinking to yourself, <em>we are starting with a collection of values (a matrix) and yet we end up with a collection of values (a vector). How is this a reduction/fold?</em></p>

<p>This is a good question: the reduction is not performed over the entire matrix, but only the <em>rows</em> of the matrix.
Each row of the matrix is <em>reduced</em> into a single value.</p>

<!---

For the following definitions:

<center>
\$\$
  \\
  M \gets   \left[ {\begin{array}{cc}
    0 & 1 & 2 \\
    3 & 4 & 5 \\
    6 & 7 & 8 \\
  \end{array} } \right] ,
  v \gets \left[ {\begin{array}{cc} 2 \\\ 2 \\\ 2  \end{array} } \right]
  \\
\$\$
</center>

--->

<p>The algorithm each unit of work performs is called <em>transform-reduce</em> (or sometimes <em>map-reduce</em>).</p>

<p>Although <em>transform-reduce</em> might seem like two algorithms (it kinda is!), it is such a universal operation that it is often considered it‚Äôs own algorithm (or at least it‚Äôs packaged as its own algorithm in libraries).
For example, <a href="https://thrust.github.io/doc/group__transformed__reductions_ga0d4232a9685675f488c3cc847111e48d.html" target="blank">the Thrust abstraction library that ships with NVIDIA‚Äôs CUDA Toolkit has the <em>transform-reduce</em> algorithm built-in.</a></p>

<p>In this case, we would like to <em>transform</em> our input tuples by multiplying two elements together, and then <em>reduce</em> our input using the sum operator.</p>

<p>In Python, a given unit of work might look like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="nb">reduce</span>
<span class="n">tuplespace_row0</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="p">]</span>

<span class="k">def</span> <span class="nf">work</span><span class="p">(</span><span class="n">tupl</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">reduce</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span>        <span class="c1"># use + to reduce
</span>            <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="c1"># use * to transform
</span>                <span class="n">tupl</span>                   <span class="c1"># input tuple
</span>                <span class="p">)</span>
            <span class="p">)</span>

<span class="c1"># Input to map is mat_row
# Input to reduce is [0, 2, 4]
# Final value is 6
</span><span class="k">print</span><span class="p">(</span><span class="n">work</span><span class="p">(</span><span class="n">tuplespace_row0</span><span class="p">))</span> <span class="c1"># yields 6
</span></code></pre></div></div>

<p>The following formula is a more formal definition of a single unit of work in our example:</p>

<center>
\$\$
\\
  r \gets current rank \\
  W_{r} \gets \sum_{i \gets 0}^{N} M_{r,i} \cdot v_{i} \\
\\
\$\$
</center>

<p>In the above case, the summation is the <em>reduce</em> operation, and the multiplication of the matrix elements and vector elements is the <em>transform</em> operation, transforming each tuple into a scalar before the reduction.</p>

<p>The key insight about this reduction is that no unit of work depends on another unit of work.
The domains of each unit of work are non-overlapping.
In other words, this algorithm is <em>loop independent</em> and can be parallelized along the rows of our tuplespace, again given by:</p>

<center>
\$\$
\\
  \left[ {\begin{array}{ccc}
    (Mat_{0,0}, v_0) &amp; (Mat_{0,1}, v_1) &amp; (Mat_{0,2}, v_2) \\
    \hline \\
    (Mat_{1,0}, v_0) &amp; (Mat_{1,1}, v_1) &amp; (Mat_{1,2}, v_2) \\
    \hline \\
    (Mat_{2,0}, v_0) &amp; (Mat_{2,1}, v_1) &amp; (Mat_{2,2}, v_2) \\
  \end{array} } \right]
\\
\$\$
</center>

<p>It was by identifying and understanding the underlying algorithms (<em>broadcast</em> and <em>transform-reduce</em>) of our higher-level algorithm that we are able to determine if and how it is parallelizable and loop independent.</p>

<blockquote>
  <p>Identify and understand the underlying algorithms</p>
</blockquote>

<p><em>NOTE: Even if your operation seems to be loop dependent, there are sometimes clever tricks you can use to parallelize your code. Perhaps you just haven‚Äôt been exposed to the correct algorithm yet!</em></p>

<p>We now hopefully understand that a matrix-vector product is formally <em>a broadcasted multiply followed by a series of sum-reductions</em> and that we can parallelize our algorithm by breaking it up into self-contained units of work.
We can now move on to implementing and parallelizing the algorithm.</p>

<h1 id="c-on-the-host">C on the Host</h1>

<p><a href="https://godbolt.org/z/T3qzr8fve" target="blank">The code for such a calculation might look like this in C</a>:</p>
<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">matvecmul</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">mat</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">vec</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">out</span><span class="p">,</span> <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>
            <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">vec</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">mat</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">n</span><span class="p">)];</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Here‚Äôs some example data fed into our matrix vector product:</p>
<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">M</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>

    <span class="kt">int</span> <span class="n">mat</span><span class="p">[</span><span class="n">M</span><span class="o">*</span><span class="n">N</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">M</span><span class="o">*</span><span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="n">mat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>

    <span class="kt">int</span> <span class="n">vec</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="n">vec</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>

    <span class="kt">int</span> <span class="n">out</span><span class="p">[</span><span class="n">M</span><span class="p">];</span>

    <span class="n">memset</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">M</span><span class="p">]));</span>
    <span class="n">matvecmul</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The output of this program (with some printing code added in):</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">Matrix:
  0   1   2   3 
  4   5   6   7 
  8   9  10  11 
Vector:
  0   1   2   3 
Output:
 14  38  62 
</span></code></pre></div></div>

<p>Feel free to verify these results and play around with other values using <a href="https://keisan.casio.com/exec/system/15052033860538" target="blank">online software like this CASIO calculator website</a>, or a scripting language.
<a href="https://mlochbaum.github.io/BQN/try.html#code=bSDihpAgMwpuIOKGkCA0Ck11bCDihpAgK8ud4oiYw5fijokx4oC/4oieCgptYXQg4oaQIG3igL9u4qWK4oaVMjAwCnZlYyDihpAg4oaVbgoKbWF0IE11bCB2ZWM=" target="blank">
Here‚Äôs an example of the above problem using BQN, one of my favorite languages to use when understanding an algorithm.
</a></p>

<p>Demonstrating that we have a <em>correct</em> algorithm with tests is a precondition for optimizing and parallelizing an algorithm:</p>

<blockquote>
  <p>Testing for correctness precedes parallelism and performance</p>
</blockquote>

<p>We know that a given index in our output vector can be computed independently of any other indices in the output vector from the respective row in our tuple space.
We can then pull out a function that performs a <em>single unit of work</em> as identified above.</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">unit_of_work</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">mat</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">vec</span><span class="p">,</span> <span class="kt">int</span> <span class="n">row</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">double</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">mat</span> <span class="o">+=</span> <span class="n">row</span> <span class="o">*</span> <span class="n">n</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
        <span class="n">sum</span> <span class="o">+=</span> <span class="n">mat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">vec</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="k">return</span> <span class="n">sum</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Compare this now with the single unit of work we described above:</p>
<center>
\$\$
\\
  r \gets current rank \\
  W_{r} \gets \sum_{i \gets 0}^{N} M_{r,i} \cdot v_{i} \\
\\
\$\$
</center>

<p>Our new <code class="language-plaintext highlighter-rouge">matvecmul</code> function can now just iterate over all the rows and dispatch the actual work to the <code class="language-plaintext highlighter-rouge">unit_of_work</code> function.
We can even use OpenMP to parallelize our loop:</p>
<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">matvecmul_on_tuplespace</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">mat</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">vec</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">out</span><span class="p">,</span> <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// dispatch calculations to unit_of_work for each row of mat</span>
    <span class="cp">#pragma omp parallel for
</span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">row</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">row</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">;</span> <span class="n">row</span><span class="o">++</span><span class="p">)</span>
        <span class="n">out</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">=</span> <span class="n">unit_of_work</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>You might have noticed that our new implementation has more code than our original implementation, and might be slightly more complex.
This is okay, and it gets at an important point:</p>

<blockquote>
  <p>Speed is not the same as efficiency</p>
</blockquote>

<p><a href="https://adspthepodcast.com/2021/11/12/Episode-51.html" target="blank">
This excellent podcast episode from the lead HPC architect at NVIDIA explains this point in detail.
</a></p>

<p>If our code performs <em>more work overall</em> it is less <em>efficient</em>.
If that additional work means we can perform calculations on multiple threads or additional devices resulting in lower runtime, it is <em>faster</em> and we‚Äôve increased its <em>speed</em>.
The key difference between speed and efficiency is this: speed is a factor of <em>time</em> and efficiency is a factor of <em>work</em>.
Sometimes optimizing code means improving speed, other times efficiency.
Most of the time, to run code on a GPU, you do have to perform more work to set up the calculation, so strictly speaking our code will be faster and less efficient.</p>

<h1 id="cuda-c">CUDA C</h1>

<p>CUDA C is the basis of the CUDA runtime, and forms the foundation for all other CUDA-related abstractions.
We‚Äôll take a look at some basic concepts before jumping into the code.
<a href="https://www.nvidia.com/content/GTC-2010/pdfs/2131_GTC2010.pdf" target="blank">
This CUDA C introductory slide deck is helpful in understanding the basics.
</a></p>

<h2 id="core-concepts">Core Concepts</h2>

<h3 id="host-and-device">Host and Device</h3>

<p>When working with a GPU, it‚Äôs important to keep in mind the difference between the <em>host</em> and the <em>device</em>.</p>

<center>
<img src="https://cis.temple.edu/~giorgio/cis307/readings/CUDA_processing_flow.png" alt="GPU-CPU interaction" />
</center>

<p>Just like your CPU, your GPU has access to it‚Äôs own <em>memory</em>.
Programming a GPU entails managing your CPU‚Äôs memory along with your GPU‚Äôs memory.
If you would like your GPU to have access to some memory you‚Äôre using on the CPU, you‚Äôll have to allocate memory on the GPU and copy it over.</p>

<p><a href="https://godbolt.org/z/9eeEedhd5" target="blank">
If you don‚Äôt tell the GPU to perform any work, then your CUDA C code is really just C code:
</a></p>
<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;cstdio&gt;</span><span class="cp">
</span><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">puts</span><span class="p">(</span><span class="s">"Hello!"</span><span class="p">);</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>You can then invoke NVIDIA‚Äôs compiler, NVCC, to compile the program:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span><span class="nb">cat </span>hello.cu
<span class="gp">#</span>include &lt;cstdio&gt;
<span class="go">int main() {
</span><span class="gp">  puts("Hello!");</span><span class="w">
</span><span class="gp">  return 0;</span><span class="w">
</span><span class="go">}
</span><span class="gp">$</span><span class="w"> </span>nvcc hello.cu <span class="nt">-o</span> hello <span class="o">&amp;&amp;</span> ./hello
<span class="go">Hello!
</span></code></pre></div></div>

<p>If you invoke <code class="language-plaintext highlighter-rouge">nvcc</code> with the <code class="language-plaintext highlighter-rouge">-v</code> flag for extra verbosity, you can see that <code class="language-plaintext highlighter-rouge">nvcc</code> actually uses a <em>host</em> compiler to build the parts of your program that don‚Äôt involve running code or manipulating memory on the GPU.
<code class="language-plaintext highlighter-rouge">nvcc</code> uses multiple passes, where it compiles the CUDA code and generates host-only source for the host compiler to compile.
<a href="https://godbolt.org/z/axTn1ex5x" target="blank">
See this Compiler Explorer link and look at the compilation output window in the bottom right pane to see all the output.
</a>
Notice that GCC is invoked, along with the program <code class="language-plaintext highlighter-rouge">ptxas</code>.
PTX is an assembly target, so your CUDA programs will emit ptx code which can be run on your GPU‚Äôs special purpose processing units.
<a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#nvcc-examples" target="blank">
The command line flags for code generation can be complicated.
Refer to the official CUDA programming guide when needed.
</a>
Just as you can use <code class="language-plaintext highlighter-rouge">asm volitile("" : : : "");</code> in C and C++ to write inline assembly, you can also write inline ptx assembly in your programs.
Also like C and C++, it is almost certainly more effective for you to write your code in a higher level language like CUDA C++, and write PTX after profiling and testing, when you are sure you need it.</p>

<p>If you‚Äôre careful, you might also have noticed that GCC was passed the command line argument <code class="language-plaintext highlighter-rouge">-x c++</code>, even though we‚Äôre working in plain CUDA C.
This is because cuda code is <em>by default built on the host as C++</em>.
If you use the oldest CUDA compiler available on Compiler Explorer, you‚Äôll see that it still defaults to building the host code under C++14.</p>

<p>The full NVCC compilation pipeline is depicted below:</p>

<center>
<img src="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/graphics/cuda-compilation-from-cu-to-executable.png" alt="The full NVCC compilation pipeline" />
</center>

<h3 id="using-the-cuda-runtime">Using the CUDA Runtime</h3>

<p><a href="https://godbolt.org/z/81v3jfehq" target="blank">
In this example, we introduce three aspects of the CUDA programming model:
</a></p>

<ul>
  <li>The special keyword <code class="language-plaintext highlighter-rouge">__global__</code></li>
  <li>Device memory management</li>
  <li>Kernel launches</li>
</ul>

<div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// square.cu</span>
<span class="cp">#include</span> <span class="cpf">&lt;cstdio&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span>
<span class="k">__global__</span> <span class="kt">void</span> <span class="nf">square</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">ar</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">ar</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">ar</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">ar</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="n">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="cp">#define N 10
</span>
  <span class="c1">// Allocate static memory on host</span>
  <span class="kt">int</span> <span class="n">ar</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="n">ar</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>

  <span class="c1">// Allocate memory on device, copy from host to device</span>
  <span class="kt">int</span><span class="o">*</span> <span class="n">d_ar</span><span class="p">;</span>
  <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_ar</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]));</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_ar</span><span class="p">,</span> <span class="n">ar</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

  <span class="c1">// Launch kernel to run on the device</span>
  <span class="n">square</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_ar</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>

  <span class="c1">// Copy memory back from device</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">ar</span><span class="p">,</span> <span class="n">d_ar</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

  <span class="c1">// Display values after kernel</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"%d "</span><span class="p">,</span> <span class="n">ar</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="n">puts</span><span class="p">(</span><span class="s">""</span><span class="p">);</span>

  <span class="c1">// Deallocate memory</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_ar</span><span class="p">);</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>nvcc square.cu <span class="nt">-o</span> square
<span class="gp">$</span><span class="w"> </span>./square
<span class="go">0 1 4 9 16 25 36 49 64 81
</span></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">__global__</code> indicates that the code <em>runs on the device</em> and is <em>called from the host</em>.
Keep in mind that we have two <em>memory spaces</em> and two <em>execution spaces</em>.</p>

<p>The following table enumerates common operations in C along with their CUDA counterpart:</p>

<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0pky">Operation</th>
    <th class="tg-0pky">C</th>
    <th class="tg-0pky">CUDA</th>
    <th class="tg-0pky">Notes</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky">Dynamically allocate memory</td>
    <td class="tg-0pky"><span style="color:#905;background-color:#ddd">`malloc`</span></td>
    <td class="tg-0pky"><span style="color:#905;background-color:#ddd">`cudaMalloc`</span></td>
    <td class="tg-0pky"></td>
  </tr>
  <tr>
    <td class="tg-0pky">Copy memory from one location to another</td>
    <td class="tg-0pky"><span style="color:#905;background-color:#ddd">`memcpy`</span></td>
    <td class="tg-0pky"><span style="color:#905;background-color:#ddd">`cudaMemcpy`</span></td>
    <td class="tg-0pky"></td>
  </tr>
  <tr>
    <td class="tg-0pky">Deallocate memory</td>
    <td class="tg-0pky"><span style="color:#905;background-color:#ddd">`free`</span></td>
    <td class="tg-0pky"><span style="color:#905;background-color:#ddd">`cudaFree`</span></td>
    <td class="tg-0pky"></td>
  </tr>
  <tr>
    <td class="tg-0pky">Call function</td>
    <td class="tg-0pky"><span style="color:#905;background-color:#ddd">`func()`</span></td>
    <td class="tg-0pky"><span style="color:#905;background-color:#DDD">`func&lt;&lt;&lt;gridSize, blockSize, sharedMemSize, cudaStream&gt;&gt;&gt;()`</span></td>
    <td class="tg-0pky">Triple angle-brackets indicate you are running in the <span style="font-style:italic">_device execution space_</span></td>
  </tr>
</tbody>
</table>

</center>

<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#execution-configuration" target="blank">
The angle brackets surrounding our <em>kernel launch parameters</em> determine how the kernel will be executed by the GPU.
The possible kernel launch parameters are enumerated at this link.
</a></p>

<p>The kernel launch parameters determine how many streaming multiprocessors (SMs) will execute code on the GPU.
The first two parameters are objects of type <code class="language-plaintext highlighter-rouge">dim3</code>, and they can be up to three-dimensional vectors.
The first kernel launch parameter is the <em>grid size</em>, and the second is the <em>block size</em>.</p>

<p>Grids consist of blocks.</p>

<p>Blocks consist of threads.</p>

<p>Therefore, the total number of threads launched by your kernel will be:</p>

<center>
\$\$
totalthreads \gets gridsize.x \times gridsize.y \times gridsize.z \\
                   \times blocksize.x \times blocksize.y \times blocksize.z \\
\$\$
</center>

<p>CUDA kernels may be launched with a 1-3 dimensional grid, and a 1-3 dimensional block.
The image below might have been launched with these kernel launch parameters:</p>

<div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kt">dim3</span> <span class="nf">grid_size</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
  <span class="kt">dim3</span> <span class="nf">block_size</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
  <span class="n">myfunc</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid_size</span><span class="p">,</span> <span class="n">block_size</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
</code></pre></div></div>

<center>
<img src="http://www.microway.com/wp-content/uploads/CUDA-GridBlockThread-Structure.png" alt="CUDA Grid and Block Depiction" />
</center>

<p>You might also notice that we guard our operation with this <code class="language-plaintext highlighter-rouge">if</code> statement.</p>
<div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">ar</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">ar</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">ar</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
</code></pre></div></div>
<p>For performance reasons, it‚Äôs usually best to launch your kernels with a multiple of the number of threads in a given block on your GPU, so you may launch with more GPU threads than you need.</p>

<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation" target="blank">
For additional reading on the hardware implementation of the CUDA programming model, please refer to chapter 4 of the NVIDIA CUDA Programming Guide.
</a></p>

<h3 id="shared-memory">Shared Memory</h3>

<p>Although each thread launches with its own stack memory, threads can share memory just like OS threads.
The third kernel launch parameter determines how many bytes will be allocated <em>for each block</em> that is launched.</p>

<p><a href="https://godbolt.org/z/nrbdK9nKj" target="blank">
In the following example, we make use of CUDA shared memory, as indicated by the <code class="language-plaintext highlighter-rouge">__shared__</code> keyword annotating the array in our kernel, as well as our use of the third kernel launch parameter:
</a></p>
<div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;cstdio&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span>
<span class="k">__global__</span> <span class="kt">void</span> <span class="nf">mulsum</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">b</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">out</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">extern</span> <span class="k">__shared__</span> <span class="kt">int</span> <span class="n">tmp</span><span class="p">[];</span>
  <span class="cm">/* ‚ñ≤
   * ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   * ‚îÇ  ‚îÇExternal and shared, allocated ‚îÇ
   * ‚îî‚îÄ‚îÄ‚î§by the cuda runtime when kernel‚îÇ
   *    ‚îÇ         is launched           ‚îÇ
   *    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   */</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&gt;=</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span><span class="p">;</span>

  <span class="n">tmp</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>

  <span class="n">__syncthreads</span><span class="p">();</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
      <span class="n">sum</span> <span class="o">+=</span> <span class="n">tmp</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="o">*</span><span class="n">out</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="n">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="cp">#define N 10
</span>
  <span class="kt">int</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>

  <span class="kt">int</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>

  <span class="kt">int</span><span class="o">*</span> <span class="n">d_a</span><span class="p">;</span>
  <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_a</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]));</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

  <span class="kt">int</span><span class="o">*</span> <span class="n">d_b</span><span class="p">;</span>
  <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_b</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]));</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_b</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

  <span class="kt">int</span><span class="o">*</span> <span class="n">d_out</span><span class="p">;</span>
  <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_out</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>

  <span class="n">mulsum</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">])</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span> <span class="n">d_b</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
  <span class="cm">/*             ‚ñ≤
   *             ‚îÇ
   *    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   *    ‚îÇSize of shared memory to be allocated‚îÇ
   *    ‚îÇ         for kernel launch           ‚îÇ
   *    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   */</span>

  <span class="kt">int</span> <span class="n">out</span><span class="p">;</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">out</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
  <span class="n">printf</span><span class="p">(</span><span class="s">"%d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">out</span><span class="p">);</span>

  <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_a</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_b</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_out</span><span class="p">);</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>nvcc mul-sum-reduce.cu <span class="o">&amp;&amp;</span> ./a.out
<span class="go">285
</span></code></pre></div></div>

<p>Notice how our shared memory is declared:</p>
<div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">extern</span> <span class="k">__shared__</span> <span class="kt">int</span> <span class="n">tmp</span><span class="p">[];</span>
</code></pre></div></div>

<p>It is <code class="language-plaintext highlighter-rouge">external</code> because we are not allocating the memory in our kernel; it‚Äôs allocated by the cuda runtime when we pass the third parameter to the kernel launch parameters:</p>

<div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mulsum</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="o">*</span><span class="n">N</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span> <span class="n">d_b</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
               <span class="err">‚ñ≤</span>
               <span class="err">‚îÇ</span>
 <span class="err">‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê</span>
 <span class="err">‚îÇ</span><span class="n">Size</span> <span class="n">of</span> <span class="n">shared</span> <span class="n">memory</span> <span class="n">to</span> <span class="n">be</span> <span class="n">allocated</span><span class="err">‚îÇ</span>
 <span class="err">‚îÇ</span>         <span class="k">for</span> <span class="n">kernel</span> <span class="n">launch</span>           <span class="err">‚îÇ</span>
 <span class="err">‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</span>
</code></pre></div></div>

<p>There can only be one segment of shared memory in a kernel launch, so the shared memory segment will be interpreted as whatever type we declare our shared memory with.
In this case, it‚Äôs an array of ints.
Although there is strictly one <em>segment</em> of shared memory in a kernel launch, you can still declare multiple variables as <code class="language-plaintext highlighter-rouge">__shared__</code>, so long as they all fit in the allocated shared memroy.</p>

<p>We also introduced another CUDA extension to the host language: <code class="language-plaintext highlighter-rouge">__syncthreads()</code>.
<code class="language-plaintext highlighter-rouge">__syncthreads()</code> is a <em>fence</em> or <em>barrier</em>, a point which no thread <em>in that block</em> can cross until all threads have reached it.
<code class="language-plaintext highlighter-rouge">__syncthreads()</code>
There are many other CUDA primitives for atomic, and synchronization operations, such as <code class="language-plaintext highlighter-rouge">atomicAdd</code>.</p>

<h3 id="cuda-mat-vec-multiply">CUDA Mat-Vec Multiply</h3>

<p>We again return to our <code class="language-plaintext highlighter-rouge">matvecmul</code> example, this time armed with some knowledge about the CUDA runtime and some software and hardware abstractions.</p>

<div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;cstdio&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span>
<span class="k">__global__</span> <span class="kt">void</span> <span class="nf">matvecmul</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">mat</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">vec</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">outv</span><span class="p">,</span>
                          <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>

  <span class="kt">int</span> <span class="n">rowidx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">colidx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

  <span class="k">extern</span> <span class="k">__shared__</span> <span class="kt">int</span> <span class="n">tmp</span><span class="p">[];</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">colidx</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">&amp;&amp;</span> <span class="n">rowidx</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">tmp</span><span class="p">[</span><span class="n">colidx</span><span class="p">]</span> <span class="o">=</span> <span class="n">mat</span><span class="p">[</span><span class="n">colidx</span> <span class="o">+</span> <span class="p">(</span><span class="n">rowidx</span> <span class="o">*</span> <span class="n">n</span><span class="p">)]</span> <span class="o">*</span> <span class="n">vec</span><span class="p">[</span><span class="n">colidx</span><span class="p">];</span>

    <span class="n">__syncthreads</span><span class="p">();</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">colidx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">int</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
        <span class="n">sum</span> <span class="o">+=</span> <span class="n">tmp</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
      <span class="n">outv</span><span class="p">[</span><span class="n">rowidx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="n">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="cp">#define M 10
</span>  <span class="cp">#define N 15
</span>
  <span class="kt">int</span> <span class="n">a</span><span class="p">[</span><span class="n">M</span><span class="o">*</span><span class="n">N</span><span class="p">];</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">M</span><span class="o">*</span><span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>

  <span class="kt">int</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>

  <span class="kt">int</span><span class="o">*</span> <span class="n">d_a</span><span class="p">;</span>
  <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_a</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">M</span><span class="o">*</span><span class="n">N</span><span class="p">]));</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">M</span><span class="o">*</span><span class="n">N</span><span class="p">]),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

  <span class="kt">int</span><span class="o">*</span> <span class="n">d_b</span><span class="p">;</span>
  <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_b</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]));</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_b</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

  <span class="kt">int</span><span class="o">*</span> <span class="n">d_c</span><span class="p">;</span>
  <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_c</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">M</span><span class="p">]));</span>

  <span class="n">matvecmul</span><span class="o">&lt;&lt;&lt;</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">])</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span> <span class="n">d_b</span><span class="p">,</span> <span class="n">d_c</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>

  <span class="kt">int</span> <span class="n">c</span><span class="p">[</span><span class="n">M</span><span class="p">];</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">d_c</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">M</span><span class="p">]),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

  <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_a</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_b</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_c</span><span class="p">);</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>After adding some printing code to our example above, we get the following:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>nvcc matvecmul.cu <span class="o">&amp;&amp;</span> ./a.out
<span class="go">Matrix:
   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14
  15   16   17   18   19   20   21   22   23   24   25   26   27   28   29
  30   31   32   33   34   35   36   37   38   39   40   41   42   43   44
  45   46   47   48   49   50   51   52   53   54   55   56   57   58   59
  60   61   62   63   64   65   66   67   68   69   70   71   72   73   74
  75   76   77   78   79   80   81   82   83   84   85   86   87   88   89
  90   91   92   93   94   95   96   97   98   99  100  101  102  103  104
 105  106  107  108  109  110  111  112  113  114  115  116  117  118  119
 120  121  122  123  124  125  126  127  128  129  130  131  132  133  134
 135  136  137  138  139  140  141  142  143  144  145  146  147  148  149

Vector:
   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14

Output:
1015 2590 4165 5740 7315 8890 10465 12040 13615 15190
</span></code></pre></div></div>

<p><a href="https://mlochbaum.github.io/BQN/try.html#code=TXVsIOKGkCAry53iiJjDl+KOiTHigL/iiJ4KCm3ihpAxMOKAvzE14qWK4oaVMjAwCnbihpDihpUxNQoKbSBNdWwgdg==" target="blank">
This BQN example verifies the output from our CUDA program:
</a></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   Mul ‚Üê +Àù‚àò√ó‚éâ1‚Äø‚àû
   m‚Üê10‚Äø15‚•ä‚Üï200
   v‚Üê‚Üï15
   m Mul v
‚ü® 1015 2590 4165 5740 7315 8890 10465 12040 13615 15190 ‚ü©
</code></pre></div></div>

<p>In our CUDA C example, we launch a block for each row of our matrix.
This way, we can share memory between each thread operating on a given row of the matrix.
A single thread per row can then perform the sum reduction and assign the value to the index in the output vector.</p>

<h1 id="cuda-c-1">CUDA C++</h1>

<h2 id="naive-approach">Naive Approach</h2>

<p>In our previous CUDA C example, we weren‚Äôt really using CUDA C <em>perse</em>, but CUDA C++.
If you read the introductory chapter in <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#cuda-general-purpose-parallel-computing-architecture" target="blank">the official NVIDIA CUDA Programming guide</a>, you‚Äôll see that CUDA C is really just CUDA mixed with the common language subset between C and C++ on the host.
We were using CUDA C++ the whole time, we just restricted ourselves to the C subset of C++ for simplicity.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;cstdio&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thrust/device_vector.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thrust/host_vector.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thrust/sequence.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span>
<span class="cm">/* ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇUsing thrust's pointer type‚îÇ
   ‚îÇ     instead of int*       ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò */</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">matvecmul</span><span class="p">(</span>
    <span class="n">thrust</span><span class="o">::</span><span class="n">device_ptr</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">mat</span><span class="p">,</span>
    <span class="n">thrust</span><span class="o">::</span><span class="n">device_ptr</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">vec</span><span class="p">,</span>
    <span class="n">thrust</span><span class="o">::</span><span class="n">device_ptr</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">outv</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>

  <span class="kt">int</span> <span class="n">rowidx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">colidx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

  <span class="k">extern</span> <span class="n">__shared__</span> <span class="kt">int</span> <span class="n">tmp</span><span class="p">[];</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">colidx</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">&amp;&amp;</span> <span class="n">rowidx</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="n">tmp</span><span class="p">[</span><span class="n">colidx</span><span class="p">]</span> <span class="o">=</span> <span class="n">mat</span><span class="p">[</span><span class="n">colidx</span> <span class="o">+</span> <span class="p">(</span><span class="n">rowidx</span> <span class="o">*</span> <span class="n">n</span><span class="p">)]</span> <span class="o">*</span> <span class="n">vec</span><span class="p">[</span><span class="n">colidx</span><span class="p">];</span>

    <span class="n">__syncthreads</span><span class="p">();</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">colidx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">int</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
        <span class="n">sum</span> <span class="o">+=</span> <span class="n">tmp</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
      <span class="n">outv</span><span class="p">[</span><span class="n">rowidx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="n">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="cp">#define M 10
</span>  <span class="cp">#define N 15
</span>
  <span class="cm">/* ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ Using thrust's host and device ‚îÇ
     ‚îÇ    vectors over raw arrays.    ‚îÇ
     ‚îÇNo longer need to use cudaMemcpy‚îÇ
     ‚îÇ        or cudaMalloc!          ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò */</span>
  <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">a</span><span class="p">(</span><span class="n">M</span><span class="o">*</span><span class="n">N</span><span class="p">);</span>
  <span class="n">thrust</span><span class="o">::</span><span class="n">sequence</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">a</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mi">0</span><span class="p">);</span>

  <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">b</span><span class="p">(</span><span class="n">N</span><span class="p">);</span>
  <span class="n">thrust</span><span class="o">::</span><span class="n">sequence</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">b</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mi">0</span><span class="p">);</span>

  <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">c</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

  <span class="n">matvecmul</span><span class="o">&lt;&lt;&lt;</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">])</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">b</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">c</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>

  <span class="cm">/* ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇThe assignment operator will‚îÇ
     ‚îÇ   perform the cudaMemcpy   ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò */</span>
  <span class="n">thrust</span><span class="o">::</span><span class="n">host_vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">c</span><span class="p">;</span>

  <span class="n">puts</span><span class="p">(</span><span class="s">"Output:"</span><span class="p">);</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"%d "</span><span class="p">,</span> <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="n">puts</span><span class="p">(</span><span class="s">""</span><span class="p">);</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>nvcc thrust-ex.cu <span class="o">&amp;&amp;</span> ./a.out
<span class="go">Output:
1015 2590 4165 5740 7315 8890 10465 12040 13615 15190
</span></code></pre></div></div>

<p>As you can see, the code looks quite similar, except for the lack of memory management.
This is hiding a few extra details as well.
In our original CUDA example, we first allocate and assign to memory on the host before copying it to the device.
In this example, we allocate memory <em>on the device first</em>, and perform assignments <em>on the device</em>.</p>

<p>In this line, we allocate <em>device</em> memory for our matrix, and assign values to it <em>on the device</em>.</p>
<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">a</span><span class="p">(</span><span class="n">M</span><span class="o">*</span><span class="n">N</span><span class="p">);</span>
  <span class="n">thrust</span><span class="o">::</span><span class="n">sequence</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">a</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mi">0</span><span class="p">);</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">thrust::sequence</code> is almost identical to <code class="language-plaintext highlighter-rouge">std::iota</code> or <code class="language-plaintext highlighter-rouge">for (int i=0; i &lt; N; i++) vec[i] = i;</code>, except that it may execute on the device.
In this new example, we launch three kernels instead of one: one for each call to <code class="language-plaintext highlighter-rouge">thrust::sequence</code>, and one for our manual kernel launch.
<a href="https://godbolt.org/z/nKvajeE5P" target="blank">
You can look at the details of the ptx assembly in Compiler Explorer here.
</a></p>

<h2 id="more-sophisticated-approach">More Sophisticated Approach</h2>

<p>Remember all that fuss about <em>fundamental algorithms</em> in the earlier sections?
How our <em>fundamental algorithm</em> here is a transform-reduce?</p>

<p>Well, in our first-pass CUDA implementation, we don‚Äôt really use this to our advantage.
Our kernel contains the following lines:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">if</span> <span class="p">(</span><span class="n">colidx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">int</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
        <span class="n">sum</span> <span class="o">+=</span> <span class="n">tmp</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
      <span class="n">outv</span><span class="p">[</span><span class="n">rowidx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div>

<p><a href="https://github.com/NVIDIA/thrust/blob/d461afaefdb0b22d830f8d5e9a7b42aebff7004f/thrust/system/cuda/detail/reduce.h#L489" target="blank">
Thrust‚Äôs <code class="language-plaintext highlighter-rouge">transform_reduce</code> uses a rather complicated multi-pass, tiled approach to reducing a collection of values to a single value, but we only use a single thread in a block to actually reduce a given index in our output vector.
</a></p>

<p>While we used a raw loop at least once per block, an optimized reduction will perform something like the following:</p>

<center>
<img src="https://i.stack.imgur.com/HxccQ.png" alt="depiction of a multi-pass sum reduction" />
</center>

<p>Extremely performant reductions are actually quite hard to get right - it‚Äôs easy to get <em>some</em> parallelism in a reduction, but it takes significant effort to truly maximize the speed you can get from a GPU.
<a href="https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf" target="blank">
This slidedeck from the NVIDIA developer blog details various approaches to optimizing a reduction operation on a GPU.
</a>
Thrust‚Äôs reduce implementation will even select different strategies and launch parameters based on the sizes of the data it operates on.</p>

<p>The point of this Thrust discussion is not to dissuade you from writing raw CUDA kernels - it‚Äôs to dissuage you from doing it too early.
In the majority of cases, it‚Äôs likely that using a library around raw CUDA kernels will result in faster code and less development time.
Once you have already written your code using <em>known algorithms</em>, once you have tested your code to demonstrate its correctness, once you have profiled your code to demonstrate where the performance bottlenecks are on the target architectures you care about, then it makes sense to write raw CUDA kernels.</p>

<blockquote>
  <p>Use sane defaults, only optimize after profiling and testing</p>
</blockquote>

<p>So let‚Äôs try again, using Thrust‚Äôs parallel algorithms to compute the reductions for each row of the matrix-vector multiplication <a href="https://godbolt.org/z/G7KEfqWcE" target="blank">(godbolt link here)</a>:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;cstdio&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thrust/iterator/zip_iterator.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thrust/tuple.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thrust/device_vector.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thrust/host_vector.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thrust/sequence.h&gt;</span><span class="cp">
</span>
<span class="n">__global__</span>
<span class="kt">void</span> <span class="nf">broadcast_to_matrix</span><span class="p">(</span><span class="n">thrust</span><span class="o">::</span><span class="n">device_ptr</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">mat</span><span class="p">,</span>
    <span class="n">thrust</span><span class="o">::</span><span class="n">device_ptr</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">vec</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="k">auto</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">const</span> <span class="k">auto</span> <span class="n">row</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">m</span> <span class="n">and</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">mat</span><span class="p">[</span><span class="n">col</span><span class="o">+</span><span class="p">(</span><span class="n">row</span><span class="o">*</span><span class="n">n</span><span class="p">)]</span> <span class="o">=</span> <span class="n">vec</span><span class="p">[</span><span class="n">col</span><span class="p">];</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="n">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="cp">#define M 10
</span>  <span class="cp">#define N 15
</span>
  <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">a</span><span class="p">(</span><span class="n">M</span><span class="o">*</span><span class="n">N</span><span class="p">);</span>
  <span class="n">thrust</span><span class="o">::</span><span class="n">sequence</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">a</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mi">0</span><span class="p">);</span>

  <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">b</span><span class="p">(</span><span class="n">N</span><span class="p">);</span>
  <span class="n">thrust</span><span class="o">::</span><span class="n">sequence</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">b</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mi">0</span><span class="p">);</span>

  <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">broadcasted_b</span><span class="p">(</span><span class="n">M</span><span class="o">*</span><span class="n">N</span><span class="p">);</span>
  <span class="n">broadcast_to_matrix</span><span class="o">&lt;&lt;&lt;</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">broadcasted_b</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">b</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>

  <span class="n">thrust</span><span class="o">::</span><span class="n">host_vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">c</span><span class="p">(</span><span class="n">M</span><span class="p">);</span>

  <span class="n">thrust</span><span class="o">::</span><span class="n">zip_iterator</span> <span class="n">iter</span><span class="p">(</span><span class="n">thrust</span><span class="o">::</span><span class="n">make_tuple</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">broadcasted_b</span><span class="p">.</span><span class="n">begin</span><span class="p">()));</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">thrust</span><span class="o">::</span><span class="n">transform_reduce</span><span class="p">(</span><span class="n">iter</span><span class="o">+</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">N</span><span class="p">),</span> <span class="n">iter</span><span class="o">+</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">N</span><span class="p">)</span><span class="o">+</span><span class="n">N</span><span class="p">,</span>
        <span class="p">[]</span> <span class="n">__device__</span> <span class="p">(</span><span class="k">auto</span> <span class="n">tup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kt">int</span> <span class="p">{</span>
          <span class="k">return</span> <span class="n">thrust</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tup</span><span class="p">)</span> <span class="o">*</span> <span class="n">thrust</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tup</span><span class="p">);</span>
        <span class="p">},</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="n">thrust</span><span class="o">::</span><span class="n">plus</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">()</span>
        <span class="p">);</span>

  <span class="n">puts</span><span class="p">(</span><span class="s">"Output:"</span><span class="p">);</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"%d "</span><span class="p">,</span> <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="n">puts</span><span class="p">(</span><span class="s">""</span><span class="p">);</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Since we‚Äôre using some more fancy C++ features, we have to pass some extra flags to the compiler:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>nvcc <span class="nt">-std</span><span class="o">=</span>c++17 <span class="nt">--extended-lambda</span> fancy.cu <span class="o">&amp;&amp;</span> ./a.out
<span class="go">Output:
1015 2590 4165 5740 7315 8890 10465 12040 13615 15190
</span></code></pre></div></div>

<p>The first kernel is launched manually, since we‚Äôre just broadcasting values from the vector to another vector to match the shape of our matrix.
We perform this step so we can create a zip iterator from the matrix and the broadcasted vector:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">thrust</span><span class="o">::</span><span class="n">zip_iterator</span> <span class="nf">iter</span><span class="p">(</span><span class="n">thrust</span><span class="o">::</span><span class="n">make_tuple</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">broadcasted_b</span><span class="p">.</span><span class="n">begin</span><span class="p">()));</span>
</code></pre></div></div>

<p>This means we can feed the single iterator into our <code class="language-plaintext highlighter-rouge">transform_reduce</code> operation.
Elements obtained by the zip iterator are passed into our lambda function, and we simply multiply the two values together, before using the <code class="language-plaintext highlighter-rouge">plus</code> functor to reduce the vector of intermediate values for a given row into a scalar:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">thrust</span><span class="o">::</span><span class="n">transform_reduce</span><span class="p">(</span><span class="n">iter</span><span class="o">+</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">N</span><span class="p">),</span> <span class="n">iter</span><span class="o">+</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">N</span><span class="p">)</span><span class="o">+</span><span class="n">N</span><span class="p">,</span>
        <span class="p">[]</span> <span class="n">__device__</span> <span class="p">(</span><span class="k">auto</span> <span class="n">tup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kt">int</span> <span class="p">{</span>
          <span class="k">return</span> <span class="n">thrust</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tup</span><span class="p">)</span> <span class="o">*</span> <span class="n">thrust</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tup</span><span class="p">);</span>
        <span class="p">},</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="n">thrust</span><span class="o">::</span><span class="n">plus</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">()</span>
        <span class="p">);</span>
</code></pre></div></div>

<p>If we want to use threading on the host as well, we can even use an OpenMP directive:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#pragma openmp parallel for
</span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">thrust</span><span class="o">::</span><span class="n">transform_reduce</span><span class="p">(</span><span class="n">iter</span><span class="o">+</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">N</span><span class="p">),</span> <span class="n">iter</span><span class="o">+</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">N</span><span class="p">)</span><span class="o">+</span><span class="n">N</span><span class="p">,</span>
        <span class="p">[]</span> <span class="n">__device__</span> <span class="p">(</span><span class="k">auto</span> <span class="n">tup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kt">int</span> <span class="p">{</span>
          <span class="k">return</span> <span class="n">thrust</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tup</span><span class="p">)</span> <span class="o">*</span> <span class="n">thrust</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tup</span><span class="p">);</span>
        <span class="p">},</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="n">thrust</span><span class="o">::</span><span class="n">plus</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">()</span>
        <span class="p">);</span>
</code></pre></div></div>

<p>We‚Äôll have to tell <code class="language-plaintext highlighter-rouge">nvcc</code> to pass the <code class="language-plaintext highlighter-rouge">-fopenmp</code> flag to the host compiler:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>nvcc <span class="nt">-std</span><span class="o">=</span>c++17 <span class="nt">--extended-lambda</span> <span class="nt">-Xcompiler</span> <span class="nt">-fopenmp</span> fancy.cu
</code></pre></div></div>

<h2 id="the-best-tool-for-the-job">The Best Tool for the Job</h2>

<p>We have by now hopefully learned that we should use the most specialized tool for the job, and we should write kernels by hand only when we‚Äôre sure we can do better than your libraries of choice.
We can take this principle one step further with a little extra knowledge of our problem.</p>

<p>A matrix-vector product is a very common linear algebra operation, and a member of the Basic Linear Algebra Subroutines interface, which CUDA provides a library for (CUBLAS).
Because this is such a common operation, NVIDIA provides an extremely fast implementation - far more optimized than anything we would write by hand.</p>

<p>This knowledge of our problem leads us to using the most appropriate library, and likely to the fastest solution.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;cstdio&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thrust/device_vector.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thrust/host_vector.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thrust/sequence.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cublas_v2.h&gt;</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="cp">#define M 10
</span>  <span class="cp">#define N 15
</span>
  <span class="n">cublasHandle_t</span> <span class="n">ch</span><span class="p">;</span>
  <span class="n">cublasCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ch</span><span class="p">);</span>

  <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">a</span><span class="p">(</span><span class="n">M</span><span class="o">*</span><span class="n">N</span><span class="p">);</span>
  <span class="n">thrust</span><span class="o">::</span><span class="n">sequence</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">a</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mi">0</span><span class="p">);</span>

  <span class="k">const</span> <span class="kt">double</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">double</span> <span class="n">beta</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>

  <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">b</span><span class="p">(</span><span class="n">N</span><span class="p">);</span>
  <span class="n">thrust</span><span class="o">::</span><span class="n">sequence</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">b</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mi">0</span><span class="p">);</span>

  <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">c</span><span class="p">(</span><span class="n">M</span><span class="p">);</span>

  <span class="cp">#define PTR(x) thrust::raw_pointer_cast(x.data())
</span>  <span class="n">cublasDgemv</span><span class="p">(</span>
      <span class="n">ch</span><span class="p">,</span>
      <span class="n">CUBLAS_OP_T</span><span class="p">,</span>
      <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
      <span class="o">&amp;</span><span class="n">alpha</span><span class="p">,</span>
      <span class="n">PTR</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">N</span><span class="p">,</span>
      <span class="n">PTR</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span>
      <span class="o">&amp;</span><span class="n">beta</span><span class="p">,</span>
      <span class="n">PTR</span><span class="p">(</span><span class="n">c</span><span class="p">),</span> <span class="mi">1</span>
      <span class="p">);</span>
  <span class="cp">#undef PTR
</span>
  <span class="n">thrust</span><span class="o">::</span><span class="n">host_vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">hc</span> <span class="o">=</span> <span class="n">c</span><span class="p">;</span>

  <span class="n">puts</span><span class="p">(</span><span class="s">"Output:"</span><span class="p">);</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"%.1f "</span><span class="p">,</span> <span class="n">hc</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="n">puts</span><span class="p">(</span><span class="s">""</span><span class="p">);</span>

  <span class="n">cublasDestroy</span><span class="p">(</span><span class="n">ch</span><span class="p">);</span>

  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>nvcc cublas.cu <span class="nt">-lcublas</span> <span class="o">&amp;&amp;</span> ./a.out
<span class="go">Output:
1015.0 2590.0 4165.0 5740.0 7315.0 8890.0 10465.0 12040.0 13615.0 15190.0
</span></code></pre></div></div>

<h1 id="conclusion">Conclusion</h1>

<h1 id="bqn-example">BQN Example</h1>

<p>Personally, I use BQN to prototype solutions to problems and to better understand the fundamental algorithms at play; you don‚Äôt have to know an APL in order to understand this, but it might be helpful.
Feel free to skip this section; it is not critical to understanding the concepts.</p>

<p><a href="https://mlochbaum.github.io/BQN/try.html#code=4oCiU2hvdyBtYXQg4oaQIDPigL8z4qWK4oaVMTAK4oCiU2hvdyB2ZWMg4oaQIDPipYoyCivLneKOiTEgbWF0w5d2ZWMK" target="blank">Here‚Äôs a permalink to the BQN snippet.</a></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   # Same matrix as in our C example
   mat ‚Üê 3‚Äø3‚•ä‚Üï10
‚îå‚îÄ       
‚ïµ 0 1 2  
  3 4 5  
  6 7 8  
        ‚îò
   # Same vector as in our C example
   vec ‚Üê 3‚•ä2
‚ü® 2 2 2 ‚ü©

   +Àù‚éâ1 mat√óvec
‚ü® 6 24 42 ‚ü©
</code></pre></div></div>

<p>The core algorithm is seen in the final expression:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+Àù‚éâ1 mat√óvec
‚ñ≤    ‚ñ≤
‚îÇ    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§Multiply rows of mat by vec‚îÇ
‚îÇ          ‚îÇ        element-wise       ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     ‚îÇSum-reduce rows of matrix‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ resulting from mat√óvec  ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div></div>

<p>Alternatively:</p>

<center>
<img height="300" src="/images/hpc-101-matvec/bqn-matvecmul-explain.png" alt="Try BQN explanation of matvecmul" />
</center>

<h1 id="links-references-additional-reading">Links, References, Additional Reading</h1>

<ul>
  <li><a href="https://mlochbaum.github.io/BQN/try.html#code=4oCiU2hvdyBtYXQg4oaQIDPigL8z4qWK4oaVMTAK4oCiU2hvdyB2ZWMg4oaQIDPipYoyCivLneKOiTEgbWF0w5d2ZWMK" target="blank">BQN matvecmul example</a></li>
  <li><a href="https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.2-Multiplying-Matrices-and-Vectors/" target="blank">Matrix-Vector Product image</a></li>
  <li><a href="https://www.cs.utexas.edu/~lin/cs380c/handout27.pdf" target="blank">UT Austin slides on loop-carried dependencies and parallelism</a></li>
  <li><a href="https://www.worldcat.org/title/how-to-write-parallel-programs-a-first-course/oclc/912171709&amp;referer=brief_results" target="blank"><em>How to Write Parallel Programs: A First Course</em></a></li>
  <li><a href="https://thrust.github.io/doc/group__transformed__reductions_ga0d4232a9685675f488c3cc847111e48d.html" target="blank">Thrust parallel algorithms library</a></li>
  <li><a href="https://adspthepodcast.com/2021/11/12/Episode-51.html" target="blank"> ADSP podcast episode from the lead HPC architect at NVIDIA discussing speed vs efficiency</a></li>
  <li><a href="https://youtu.be/KK3JXvSiJG4" target="blank"> Bryce Adelstein Lelbach‚Äôs talk on C++ standard parallelism </a></li>
  <li><a href="https://github.com/kokkos/mdspan/blob/single-header/mdspan.hpp" target="blank"> Kokkos <code class="language-plaintext highlighter-rouge">mdspan</code> single header </a></li>
  <li><a href="https://www.nvidia.com/content/GTC-2010/pdfs/2131_GTC2010.pdf" target="blank">CUDA C Introduction Slides</a></li>
  <li><a href="https://github.com/uysalere/cuda-matrix-vector-multiplication" target="blank"> More sophisticated CUDA matrix-vector product implementations </a></li>
  <li><a href="https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf" target="blank"> Slides on CUDA reduction operation </a></li>
</ul>

<font size="-1">
  <em>
    These views do not in any way represent those of NVIDIA or any other organization or institution that I am professionally associated with.
    These views are entirely my own.
  </em>
</font>]]></content><author><name></name></author><category term="c, c++, cuda, GPU, HPC" /><summary type="html"><![CDATA[]]></summary></entry></feed>