<!DOCTYPE HTML>
<html lang="en" class="ayu" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>One Problem, Four Languages, Two Paradigms 10/19/2021 - Notes</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href=".././mdbook-admonish.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "coal" : "ayu";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('ayu')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../about.html">About</a></li><li class="chapter-item expanded affix "><li class="part-title">Blog</li><li class="chapter-item expanded "><a href="../csblog/2024-9-4-Debugging-In-Parallel.html">Debugging in Parallel 9/4/2024</a></li><li class="chapter-item expanded "><a href="../notes/2024-8-31-Linux-Perf-Notes.html">The Linux Perf Tool 8/31/2024</a></li><li class="chapter-item expanded "><a href="../notes/values.html">Values 8/26/2024</a></li><li class="chapter-item expanded "><a href="../notes/2024-8-30-Shell.html">Shell+Scripting 8/24/2024</a></li><li class="chapter-item expanded "><a href="../notes/editors.html">Editors+Tools 8/24/2024</a></li><li class="chapter-item expanded "><a href="../csblog/2023-6-1-C-VLA-Implementation.html">Understanding VLA 6/1/2023</a></li><li class="chapter-item expanded "><a href="../csblog/2022-5-2-BQN-reflections.html">BQN and Reflections on the Joy of Programming 5/2/2022</a></li><li class="chapter-item expanded "><a href="../csblog/2022-2-2-LLVM-Development-On-NixOS.html">LLVM Development on NixOS 2/2/2022</a></li><li class="chapter-item expanded "><a href="../csblog/2022-2-10-CUDA-101-Matvec.html">CUDA 101: Matrix-Vector Product 2/10/2022</a></li><li class="chapter-item expanded "><a href="../csblog/2022-12-12-Compiler-Perf-Debugging.html">Debugging Performance in Compilers 12/12/2022</a></li><li class="chapter-item expanded "><a href="../csblog/2022-1-15-Std-Expected.html">std::expected And Why It's Awesome 1/15/2022</a></li><li class="chapter-item expanded "><a href="../csblog/2021-3-7-GTest-Type-Value-Params.html">GTest Type and Value Parameterized Tests 3/7/2021</a></li><li class="chapter-item expanded "><a href="../csblog/2021-3-6-Spack-Development-3.html">Spack for Package Development Part 3 3/6/2021</a></li><li class="chapter-item expanded "><a href="../csblog/2021-3-6-Clang-Tools-Lambda.html">Clang Tools for Checking Domain-Specific Errors 3/6/2021</a></li><li class="chapter-item expanded "><a href="../csblog/2021-3-5-Spack-Development-2.html">Spack for Package Development Part 2 3/5/2021</a></li><li class="chapter-item expanded "><a href="../csblog/2021-3-4-Spack-Development-1.html">Spack for Package Development Part 1 3/4/2021</a></li><li class="chapter-item expanded "><a href="../csblog/2021-12-23-std-mdspan-Response.html">A Look at std::mdspan 12/23/2021</a></li><li class="chapter-item expanded "><a href="../csblog/2021-10-24-Popular-Languages-1965.html">Using the Most Popular Programming Languages of the '60s 10/24/2021</a></li><li class="chapter-item expanded "><a href="../csblog/2021-10-19-Leetcode-And-Distributed-Computing.html" class="active">One Problem, Four Languages, Two Paradigms 10/19/2021</a></li><li class="chapter-item expanded "><a href="../csblog/2021-10-11-BQN-Cpp-CUDA.html">BQN and CUDA C++ LeetCode Solutions 10/11/2021</a></li><li class="chapter-item expanded affix "><li class="part-title">Coffee</li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-11-Best-Espresso-In-Portland.html">Best Espresso In Portland (6/11/2023)</a></li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-22-Sterling.html">Sterling (6/22/2023)</a></li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-14-Deadstock.html">Deadstock (6/14/2023)</a></li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-22-Barista.html">Barista (6/22/2023)</a></li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-13-Never-Coffee.html">Never Coffee (6/13/2023)</a></li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-15-Upper-Left-Roasters.html">Upper Left Roasters (6/15/2023)</a></li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-14-Abba.html">Abba (6/14/2023)</a></li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-15-Rose-City-Coffee.html">Rose City Coffee (6/15/2023)</a></li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-14-Sterling.html">Sterling (6/14/2023)</a></li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-21-Superjoy.html">Superjoy (6/21/2023)</a></li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-13-Beginners-Guide.html">Beginners Guide (6/13/2023)</a></li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-19-Seattle-Trip-Report.html">Seattle Trip Report (6/19/2023)</a></li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-15-Adapt-Coffee.html">Adapt Coffee (6/15/2023)</a></li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-13-Coava.html">Coava (6/13/2023)</a></li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-14-PDX-Espresso-Research.html">PDX Espresso Research (6/14/2023)</a></li><li class="chapter-item expanded "><a href="../coffeeblog/2023-6-15-Nossa-Familia-Coffee.html">Nossa Familia Coffee (6/15/2023)</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <!--
layout: post
title: One Problem, Four Languages, Two Paradigms
permalink: /leetcode-distributed-computing
cat: cs
-->
<p>Solving a leetcode problem in four programming languages using two acceleration paradigms!</p>
<p><em>NOTE: This post is a transcript of <a href="https://youtu.be/Xk7-xjnEISE">the youtube video linked here</a>.</em></p>
<p>In addition to that, we’ll be using various combinations of two programming paradigms common in distributed computing: using a GPU to perform some calculations and MPI to distribute our calculation among multiple processes, potentially on multiple machines.</p>
<p>We’ll be looking at <a href="https://leetcode.com/problems/valid-sudoku/">this leetcode problem,</a> which is to determine if a 9 x 9 Sudoku board is valid, but not necessarily solvable.
Each row, column, and subbox of the grid must have the digits 1-9.</p>
<p>Let’s jump right in to our BQN solution.</p>
<h2 id="content"><a class="header" href="#content">Content</a></h2>
<ol>
<li><a href="#bqn">BQN</a></li>
<li><a href="#approach">Approach</a></li>
<li><a href="#python">Python</a></li>
<li><a href="#python-and-mpi">Python And MPI</a></li>
<li><a href="#c++">C++</a></li>
<li><a href="#c++-and-mpi">C++ And MPI</a></li>
<li><a href="#c++-and-cuda">C++ And CUDA</a></li>
<li><a href="#c++-and-cuda-and-mpi">C++ And CUDA And MPI</a></li>
<li><a href="#fortran">Fortran</a></li>
<li><a href="#fortran-and-mpi">Fortran And MPI</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#youtube-description">YouTube Video Description</a></li>
</ol>
<h2 id="bqn"><a class="header" href="#bqn"><a href="#content">BQN</a></a></h2>
<p>This is what our sudoku boards will look like:</p>
<pre><code># two 8s in the first block
bad ← ⟨8, 3, 0, 0, 7, 0, 0, 0, 0
       6, 0, 0, 1, 9, 5, 0, 0, 0
       0, 9, 8, 0, 0, 0, 0, 6, 0
       8, 0, 0, 0, 6, 0, 0, 0, 3
       4, 0, 0, 8, 0, 3, 0, 0, 1
       7, 0, 0, 0, 2, 0, 0, 0, 6
       0, 6, 0, 0, 0, 0, 2, 8, 0
       0, 0, 0, 4, 1, 9, 0, 0, 5
       0, 0, 0, 0, 8, 0, 0, 7, 9⟩

# valid sudoku
good ← ⟨5, 3, 0, 0, 7, 0, 0, 0, 0
        6, 0, 0, 1, 9, 5, 0, 0, 0
        0, 9, 8, 0, 0, 0, 0, 6, 0
        8, 0, 0, 0, 6, 0, 0, 0, 3
        4, 0, 0, 8, 0, 3, 0, 0, 1
        7, 0, 0, 0, 2, 0, 0, 0, 6
        0, 6, 0, 0, 0, 0, 2, 8, 0
        0, 0, 0, 4, 1, 9, 0, 0, 5
        0, 0, 0, 0, 8, 0, 0, 7, 9⟩

</code></pre>
<p>And here is our full solution.
This solution will be the basis for all of our later solutions.</p>
<pre><code>F ← {𝕊𝕩:
  Fl ← 0⊸≠⊸/                       # Filter 0s out
  Dup ← (∨´∾´)¬∘∊¨                 # Are there any duplicates?

  rs ← Dup Fl¨(9/↕9)⊔𝕩             # Check rows
  cs ← Dup Fl¨(81⥊↕9)⊔𝕩            # Check columns

  bi ← 27⥊3/↕3
  bs ← Dup Fl¨(bi∾(3+bi)∾(6+bi))⊔𝕩 # Check blocks

  (bs ∨ rs ∨ cs)⊑"true"‿"false"
}
</code></pre>
<p>This first line is a function to filter out any 0s:</p>
<pre><code>   Fl ← 0⊸≠⊸/
   Fl ⟨5, 3, 0, 0, 7, 0, 0, 0, 0⟩
⟨ 5 3 7 ⟩
</code></pre>
<p>Here we have another utility function to return an integer indicating whether any duplicates were found in any sublists:</p>
<pre><code>   Dup ← (∨´∾´)¬∘∊¨
   Dup ⟨⟨5, 3, 7⟩, ⟨1, 2, 3⟩⟩
0
   Dup ⟨⟨5, 3, 7⟩, ⟨1, 2, 2⟩⟩
1
</code></pre>
<p>Next we check for duplicates in all the filtered rows and columns:</p>
<pre><code>   rs ← Dup Fl¨(9/↕9)⊔𝕩
   cs ← Dup Fl¨(81⥊↕9)⊔𝕩
</code></pre>
<p>These ranges are used to create indices for grouping the values in X.
I’ll show a trimmed down version of their output here to give you an idea:</p>
<pre><code>   3‿3⥊(3/↕3) # For the rows
┌─       
╵ 0 0 0  
  1 1 1  
  2 2 2  
        ┘
   3‿3⥊(9⥊↕3) # For the columns
┌─       
╵ 0 1 2  
  0 1 2  
  0 1 2  
        ┘
</code></pre>
<p>Next I do something similar to get the indices for the boxes.</p>
<pre><code>   bi ← 27⥊3/↕3
   3‿9⥊bi
┌─                   
╵ 0 0 0 1 1 1 2 2 2  
  0 0 0 1 1 1 2 2 2  
  0 0 0 1 1 1 2 2 2  
                    ┘
</code></pre>
<p>This creats indices for the first three boxes, and you can probably imagine how to extend this to get the indices for all the boxes. I just add three to the previous indices, and then add six, and then append them all together. Here’s the second layer:</p>
<pre><code>   3‿9⥊bi+3
┌─                   
╵ 3 3 3 4 4 4 5 5 5  
  3 3 3 4 4 4 5 5 5  
  3 3 3 4 4 4 5 5 5  
                    ┘
</code></pre>
<p>And the final layer:</p>
<pre><code>   3‿9⥊bi+6
┌─                   
╵ 6 6 6 7 7 7 8 8 8  
  6 6 6 7 7 7 8 8 8  
  6 6 6 7 7 7 8 8 8  
                    ┘
</code></pre>
<p>And all three layers of indices stacked on top of each other:</p>
<pre><code>   9‿9⥊(bi∾(3+bi)∾(6+bi))
┌─                   
╵ 0 0 0 1 1 1 2 2 2  
  0 0 0 1 1 1 2 2 2  
  0 0 0 1 1 1 2 2 2  
  3 3 3 4 4 4 5 5 5  
  3 3 3 4 4 4 5 5 5  
  3 3 3 4 4 4 5 5 5  
  6 6 6 7 7 7 8 8 8  
  6 6 6 7 7 7 8 8 8  
  6 6 6 7 7 7 8 8 8  
                    ┘
</code></pre>
<p>Using these indices, I group all the elements of the input, and then check all of them for duplicates:</p>
<pre><code>   bs ← Dup Fl¨(bi∾(3+bi)∾(6+bi))⊔𝕩 # Check blocks
</code></pre>
<p>And in the end I check if there were duplicates in the blocks, in the rows, or in the columns, and use that to index into our strings that indicate whether our sudoku board is valid or not.</p>
<pre><code>   (bs ∨ rs ∨ cs)⊑"true"‿"false"
</code></pre>
<h2 id="approach"><a class="header" href="#approach"><a href="#content">Approach</a></a></h2>
<p>Before we move on to the Python solution, I’d like to talk about our approach to this solution in the rest of the languages, because they will all be pretty similar.</p>
<p>Just like in the BQN solution, we have three collections which represent the validity of the rows, another for the columns, and a third for the blocks.</p>
<p>Here I have a subset of a sudoku board on the bottom.</p>
<p><img src="/images/approach1.png" alt="Initial Row, Column, and Block Matrices" /></p>
<p>In our procedural languages, we’ll create an array thrice the size of the grid to hold these values.</p>
<p>Note that this is not as space (or time) efficient as many of the solutions that you can find on the discussion page for the leetcode problem, but it is much easier to parallelize and that’s really the point of this video.</p>
<p>Let’s now walk through a few steps of our algorithm starting at the second row and first column of our sudoku board, which relates to the second row of our “row matrix.”</p>
<p>Because we’re looking at our row matrix, we’ll take the row index in our sudoku board as the row for our row matrix, and we’ll take the value in the cell, in this case 6, as the column in our row matrix.
We’ll increment the value at this location in our row matrix, or in the first layer of our 3-d sum matrix that we’ll use to get our final answer.</p>
<p><img src="/images/approach2.png" alt="Checking Rows" /></p>
<p>Let’s move on to check the first row and second column of our sudoku board for our column matrix.
Because we’re looking at our column matrix, or the second layer of our final sum array, we’ll use the column index as the row index in our column matrix, and the value in that cell for the column index in our column matrix.</p>
<p>We’ll increment the value at this location in our column matrix, or in the second layer of our 3-d sum matrix that we’ll use to get our final answer.</p>
<p><img src="/images/approach3.png" alt="Checking Columns" /></p>
<p>Finally, let’s look at the first block in our sudoku board, which corresponds to the first row in our block matrix, and let’s look at the first cell in that block.
The value in the first cell in the first block is 8, so we’ll increment the first row and eighth column in our block matrix.</p>
<p><img src="/images/approach4.png" alt="Checking Blocks" /></p>
<p>If we then perform these three operations for every cell in the sudoku board, we’ll have a final matrix that indicates all the row-column-block-value combinations that we have, and if any cell in that final matrix has a value greater than one, then our board is invalid.</p>
<p>If we were then to check the final cell in the first block of our board, we would find that the eighth element of the first row of our block matrix would be incremented again, which would mean we have an invalid board!</p>
<p><img src="/images/approach5.png" alt="Checking Last Element of Block" /></p>
<p>If any value in our final array is greater than one, then we know we have at least one duplicate in at least one row, column, or block.</p>
<p>What’s neat about this solution is that no single operation depends on any other operation as long as we perform our operations atomically.
This way, our work can be performed on multiple machines or different devices, and as long as we synchronize at the end, our solution will be the same.</p>
<p>Now that we’ve talked strategy, let’s see what this looks like in our Python solution.</p>
<h2 id="python"><a class="header" href="#python"><a href="#content">Python</a></a></h2>
<p>Here’s our simple Python solution:</p>
<pre><code class="language-python">shape = 9
blksz = 3
def solve(board):
    ar = [[[0 for j in range(3)] for i in range(shape)] for k in range(shape)]
    for r in range(shape):
        for c in range(shape):
            v = board[r][c]
            if 0 == v:
                continue
            ar[r][v - 1][0] += 1
            ar[c][v - 1][1] += 1

            bi = (r // blksz) * blksz + (c // blksz)
            ar[bi][v - 1][2] += 1
    return max(max(i) for j in ar for i in j) &lt; 2
</code></pre>
<p>You can see here that we increment the value in the first layer of our full 3D matrix according to the row and the value in the cell currently being examined:</p>
<pre><code class="language-python">            ar[r][v - 1][0] += 1
</code></pre>
<p>We do the same for our column matrix:</p>
<pre><code class="language-python">            ar[c][v - 1][1] += 1
</code></pre>
<p>And finally for our block matrix, it just takes a little bit of math to figure out what our block index is.</p>
<pre><code class="language-python">            bx = r // blksz
            by = c // blksz
            bi = bx * blksz + by
            ar[bi][v - 1][2] += 1
</code></pre>
<p>I use this main function to run the python solution:</p>
<pre><code class="language-python">if __name__ == "__main__":
    for b in sudoku_9x9.boards():
        print(solve(b))
</code></pre>
<p>We run our example with two valid boards and two invalid boards and get the answers we expect:</p>
<pre><code class="language-console">$ python ./src/python/lc-valid-sudoku.py
True
True
False
False
</code></pre>
<h2 id="python-and-mpi"><a class="header" href="#python-and-mpi"><a href="#content">Python And MPI</a></a></h2>
<p>Now we’ll look at another python example, but this time one that uses MPI to distribute the calculations.</p>
<p>MPI provides a lot of infrastructure for distributed computing: using the <code>mpirun</code> command spawns N processes, each of which knows how many processes were spawned, what its unique process ID is, and some other relevant information.
These processes may be spawned on multiple machines even, and MPI gives us the tools to communicate between these processes.
We’ll take advantage of this infrastructure to perform our calculations on multiple processes.</p>
<pre><code class="language-python">import numpy as np
from mpi4py import MPI
shape = 9
blksz = 3
comm = MPI.COMM_WORLD
def solve(board, comm):
    ar = np.zeros((9, 9, 3), dtype=np.int64)
    chunk = (81 + comm.size - 1) // comm.size
    subscripts = (*itertools.product(range(9), range(9)),)
    for i in range(comm.rank * chunk, (comm.rank * chunk) + chunk):
        if i &gt;= 81:
            break
        r, c = subscripts[i]
        v = board[r][c]
        if 0 == v:
            continue
        ar[r][v - 1][0] += 1
        ar[c][v - 1][1] += 1
        bi = (r // blksz) * blksz + (c // blksz)
        ar[bi][v - 1][2] += 1
    gar = np.zeros((9 * 9 * 3,), dtype=np.int64)
    comm.Reduce([ar.flatten(), MPI.INT], [gar, MPI.INT], op=MPI.SUM, root=0)
    comm.Barrier()
    return max(gar.flatten()) &lt; 2 if 0 == comm.rank else False
</code></pre>
<p>This is what the setup looks like to get an MPI program running.</p>
<pre><code class="language-python">if __name__ == "__main__":
    if 0 == comm.rank:
        print("Running with size {0}".format(comm.size))

    for b in sudoku_9x9.boards():
        comm.Barrier()
        if 0 == comm.rank:
            ret = solve(b, comm)
            print(ret)
        else:
            solve(b, comm)
</code></pre>
<p>Here we chunk our work up based on how many processes we have:</p>
<pre><code class="language-python">    chunk = ((9 * 9) + comm.size - 1) // comm.size
</code></pre>
<p>Say we’re given 5 processes and we have 81 cells to check (because that’s the size of our sudoku board).
The calculation would look something like this:</p>
<p><code>chunk</code> is then the smallest amount of work for each process such that all the work that needs to be done is performed.
This is a common calculation that needs to be done in parallel computing.
Note that our final process may exit early if the work is not evenly divisible by the chunk size.</p>
<pre><code class="language-console">&gt;&gt;&gt; work = 81
&gt;&gt;&gt; size = 5
&gt;&gt;&gt; chunk = (work + size - 1) // size
&gt;&gt;&gt; chunk
17
&gt;&gt;&gt; chunk * size
85
</code></pre>
<p>We then generate all the possible combinations of rows and columns, and iterate over only the elements that fall within the chunk of work that belongs to our current MPI process.</p>
<pre><code class="language-python">    subscripts = (*itertools.product(range(9), range(9)),)
    for i in range(comm.rank * chunk, (comm.rank * chunk) + chunk):
        if i &gt;= work:
            break
        r, c = subscripts[i]
</code></pre>
<p>The rest of this code is exactly the same as our serial implementation:</p>
<pre><code class="language-python">        v = board[r][c]
        if 0 == v:
            continue
        ar[r][v - 1][0] += 1
        ar[c][v - 1][1] += 1
        bi = (r // blksz) * blksz + (c // blksz)
        ar[bi][v - 1][2] += 1
</code></pre>
<p>This next bit is more interesting.
We create a global array with the size we need to hold our final sum matrix, and we use the MPI function <code>Reduce</code>.
This function will perform the operation <code>op</code>, in this case <code>MPI.SUM</code>, to join the arrays <code>ar</code> and <code>gar</code> together on rank 0 specified by the <code>root</code> argument.
This means that our final summed matrix for all components of the solution is on the MPI process with rank 0.
We can then check if we have any cells with values greater than one, and return that value if we’re on rank 0.
Otherwise, we can just return false since no other rank has the final array.</p>
<pre><code class="language-python">    gar = np.zeros((9 * 9 * 3,), dtype=np.int64)
    comm.Reduce([ar.flatten(), MPI.INT], [gar, MPI.INT], op=MPI.SUM, root=0)
    comm.Barrier()
    return max(gar.flatten()) &lt; 2 if 0 == comm.rank else False
</code></pre>
<p>Here I run the example on 5 processes, and we see we get the same solution as with our serial example.</p>
<pre><code class="language-console">$ mpirun -n 5 python ./src/python/lc-valid-sudoku-mpi.py
Running with size 5
True
True
False
False
</code></pre>
<p>Now let’s move on to all our C++ solutions.</p>
<h2 id="c"><a class="header" href="#c"><a href="#content">C++</a></a></h2>
<p>All of our C++ solutions will use a board like this:</p>
<pre><code class="language-cpp">const auto board = std::array&lt;int, 81&gt;{
  5, 3, 0,  0, 7, 0,  0, 0, 0,
  6, 0, 0,  1, 9, 5,  0, 0, 0,
  0, 9, 8,  0, 0, 0,  0, 6, 0,
  
  8, 0, 0,  0, 6, 0,  0, 0, 3,
  4, 0, 0,  8, 0, 3,  0, 0, 1,
  7, 0, 0,  0, 2, 0,  0, 0, 6,
  
  0, 6, 0,  0, 0, 0,  2, 8, 0,
  0, 0, 0,  4, 1, 9,  0, 0, 5,
  0, 0, 0,  0, 8, 0,  0, 7, 9
};
</code></pre>
<p>Here’s our serial C++ solution.</p>
<pre><code class="language-cpp">auto isgood(const std::array&lt;int, 81&gt; &amp;board) -&gt; bool {
  auto ar = std::vector&lt;int&gt;(9 * 9 * 3, 0);
  for (int r = 0; r &lt; 9; r++)
    for (int c = 0; c &lt; 9; c++) {
      const auto v = board[idx2(r, c)];
      if (0 == v)
        continue;
      ar[idx3(r, v - 1, 0)] += 1;
      ar[idx3(c, v - 1, 1)] += 1;
      const auto bi = (r / blksz) * blksz + (c / blksz);
      ar[idx3(bi, v - 1, 2)] += 1;
    }
  const auto m =
    std::accumulate(ar.begin(), ar.end(), -1, max);
  return m &lt; 2;
}
</code></pre>
<p>You can see pretty much everything about our solution is the same so far.
You may notice the <code>idx2</code> and <code>idx3</code> functions - these just calculate the linear index from subscripts so we can almost use 2d and 3d subscripts while keeping our arrays totally linear.</p>
<p>Here’s our <code>idx2</code> function for example. I’ll be using these functions for the rest of my solutoins since they make the code much more readable.</p>
<pre><code class="language-cpp">int idx2(int r, int c) {
  return (r * 9) + c;
};
</code></pre>
<p>Here at the end I find the max value again and check to make sure it’s less than two:</p>
<pre><code class="language-cpp">  const auto m = 
    std::accumulate(ar.begin(), ar.end(), -1, max);
  return m &lt; 2;
</code></pre>
<p>Running our executable gives us the same answers as our previous implementations:</p>
<pre><code class="language-console">$ ./src/cpp/lc-valid-sudoku
true
true
false
false
</code></pre>
<h2 id="c-and-mpi"><a class="header" href="#c-and-mpi"><a href="#content">C++ And MPI</a></a></h2>
<p>Here we have our MPI distributed C++ solution, let’s walk through it in a few steps.</p>
<pre><code class="language-cpp">auto isgood(const std::array&lt;int, 81&gt; &amp;board, int rank, int size) -&gt; bool {
  const auto chunk = (81 + size - 1) / size;
  auto ar = std::vector&lt;int&gt;(81 * 3, 0);
  auto indices = std::vector&lt;std::pair&lt;int, int&gt;&gt;(81 * size, std::make_pair(-1, -1));
  for (int r = 0; r &lt; 9; r++)
    for (int c = 0; c &lt; 9; c++)
      indices[idx2(r, c)] = std::make_pair(r, c);
  for (std::size_t i = chunk * rank; i &lt; chunk + (chunk * rank); i++) {
    const auto &amp;[r, c] = indices[i];
    const auto v = board[idx2(r, c)];
    if (r &lt; 0 or 0 == v) continue;
    ar[idx3(r, v - 1, 0)] += 1;
    ar[idx3(c, v - 1, 1)] += 1;
    const auto bi = (r / blksz) * blksz + (c / blksz);
    ar[idx3(bi, v - 1, 2)] += 1;
  }
  std::vector&lt;int&gt; gar(9 * 9 * 3, 0);
  MPI_Reduce(ar.data(), gar.data(), gar.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
  return 0 == rank ? std::accumulate(gar.begin(), gar.end(), -1, std::max) &lt; 2
                   : false;
}
</code></pre>
<p>All the setup is the same between the last several solutions.</p>
<p>Astute viewers may recognize this as a cartesian product, but I couldn’t find a nice way to do this with the STL algorithms.
If any viewers know of a nicer way to generate the cartesian product of two containers, please let me know.</p>
<pre><code class="language-cpp">  for (int r = 0; r &lt; 9; r++)
    for (int c = 0; c &lt; 9; c++)
      indices[idx2(r, c)] = std::make_pair(r, c);
</code></pre>
<p>The core loop is much the same as our other solutions, aside from unpacking the row and column as a tuple.</p>
<pre><code class="language-cpp">  for (std::size_t i = chunk * rank; i &lt; chunk + (chunk * rank); i++) {
    const auto &amp;[r, c] = indices[i];
    const auto v = board[idx2(r, c)];
    if (r &lt; 0 or 0 == v)
      continue;
    ar[idx3(r, v - 1, 0)] += 1;
    ar[idx3(c, v - 1, 1)] += 1;
    const auto bi = (r / 3) * 3 + (c / 3);
    ar[idx3(bi, v - 1, 2)] += 1;
  }
</code></pre>
<p>This section is exactly equivilant to the Python version below.
This should give you an idea of what it’s like to use the raw C and Fortran interfaces to MPI.</p>
<pre><code class="language-cpp">  std::vector&lt;int&gt; gar(9 * 9 * 3, 0);
  MPI_Reduce(ar.data(), gar.data(), gar.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
  return 0 == rank ? std::accumulate(gar.begin(), gar.end(), -1, sb::max) &lt; 2
                   : false;
</code></pre>
<p>Python version:</p>
<pre><code class="language-python">    gar = np.zeros((9 * 9 * 3,), dtype=np.int64)
    comm.Reduce([ar.flatten(), MPI.INT], [gar, MPI.INT], op=MPI.SUM, root=0)
    comm.Barrier()
    return max(gar.flatten()) &lt; 2 if 0 == comm.rank else False
</code></pre>
<p>In my main function I iterate over the same boards and use some extra logic so we only see the results that rank 0 gave back:</p>
<pre><code class="language-cpp">int main(int argc, char **argv) {
  int size, rank;
  MPI_Init(&amp;argc, &amp;argv);
  MPI_Comm comm = MPI_COMM_WORLD;
  MPI_Comm_size(comm, &amp;size);
  MPI_Comm_rank(comm, &amp;rank);
  for (const auto &amp;board : all_boards) {
    bool ret;
    if (0 == rank) {
      ret = isgood(board, rank, size);
      std::cout &lt;&lt; bool2str(ret) &lt;&lt; "\n";
    } else isgood(board, rank, size);
    MPI_Barrier(comm);
  }
  MPI_Finalize();
  return 0;
}
</code></pre>
<p>Running this works just as all our previous solutions did:</p>
<pre><code class="language-console">$ mpirun -n 5 ./src/cpp/lc-valid-sudoku-mpi
true
true
false
false
</code></pre>
<p>We’ll now take a look at our CUDA-enabled solution.</p>
<h2 id="c-and-cuda"><a class="header" href="#c-and-cuda"><a href="#content">C++ And CUDA</a></a></h2>
<p>Here’s our single-process CUDA implementation.
I for the most part am using raw CUDA, but I use a few helper methods from Thrust as well, such as the type-safe device malloc and free and some pointer-casting methods.
For those that are unfamiliar, the funny-looking function calls with the triple braces are how you launch a raw cuda kernel.
These allow you to pass arguments to the CUDA runtime to let it know how you’d like your CUDA kernel to be launched.</p>
<pre><code class="language-cpp">auto isgood(const Board &amp;board) -&gt; bool {
  auto d_ar = device_malloc&lt;int&gt;(81 * 3);
  setar&lt;&lt;&lt;1, dim3(9, 9)&gt;&gt;&gt;(
      raw_pointer_cast(d_ar), 
      raw_pointer_cast((thrust::device_vector&lt;int&gt;(board.begin(), board.end())).data()));
  cudaDeviceSynchronize();
  const auto m = thrust::reduce(d_ar, d_ar+(81*3), -1, thrust::maximum&lt;int&gt;());
  device_free(d_ar);
  return m &lt; 2;
}
</code></pre>
<p>I have the following <code>using</code> statements to make the code a little more readable hopefully.</p>
<pre><code class="language-cpp">using thrust::device_malloc;
using thrust::device_free;
using thrust::device_vector;
using thrust::host_vector;
using thrust::raw_pointer_cast;
</code></pre>
<p>Along with the previous code that should look pretty familiar at this point, I define two other CUDA kernels.
The first is this short <code>setrc</code> kernel, which sets rows and columns based on the kernel launch parameters I pass.
This is a shortcut for a cartesian product of the rows and columns that runs on the GPU.</p>
<pre><code class="language-cpp">__global__ void setrc(int *rows, int *cols) {
  const int r = threadIdx.x, c = threadIdx.y;
  rows[idx2(r, c)] = r;
  cols[idx2(r, c)] = c;
}
</code></pre>
<p>The other kernel is this <code>setar</code> function, which is the same core kernel that’s been at the heart of all of our solutions so far.</p>
<pre><code class="language-cpp">__global__ void setar(int *ar, const int *board) {
  const auto row = threadIdx.x, col = threadIdx.y;
  const int value = board[idx2(row, col)];
  if (0 == value) return;
  atomicAdd(&amp;ar[idx3(row, value, 0)], 1);
  atomicAdd(&amp;ar[idx3(col, value, 1)], 1);
  const int bi = (row / blksz) * blksz + (col / blksz);
  atomicAdd(&amp;ar[idx3(bi, value, 2)], 1);
}
</code></pre>
<p>Outside of those two kernels, the solution should look pretty familiar at this point.
We allocate our final array and pass it to our cuda kernel, along with the sudoku board after copying it to the GPU.</p>
<pre><code class="language-cpp">  auto d_ar = device_malloc&lt;int&gt;(81 * 3);
  setar&lt;&lt;&lt;1, dim3(9, 9)&gt;&gt;&gt;(
    raw_pointer_cast(d_ar),
    raw_pointer_cast(
      (device_vector&lt;int&gt;(board.begin(), board.end())).data()
    )
  );
</code></pre>
<p>We then syncronize with our GPU to make sure the kernel finishes before reducing to find the maximum value with <code>thrust::reduce</code>, freeing our device memory, and returning whether all values fell below two.</p>
<pre><code class="language-cpp">  cudaDeviceSynchronize();
  const auto m = thrust::reduce(d_ar, d_ar+(81*3), -1, thrust::maximum&lt;int&gt;());
  device_free(d_ar);
  return m &lt; 2;
</code></pre>
<p>Let’s move on to our most complex example, the C++ CUDA-enabled, MPI-distributed implementation.</p>
<h2 id="c-and-cuda-and-mpi"><a class="header" href="#c-and-cuda-and-mpi"><a href="#content">C++ And CUDA And MPI</a></a></h2>
<p>Now that we’re using two extra paradigms, CUDA GPU device offloading and MPI distributed computing, our code is looking more noisy.
It’s still pretty much the same solution as our non-distributed CUDA solution though.</p>
<pre><code class="language-cpp">auto isgood(const Board &amp;board, int rank, int size) -&gt; bool {
  const auto chunk = (81 + size - 1) / size;
  const auto rows = device_malloc&lt;int&gt;(chunk * size),
             cols = device_malloc&lt;int&gt;(chunk * size);
  thrust::fill(rows, rows + (chunk * size), -1);
  setrc&lt;&lt;&lt;1, dim3(9, 9)&gt;&gt;&gt;(raw_pointer_cast(rows), raw_pointer_cast(cols));
  auto d_ar = device_malloc&lt;int&gt;(81 * 3);
  thrust::fill(d_ar, d_ar + (81 * 3), 0);
  setar&lt;&lt;&lt;1, chunk&gt;&gt;&gt;(
      raw_pointer_cast(d_ar),
      raw_pointer_cast((device_vector&lt;int&gt;(board.begin(), board.end())).data()),
      raw_pointer_cast(rows), raw_pointer_cast(cols), chunk * rank);
  cudaDeviceSynchronize();
  auto h_ar = host_vector&lt;int&gt;(d_ar, d_ar + (81 * 3));
  auto gar = host_vector&lt;int&gt;(81 * 3, 0);
  MPI_Reduce(h_ar.data(), gar.data(), gar.size(), MPI_INT, MPI_SUM, 0,
             MPI_COMM_WORLD);
  device_free(rows); device_free(cols); device_free(d_ar);
  if (rank &gt; 0)
    return false;
  const auto m = thrust::reduce(thrust::host, gar.begin(), gar.end(), -1,
                                thrust::maximum&lt;int&gt;());
  return m &lt; 2;
}
</code></pre>
<p>The <code>setar</code> kernel is a bit different from our non distributed CUDA solution since we’re only operating on a subset of our sudoku board.
We set the values in our final sum matrix for the row, column and block submatrices just like before.
This time however, we’re given this <code>offset</code> parameter.
This is because we’re not just running CUDA kernels, we’re running CUDA kernels on multiple processes and potentially multiple machines, so we’re only performing a subset of the full set of operations.
This offset parameter tells us where we should start relative to the entire set of operations.
We’re also not using the builtin <code>threadIdx.y</code> value since we’re launching our kernel in a 1D grid with precalculated row and column indices instead of a 2D grid.</p>
<pre><code class="language-cpp">__global__ void setar(int *ar, const int *board, const int *rows,
                      const int *cols, const int offset) {
  const auto i = offset + threadIdx.x;
  const int r = rows[i], c = cols[i];
  const int value = board[idx2(r, c)];
  if (r &lt; 0 || 0 == value)
    return;
  atomicAdd(&amp;ar[idx3(r, value, 0)], 1);
  atomicAdd(&amp;ar[idx3(c, value, 1)], 1);
  const int bi = (r / blksz) * blksz + (c / blksz);
  atomicAdd(&amp;ar[idx3(bi, value, 2)], 1);
}
</code></pre>
<p>If we return to the start of our top-level function, you’ll see that we calculate the work that should be performed on this MPI process.
We also set up our row and column indices using our cartesian product kernel.</p>
<pre><code class="language-cpp">  const auto chunk = (81 + size - 1) / size;
  const auto rows = device_malloc&lt;int&gt;(chunk * size),
             cols = device_malloc&lt;int&gt;(chunk * size);
  thrust::fill(rows, rows + (chunk * size), -1);
  setrc&lt;&lt;&lt;1, dim3(9, 9)&gt;&gt;&gt;(raw_pointer_cast(rows), raw_pointer_cast(cols));
</code></pre>
<p>We then set up our final sum matrix on the device:</p>
<pre><code class="language-cpp">  auto d_ar = device_malloc&lt;int&gt;(81 * 3);
  thrust::fill(d_ar, d_ar + (81 * 3), 0);
</code></pre>
<p>And then launch our core kernel to perform the operations assigned to the current rank:</p>
<pre><code class="language-cpp">  setar&lt;&lt;&lt;1, chunk&gt;&gt;&gt;(
      raw_pointer_cast(d_ar),
      raw_pointer_cast((device_vector&lt;int&gt;(board.begin(), board.end())).data()),
      raw_pointer_cast(rows), raw_pointer_cast(cols), chunk * rank);
</code></pre>
<p>We syncronize with our GPU device and copy the data to a host vector before reducing the final sum array across all of our ranks using MPI.
Note that if we used a GPU-enabled MPI provider we could send the data on the device directly to another system’s GPU without copying the memory to the host, but this has other complications so I kept it simple for this example.</p>
<pre><code class="language-cpp">  cudaDeviceSynchronize();
  auto h_ar = host_vector&lt;int&gt;(d_ar, d_ar + (81 * 3));
  auto gar = host_vector&lt;int&gt;(81 * 3, 0);
  MPI_Reduce(h_ar.data(), gar.data(), gar.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
</code></pre>
<p>And then we perform our final reduction on our root rank to see if we have any cells with values greater than 1.
We could perform this reduction on the device, but it’s probably not worth it to copy the data back to the device for just one operation.</p>
<pre><code class="language-cpp">  if (rank &gt; 0)
    return false;
  const auto m = thrust::reduce(thrust::host, gar.begin(), gar.end(), -1,
                                thrust::maximum&lt;int&gt;());
  return m &lt; 2;
</code></pre>
<p>And there we have it, our sudoku validator is running on multiple processes and using GPUs.</p>
<pre><code class="language-console">$ mpirun -n 7 ./src/thrust/lc-valid-sudoku-mpi-thrust
true
true
false
false
</code></pre>
<p>Now let’s move on to Fortran.</p>
<h2 id="fortran"><a class="header" href="#fortran"><a href="#content">Fortran</a></a></h2>
<p>You’re likely not surprised that this looks a lot like our previous solutions.</p>
<pre><code class="language-fortran">subroutine isgood(board, ret)
  implicit none
  integer, dimension(0:(shape*shape)-1), intent(in) :: board
  logical, intent(out) :: ret
  integer, dimension(0:(shape * shape * 3)-1) :: ar
  integer :: v, row, col, i, bx, by
  ar = 0
  do row = 0, shape-1
    do col = 0, shape-1
      v = board(idx2(row, col))
      if (v .eq. 0) cycle
      ar(idx3(row, v-1, 0)) = ar(idx3(row, v-1, 0)) + 1      
      ar(idx3(col, v-1, 1)) = ar(idx3(col, v-1, 1)) + 1
      ar(idx3(bi(row, col), v-1, 2)) = ar(idx3(bi(row, col), v-1, 2)) + 1
    end do
  end do
  v = maxval(ar) - 1
  ret = (v .lt. 1)
end subroutine isgood
</code></pre>
<p>If I clear away the declarations and initializations, this looks fairly readable.
You may notice that I have to repeat myself a few times because there’s not a really nice way to incremenet a value in fortran.</p>
<pre><code class="language-fortran">subroutine isgood(board, ret)
  do row = 0, shape-1
    do col = 0, shape-1
      v = board(idx2(row, col))
      if (v .eq. 0) cycle
      ar(idx3(row, v-1, 0)) = ar(idx3(row, v-1, 0)) + 1      
      ar(idx3(col, v-1, 1)) = ar(idx3(col, v-1, 1)) + 1
      ar(idx3(bi(row, col), v-1, 2)) = ar(idx3(bi(row, col), v-1, 2)) + 1
    end do
  end do
  v = maxval(ar) - 1
  ret = (v .lt. 1)
end subroutine isgood
</code></pre>
<p>Now we move on to the MPI-distributed Fortran implementation.
This solution is pretty long so I’ll break the function into a few slides like a sliding window.</p>
<h2 id="fortran-and-mpi"><a class="header" href="#fortran-and-mpi"><a href="#content">Fortran And MPI</a></a></h2>
<p>Here is the full solution.</p>
<pre><code class="language-fortran">subroutine isgood(board, ret)
  use mpi
  implicit none
  integer, dimension(shape*shape), intent(in) :: board
  logical, intent(out) :: ret
  integer, dimension(shape * shape * 3) :: ar, gar
  integer, dimension(shape * shape) :: rows, cols
  integer :: v, row, col, i, chunk, rank, size, ierr

  ar = 0
  gar = 0

  call MPI_Comm_rank(MPI_COMM_WORLD, rank, ierr)
  call MPI_Comm_size(MPI_COMM_WORLD, size, ierr)

  do row = 0, shape-1
    do col = 0, shape-1
      rows(1+idx2(row, col)) = row
      cols(1+idx2(row, col)) = col
    end do
  end do

  chunk = ((shape*shape) + size - 1) / size

  do i = 1+(rank*chunk), (rank*chunk)+chunk
    if (i .gt. (shape*shape)) exit
    row = rows(i)
    col = cols(i)
    v = board(1+idx2(row, col))
    if (v .eq. 0) cycle
    ar(idx3(row, v-1, 0)+1) = ar(idx3(row, v-1, 0)+1) + 1      
    ar(idx3(col, v-1, 1)+1) = ar(idx3(col, v-1, 1)+1) + 1
    ar(idx3(bi(row, col), v-1, 2)+1) = ar(idx3(bi(row, col), v-1, 2)+1) + 1
  end do
  
  call MPI_Reduce(ar, gar, 3*shape*shape, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD, ierr)
  call MPI_Barrier(MPI_COMM_WORLD, ierr)

  if (0 .eq. rank) then
    v = maxval(gar) - 1
    ret = (v .lt. 1)
  else
    ret = .false.
  end if

end subroutine isgood
</code></pre>
<p>Let’s trim away the declarations and initializations again:</p>
<pre><code class="language-fortran">subroutine isgood(board, ret)
  do row = 0, 8
    do col = 0, 8
      rows(1+idx2(row, col)) = row
      cols(1+idx2(row, col)) = col
    end do
  end do
  chunk = (81 + size - 1) / size
  do i = 1+(rank*chunk), (rank*chunk)+chunk
    if (i .gt. 81) exit
    row = rows(i)
    col = cols(i)
    v = board(1+idx2(row, col))
    if (v .eq. 0) return
    ar(idx3(row, v-1, 0)+1) = ar(idx3(row, v-1, 0)+1) + 1      
    ar(idx3(col, v-1, 1)+1) = ar(idx3(col, v-1, 1)+1) + 1
    ar(idx3(bi(row, col), v-1, 2)+1) = ar(idx3(bi(row, col), v-1, 2)+1) + 1
  end do
  call MPI_Reduce(ar, gar, 3*81, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD, ierr)
  call MPI_Barrier(MPI_COMM_WORLD, ierr)
  if (0 .eq. rank) then
    v = maxval(gar) - 1
    ret = (v .lt. 1)
  else
    ret = .false.
  end if
end subroutine isgood
</code></pre>
<p>You’ll notice that I create row and column arrays again because this makes distributing the processes much simpler.</p>
<pre><code class="language-fortran">  do row = 0, 8
    do col = 0, 8
      rows(1+idx2(row, col)) = row
      cols(1+idx2(row, col)) = col
    end do
  end do
</code></pre>
<p>The core loop is the same as the other distributed solutions.
I work only on the rows and columns assigned to the current rank.</p>
<pre><code class="language-fortran">  do i = 1+(rank*chunk), (rank*chunk)+chunk
    if (i .gt. 81) exit
    row = rows(i)
    col = cols(i)
    v = board(1+idx2(row, col))
    if (v .eq. 0) return
    ar(idx3(row, v-1, 0)+1) = ar(idx3(row, v-1, 0)+1) + 1      
    ar(idx3(col, v-1, 1)+1) = ar(idx3(col, v-1, 1)+1) + 1
    ar(idx3(bi(row, col), v-1, 2)+1) = ar(idx3(bi(row, col), v-1, 2)+1) + 1
  end do
</code></pre>
<p>We reduce the solution across all of our ranks to get the full array on rank 0.
We then perform our max reduce to get our answer and we return!</p>
<pre><code class="language-fortran">  call MPI_Reduce(ar, gar, 3*81, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD, ierr)
  call MPI_Barrier(MPI_COMM_WORLD, ierr)
  if (0 .eq. rank) then
    v = maxval(gar)
    ret = (v .lt. 2)
  else
    ret = .false.
  end if
</code></pre>
<p>Running this gives us the answers we expect.</p>
<pre><code class="language-console">$ mpirun -n 7 ./src/fortran/lc-valid-sudoku-ftn-mpi
 Running with world size of           7
 T
 T
 F
 F
</code></pre>
<h2 id="conclusion"><a class="header" href="#conclusion"><a href="#content">Conclusion</a></a></h2>
<p>I hope you’ve all enjoyed this video and the foray into distributed computing in a few different programming languages.</p>
<p>{% include footer.html %}</p>
<h2 id="youtube-description"><a class="header" href="#youtube-description"><a href="#content">YouTube Description</a></a></h2>
<p>We solve a Leetcode problem in four languages using various combinations of MPI and CUDA!</p>
<ul>
<li>
<p>0:00 Problem Introduction</p>
</li>
<li>
<p>0:36 BQN Solution</p>
</li>
<li>
<p>2:07 Solution Strategy</p>
</li>
<li>
<p>4:54 Python Solution</p>
</li>
<li>
<p>5:42 Python &amp; MPI Solution</p>
</li>
<li>
<p>8:01 C++ Solution</p>
</li>
<li>
<p>8:55 C++ &amp; MPI Solution</p>
</li>
<li>
<p>9:58 C++ &amp; CUDA Solution</p>
</li>
<li>
<p>11:24 C++ &amp; MPI &amp; CUDA Solution</p>
</li>
<li>
<p>13:31 Fortran Solution</p>
</li>
<li>
<p>13:55 Fortran &amp; MPI Solution</p>
</li>
<li>
<p>14:38 Conclusion</p>
</li>
<li>
<p>Written version: http://www.ashermancinelli.com/leetcode-distributed-computing</p>
</li>
<li>
<p>LinkedIn: https://www.linkedin.com/in/asher-mancinelli-bb4a56144/</p>
</li>
<li>
<p>GitHub Repo for Examples: https://github.com/ashermancinelli/algorithm-testbed</p>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../csblog/2021-10-24-Popular-Languages-1965.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../csblog/2021-10-11-BQN-Cpp-CUDA.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../csblog/2021-10-24-Popular-Languages-1965.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../csblog/2021-10-11-BQN-Cpp-CUDA.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
