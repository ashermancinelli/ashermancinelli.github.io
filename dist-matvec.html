<!DOCTYPE html>
<html>
  <head>
    <title>HPC 101: Matrix-Vector Product – Asher Mancinelli – HPC Software Development</title>

        <meta charset="utf-8">
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

    
    <meta name="description" content="





">
    <meta property="og:description" content="





">
    
    <meta name="author" content="Asher Mancinelli">

    
    <meta property="og:title" content="HPC 101: Matrix-Vector Product">
    <meta property="twitter:title" content="HPC 101: Matrix-Vector Product">
    

    <link rel="icon" href="/images/chip.png">


    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css">
    <link rel="alternate" type="application/rss+xml" title="Asher Mancinelli - HPC Software Development" href="/feed.xml">

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="https://raw.githubusercontent.com/ashermancinelli/ashermancinelli.github.io/master/images/chip.png"></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">Asher Mancinelli</a></h1>
            <p class="site-description">HPC Software Development</p>
          </div>

          <nav>
            <a href="/">Blog</a>
            <a href="/about">About</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>HPC 101: Matrix-Vector Product</h1>

  <div class="entry">
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>

<script async="" src="https://unpkg.com/mermaid@8.2.3/dist/mermaid.min.js"></script>

<!---

  To use:
  <div class="mermaid">
    graph LR
      ...
  </div>

  --->

<p>The backbone of all scientific computing is linear algebra, often distributed and often using acceleration hardware and software such as OpenMP, CUDA, etc.
This post takes you from a basic example problem to a GPU-accelerated example that calculates a matrix-vector product.</p>

<font size="-1">
  <em>
    These views do not in any way represent those of Pacific Northwest National Laboratory, the US Department of Energy, Battelle, or any other organization or institution that I, Asher Mancinelli, am professionally associated with.
    These views are entirely my own.
  </em>
</font>

<h1 id="key-takeaways">Key Takeaways</h1>

<p>These are the key takeaways from this post.
If you don’t read the entire post, at least take these points:</p>

<ol>
  <li>Correctness precedes parallelism and performance</li>
  <li>Identify and understand the underlying algorithms at play</li>
  <li>Speed is not the same as efficiency</li>
</ol>

<p><strong><em>NOTE: This post is geared towards those without significant experience in linear algebra, high performance computing, and GPU programming.</em></strong></p>

<h1 id="outline">Outline</h1>

<ol>
  <li>Mathematical Understanding of Algorithm
    <ol>
      <li>Short Example in Python</li>
    </ol>
  </li>
  <li>Example in C on Host</li>
  <li>Example in CUDA C</li>
  <li>Example in CUDA C++</li>
  <li>Example in BQN</li>
  <li>Links, References, Additional Reading</li>
</ol>

<h1 id="mathematical-understanding-of-algorithm">Mathematical Understanding of Algorithm</h1>

<p>We’ll be performing a matrix-vector dot product several ways in this post.
I may refer to the operation as <code class="language-plaintext highlighter-rouge">dgemv</code> as that’s the official name for a matrix-vector product on data of type <code class="language-plaintext highlighter-rouge">double</code> in the BLAS interface, though we will not make any attempt to make our code conformant with BLAS - it’s just a helpful shorthand.</p>

<p>The operation is depicted below:</p>

<center>
<img src="/images/hpc-101-matvec/matvec.png" alt="Matvec dot product, credit this post: https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.2-Multiplying-Matrices-and-Vectors/">
</center>

<p>Let <code class="language-plaintext highlighter-rouge">p</code> be the result of the dot product of matrix <code class="language-plaintext highlighter-rouge">Mat</code> and vector <code class="language-plaintext highlighter-rouge">v</code>.
The dot product is calculated like so:</p>

<center>
$$
\\
  p \gets Mat \cdot v

  \\

  =

  v_0 \cdot
  \left[ {\begin{array}{c}
    Mat_{0, 0} \\
    Mat_{1, 0} \\
    Mat_{2, 0} \\
  \end{array} } \right]
  +
  v_1 \cdot
  \left[ {\begin{array}{c}
    Mat_{0, 1} \\
    Mat_{1, 1} \\
    Mat_{2, 1} \\
  \end{array} } \right]
  +
  v_2 \cdot
  \left[ {\begin{array}{c}
    Mat_{0, 2} \\
    Mat_{1, 2} \\
    Mat_{2, 2} \\
  \end{array} } \right]

  \\

  =

  \left[ {\begin{array}{cc}
    (Mat_{0,0} \cdot v_0) + (Mat_{0,1} \cdot v_1) + (Mat_{0,2} \cdot v_2) \\
    (Mat_{1,0} \cdot v_0) + (Mat_{1,1} \cdot v_1) + (Mat_{1,2} \cdot v_2) \\
    (Mat_{2,0} \cdot v_0) + (Mat_{2,1} \cdot v_1) + (Mat_{2,2} \cdot v_2) \\
  \end{array} } \right]
\\
$$
<!---
  =   \left[ {\begin{array}{cc}
    6 \\ 24 \\ 42
  \end{array} } \right]
  --->
</center>

<p>Notice how values of <code class="language-plaintext highlighter-rouge">v</code> are broadcast to match the shape of <code class="language-plaintext highlighter-rouge">Mat</code>:</p>

<center>
$$
\\
  \left[ {\begin{array}{c}
    v_{0} &amp; v_{1} &amp; \cdots &amp; v_{n}\\
    v_{0} &amp; v_{1} &amp; \cdots &amp; v_{n}\\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    v_{0} &amp; v_{1} &amp; \cdots &amp; v_{n}\\
  \end{array} } \right]
\\
$$
</center>

<p>We can broadcast values of <code class="language-plaintext highlighter-rouge">v</code> into columns of a matrix with the same shape as the matrix <code class="language-plaintext highlighter-rouge">Mat</code>, and then pair the <code class="language-plaintext highlighter-rouge">Mat</code> and <code class="language-plaintext highlighter-rouge">v</code> element-wise, creating a matrix of tuples (or a 3d matrix if you prefer):</p>

<center>
$$
\\
  tuplespace \gets
  \left[ {\begin{array}{cc}
    (Mat_{0,0}, v_0) &amp; (Mat_{0,1}, v_1) &amp; (Mat_{0,2}, v_2) \\
    (Mat_{1,0}, v_0) &amp; (Mat_{1,1}, v_1) &amp; (Mat_{1,2}, v_2) \\
    (Mat_{2,0}, v_0) &amp; (Mat_{2,1}, v_1) &amp; (Mat_{2,2}, v_2) \\
  \end{array} } \right]
\\
$$
</center>

<p>This is sometimes called a <em>tuple space</em>, or the <em>domain</em> of our algorithm.
The book <a href="https://www.worldcat.org/title/how-to-write-parallel-programs-a-first-course/oclc/912171709&amp;referer=brief_results" target="blank"><em>How to Write Parallel Programs: A First Course</em></a> covers tuple spaces in great detail.</p>

<p>Now that we have constructed our tuple space, we might group our computations into self-contained units of work along each row.</p>

<p>Partitioning our tuple space row-wise gives:</p>
<center>
$$
\\
  tuplespace \gets
  \left[ {\begin{array}{ccc}
    (Mat_{0,0}, v_0) &amp; (Mat_{0,1}, v_1) &amp; (Mat_{0,2}, v_2) \\
    \hline \\
    (Mat_{1,0}, v_0) &amp; (Mat_{1,1}, v_1) &amp; (Mat_{1,2}, v_2) \\
    \hline \\
    (Mat_{2,0}, v_0) &amp; (Mat_{2,1}, v_1) &amp; (Mat_{2,2}, v_2) \\
  \end{array} } \right]
\\
$$
</center>

<p>Let <em>tuplespace</em> be the 2 dimensional matrix tuple space given above.
We then may form a vector with units of work yielding indices of the output vector:</p>

<center>
$$
\\
  \left[ {\begin{array}{cccc}
    w(0) \gets \sum_{i \gets 0}^{N} tuplespace_{0, i, 0} \cdot tuplespace_{0, i, 1} \\
    w(1) \gets \sum_{i \gets 0}^{N} tuplespace_{1, i, 0} \cdot tuplespace_{1, i, 1} \\
    \vdots \\
    w(M) \gets \sum_{i \gets 0}^{N} tuplespace_{M, i, 0} \cdot tuplespace_{M, i, 1} \\
  \end{array} } \right]
\\
$$
</center>

<p>Equivalently:</p>

<center>
$$
\\
  \left[ {\begin{array}{cccc}
    w(0) \gets \sum_{i \gets 0}^{N} Mat_{0,i} \cdot v_{i} \\
    w(1) \gets \sum_{i \gets 0}^{N} Mat_{1,i} \cdot v_{i} \\
    \vdots \\
    w(M) \gets \sum_{i \gets 0}^{N} Mat_{M,i} \cdot v_{i} \\
  \end{array} } \right]
\\
$$
</center>

<p>Our units of work may independently operate on subsets (rows) of our tuple space.</p>

<h1 id="algorithm-analysis">Algorithm Analysis</h1>

<p>The first question we must ask ourselves when parallelizing code is this: <em>are any iterations of the algorithm dependent on values calculated in other iterations? Is iteration <code class="language-plaintext highlighter-rouge">N</code> dependent on calculations in iteration <code class="language-plaintext highlighter-rouge">N-1</code>?</em>
In other words, <em>are the loop bodies entirely</em> <strong><em>independent</em></strong> <em>of each other?</em></p>

<p>If so, our algorithm is <em>loop independent</em> and <em>trivially parallelizable</em>.
<a href="https://www.cs.utexas.edu/~lin/cs380c/handout27.pdf" target="blank">This slidedeck from a UT Austin lecture</a> are helpful additional reading on this topic.</p>

<p>The fundamental algorithm at play here is a <em>reduction</em> or a <em>fold</em>.
If you see these terms elsewhere in literature, documentation, or algorithms in libraries or programming languages, they almost certainly mean the same thing.
Some collection of values are <em>reduced</em> or <em>folded</em> into a single value.</p>

<p>You might be thinking to yourself, <em>we are starting with a collection of values (a matrix) and yet we end up with a collection of values (a vector). How is this a reduction/fold?</em></p>

<p>This is a good question: the reduction is not performed over the entire matrix, but only the <em>rows</em> of the matrix.
Each row of the matrix is <em>reduced</em> into a single value.</p>

<!---

For the following definitions:

<center>
$$
  \\
  M \gets   \left[ {\begin{array}{cc}
    0 & 1 & 2 \\
    3 & 4 & 5 \\
    6 & 7 & 8 \\
  \end{array} } \right] ,
  v \gets \left[ {\begin{array}{cc} 2 \\ 2 \\ 2  \end{array} } \right]
  \\
$$
</center>

--->

<p>The algorithm each unit of work performs is called <em>transform-reduce</em> (or sometimes <em>map-reduce</em>).</p>

<p>Although <em>transform-reduce</em> might seem like two algorithms (it kinda is!), it is such a universal operation that it is often considered it’s own algorithm (or at least it’s packaged as its own algorithm in libraries).
For example, <a href="https://thrust.github.io/doc/group__transformed__reductions_ga0d4232a9685675f488c3cc847111e48d.html" target="blank">the Thrust abstraction library that ships with NVIDIA’s CUDA Toolkit has the <em>transform-reduce</em> algorithm built-in.</a></p>

<p>In this case, we would like to <em>transform</em> our input tuples by multiplying two elements together, and then <em>reduce</em> our input using the sum operator.</p>

<p>In Python, a given unit of work might look like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="nb">reduce</span>
<span class="n">tuplespace_row0</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="p">]</span>

<span class="k">def</span> <span class="nf">work</span><span class="p">(</span><span class="n">tupl</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">reduce</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span>        <span class="c1"># use + to reduce
</span>            <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="c1"># use * to transform
</span>                <span class="n">tupl</span>                   <span class="c1"># input tuple
</span>                <span class="p">)</span>
            <span class="p">)</span>

<span class="c1"># Input to map is mat_row
# Input to reduce is [0, 2, 4]
# Final value is 6
</span><span class="k">print</span><span class="p">(</span><span class="n">work</span><span class="p">(</span><span class="n">tuplespace_row0</span><span class="p">))</span> <span class="c1"># yields 6
</span></code></pre></div></div>

<p>The following formula is a more formal definition of a single unit of work in our example:</p>

<center>
$$
\\
  r \gets current rank \\
  W_{r} \gets \sum_{i \gets 0}^{N} M_{r,i} \cdot v_{i} \\
\\
$$
</center>

<p>In the above case, the summation is the <em>reduce</em> operation, and the multiplication of the matrix elements and vector elements is the <em>transform</em> operation, transforming each tuple into a scalar before the reduction.</p>

<p>The key insight about this reduction is that no unit of work depends on another unit of work.
The domains of each unit of work are non-overlapping.
In other words, this algorithm is <em>loop independent</em> and can be parallelized along the rows of our tuplespace, again given by:</p>

<center>
$$
\\
  \left[ {\begin{array}{ccc}
    (Mat_{0,0}, v_0) &amp; (Mat_{0,1}, v_1) &amp; (Mat_{0,2}, v_2) \\
    \hline \\
    (Mat_{1,0}, v_0) &amp; (Mat_{1,1}, v_1) &amp; (Mat_{1,2}, v_2) \\
    \hline \\
    (Mat_{2,0}, v_0) &amp; (Mat_{2,1}, v_1) &amp; (Mat_{2,2}, v_2) \\
  \end{array} } \right]
\\
$$
</center>

<p>It was by identifying and understanding the underlying algorithms (<em>broadcast</em> and <em>transform-reduce</em>) of our higher-level algorithm that we are able to determine if and how it is parallelizable and loop independent.</p>

<blockquote>
  <p>Identify and understand the underlying algorithms</p>
</blockquote>

<p><em>NOTE: Even if your operation seems to be loop dependent, there are sometimes clever tricks you can use to parallelize your code. Perhaps you just haven’t been exposed to the correct algorithm yet!</em></p>

<p>We now hopefully understand that a matrix-vector product is formally <em>a broadcasted multiply followed by a series of sum-reductions</em> and that we can parallelize our algorithm by breaking it up into self-contained units of work.
We can now move on to implementing and parallelizing the algorithm.</p>

<h1 id="c-on-the-host">C on the Host</h1>

<p><a href="https://godbolt.org/z/KqnfKvc4h" target="blank">The code for such a calculation might look like this in C</a>, assuming you’re not using any BLAS or LAPACK routines:</p>
<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">typedef</span> <span class="k">struct</span> <span class="p">{</span>
  <span class="kt">double</span><span class="o">*</span> <span class="n">d</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">M</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">N</span><span class="p">;</span>
<span class="p">}</span> <span class="n">mat_t</span><span class="p">;</span>

<span class="k">typedef</span> <span class="k">struct</span> <span class="p">{</span>
  <span class="kt">double</span><span class="o">*</span> <span class="n">d</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">size</span><span class="p">;</span>
<span class="p">}</span> <span class="n">vec_t</span><span class="p">;</span>

<span class="kt">void</span> <span class="nf">dgemv</span><span class="p">(</span><span class="n">mat_t</span> <span class="n">m</span><span class="p">,</span> <span class="n">vec_t</span> <span class="n">v</span><span class="p">,</span> <span class="n">vec_t</span> <span class="n">out</span><span class="p">)</span> <span class="p">{</span>

  <span class="c1">// Ensure the output is all set to zero</span>
  <span class="n">memset</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="n">d</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="p">.</span><span class="n">size</span><span class="p">);</span>

  <span class="c1">// For each column</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">M</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>

    <span class="c1">// For each row</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">N</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>

      <span class="c1">// Sum the products into the output vector</span>
      <span class="n">out</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">v</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">m</span><span class="p">.</span><span class="n">N</span><span class="p">)];</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Here’s some example data fed into our matrix vector product:</p>
<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">mat_t</span> <span class="n">m</span><span class="p">;</span>
  <span class="kt">double</span> <span class="n">dm</span><span class="p">[</span><span class="mi">9</span><span class="p">];</span>
  <span class="n">m</span><span class="p">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">dm</span><span class="p">;</span>
  <span class="n">m</span><span class="p">.</span><span class="n">M</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">N</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">M</span><span class="o">*</span><span class="n">m</span><span class="p">.</span><span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
      <span class="n">m</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="kt">double</span><span class="p">)</span><span class="n">i</span><span class="p">;</span>

  <span class="n">vec_t</span> <span class="n">v</span><span class="p">;</span>
  <span class="n">v</span><span class="p">.</span><span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="kt">double</span><span class="p">[])</span> <span class="p">{</span> <span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.</span> <span class="p">};</span>
  <span class="n">v</span><span class="p">.</span><span class="n">size</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>

  <span class="n">vec_t</span> <span class="n">out</span><span class="p">;</span>
  <span class="n">out</span><span class="p">.</span><span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="kt">double</span><span class="p">[])</span> <span class="p">{</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span> <span class="p">};</span>
  <span class="n">out</span><span class="p">.</span><span class="n">size</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
  
  <span class="n">dgemv</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">out</span><span class="p">);</span>

  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The output of this program (with some printing code added in):</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">Matrix:
0.0 1.0 2.0 
3.0 4.0 5.0 
6.0 7.0 8.0 

Vector:
2.0 2.0 2.0 

Final vec:
6.0 24.0 42.0 
</span></code></pre></div></div>

<p>Feel free to verify these results and play around with other values using <a href="https://keisan.casio.com/exec/system/15052033860538" target="blank">online software like this CASIO calculator website.</a></p>

<p>Demonstrating that we have a <em>correct</em> algorithm with tests is a precondition for optimizing and parallelizing an algorithm:</p>

<blockquote>
  <p>Testing for correctness precedes parallelism and performance</p>
</blockquote>

<p>We know that a given index in our output vector can be computed independently of any other indices in the output vector from the respective row in our tuple space.
We can then perform the first step in parallelizing our algorithm: pulling out a function that performs a <em>single unit of work</em> as identified above.</p>

<p>In our C example, we might use the following structs to encapsulate our data:</p>
<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">typedef</span> <span class="k">struct</span> <span class="p">{</span>
    <span class="kt">double</span> <span class="n">matval</span><span class="p">;</span>
    <span class="kt">double</span> <span class="n">vecval</span><span class="p">;</span>
<span class="p">}</span> <span class="n">tuple_t</span><span class="p">;</span>

<span class="k">typedef</span> <span class="k">struct</span> <span class="p">{</span>
    <span class="n">tuple_t</span><span class="o">*</span> <span class="n">tuples</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">size</span><span class="p">;</span>
<span class="p">}</span> <span class="n">tuplespace_t</span><span class="p">;</span>
</code></pre></div></div>

<p>A struct of type <code class="language-plaintext highlighter-rouge">tuplespace_t</code> may then hold the domain of a single unit of work.
This is important as we start to parallelize our code: the domain of a unit of work may need to be sent over the network to another computer entirely if we distribute our computation (say with MPI), or it may need to be copied to another component of the current machine if we are to use a GPU.</p>

<p>Our single unit of work then takes a <code class="language-plaintext highlighter-rouge">tuplespace_t</code> struct and returns a <code class="language-plaintext highlighter-rouge">double</code>, which is the value of the output vector at a given index:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">double</span> <span class="nf">unit_of_work</span><span class="p">(</span><span class="n">tuplespace_t</span> <span class="n">ts</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">double</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">ts</span><span class="p">.</span><span class="n">size</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">sum</span> <span class="o">+=</span> <span class="n">ts</span><span class="p">.</span><span class="n">tuples</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">matval</span> <span class="o">*</span> <span class="n">ts</span><span class="p">.</span><span class="n">tuples</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">vecval</span><span class="p">;</span>
  <span class="k">return</span> <span class="n">sum</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Compare this now with the single unit of work we described above:</p>
<center>
$$
\\
w(row) \gets \sum_{i \gets 0}^{N} tuple_{i, 0} \cdot tuple_{i, 1} \\
\\
$$
</center>

<p>Our <code class="language-plaintext highlighter-rouge">dgemv</code> function now must first construct the tuplespace before passing a segment of the tuplespace to the single unit of work:</p>
<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">dgemv_on_tuplespace</span><span class="p">(</span><span class="n">mat_t</span> <span class="n">m</span><span class="p">,</span> <span class="n">vec_t</span> <span class="n">v</span><span class="p">,</span> <span class="n">vec_t</span> <span class="n">out</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// allocate memory for our tuplespace</span>
  <span class="c1">// one tuple per entry in the matrix</span>
  <span class="n">tuple_t</span><span class="o">*</span> <span class="n">ts</span> <span class="o">=</span> <span class="n">malloc</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="n">tuple_t</span><span class="p">[</span><span class="n">m</span><span class="p">.</span><span class="n">M</span><span class="o">*</span><span class="n">m</span><span class="p">.</span><span class="n">N</span><span class="p">]));</span>

  <span class="c1">// set up tuplespace</span>

  <span class="c1">// for each row in matrix</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">M</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>

    <span class="c1">// for each column in matrix</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">N</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>

      <span class="c1">// elements of vector are broadcast to columns of matrix</span>
      <span class="n">ts</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">m</span><span class="p">.</span><span class="n">N</span><span class="p">)]</span> <span class="o">=</span> <span class="p">(</span><span class="n">tuple_t</span><span class="p">)</span> <span class="p">{</span>
        <span class="p">.</span><span class="n">matval</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">m</span><span class="p">.</span><span class="n">N</span><span class="p">)],</span>
        <span class="p">.</span><span class="n">vecval</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
      <span class="p">};</span>

  <span class="c1">// dispatch calculations to unit_of_work for each row of mat</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">M</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>

    <span class="c1">// element in output vector is determined by passing a row of tuplespace</span>
    <span class="c1">// into our unit of work</span>
    <span class="n">out</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">unit_of_work</span><span class="p">((</span><span class="n">tuplespace_t</span><span class="p">)</span> <span class="p">{</span>
      <span class="p">.</span><span class="n">tuples</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">ts</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">m</span><span class="p">.</span><span class="n">N</span><span class="p">],</span>
      <span class="p">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">N</span><span class="p">,</span>
    <span class="p">});</span>

  <span class="n">free</span><span class="p">(</span><span class="n">ts</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>You might have noticed that our new algorithm has significantly more code than our original implementation.
This is okay, and it gets at an important point:</p>

<blockquote>
  <p>Speed is not the same as efficiency</p>
</blockquote>

<p><a href="https://adspthepodcast.com/2021/11/12/Episode-51.html" target="blank">
This excellent podcast episode from the lead HPC architect at NVIDIA explains this point in detail.
</a></p>

<p>If our code performs <em>more work overall</em> it is less <em>efficient</em>.
If that additional work means we can perform calculations on multiple threads or additional devices resulting in lower runtime, it is <em>faster</em> and we’ve increased its <em>speed</em>.
The key difference between speed and efficiency is this: speed is a factor of <em>time</em> and efficiency is a factor of <em>work</em>.
Sometimes optimizing code means improving speed, other times efficiency.
Most of the time, to run code on a GPU, you do have to perform more work to set up the calculation, so strictly speaking our code will be faster and less efficient.</p>

<h1 id="cuda-c">CUDA C</h1>

<p>CUDA C is the basis of the CUDA runtime, and forms the foundation for all other CUDA-related abstractions.
<a href="https://www.nvidia.com/content/GTC-2010/pdfs/2131_GTC2010.pdf" target="blank">
This CUDA C introductory slide deck is helpful in understanding the basics.
</a></p>

<h2 id="core-concepts">Core Concepts</h2>

<h3 id="host-and-device">Host and Device</h3>

<p>When working with a GPU, it’s important to keep in mind the difference between the <em>host</em> and the <em>device</em>.</p>

<center>
<img src="https://cis.temple.edu/~giorgio/cis307/readings/CUDA_processing_flow.png" alt="GPU-CPU interaction">
</center>

<p>Just like your CPU, your GPU has access to it’s own <em>memory</em>.
Programming a GPU entails managing your CPU’s memory along with your GPU’s memory.
If you would like your GPU to have access to some memory you’re using on the CPU, you’ll have to allocate memory on the GPU and copy it over.</p>

<p><a href="https://godbolt.org/z/9eeEedhd5" target="blank">
If you don’t tell the GPU to perform any work, then your CUDA C code is really just C code:
</a></p>
<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include &lt;cstdio&gt;
</span><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">puts</span><span class="p">(</span><span class="s">"Hello!"</span><span class="p">);</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>You can then invoke NVIDIA’s compiler, NVCC, to compile the program:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span><span class="nb">cat </span>hello.cu
<span class="gp">#</span>include &lt;cstdio&gt;
<span class="go">int main() {
</span><span class="gp">  puts("Hello!");</span><span class="w">
</span><span class="gp">  return 0;</span><span class="w">
</span><span class="go">}
</span><span class="gp">$</span><span class="w"> </span>nvcc hello.cu <span class="nt">-o</span> hello <span class="o">&amp;&amp;</span> ./hello
<span class="go">Hello!
</span></code></pre></div></div>

<p>If you invoke <code class="language-plaintext highlighter-rouge">nvcc</code> with the <code class="language-plaintext highlighter-rouge">-v</code> flag for extra verbosity, you can see that <code class="language-plaintext highlighter-rouge">nvcc</code> actually uses a <em>host</em> compiler to build the parts of your program that don’t involve running code or manipulating memory on the GPU.
<code class="language-plaintext highlighter-rouge">nvcc</code> uses multiple passes, where it compiles the CUDA code and generates host-only source for the host compiler to compile.
<a href="https://godbolt.org/z/axTn1ex5x" target="blank">
See this Compiler Explorer link and look at the compilation output window in the bottom right pane to see all the output.
</a>
Notice that GCC is invoked, along with the program <code class="language-plaintext highlighter-rouge">ptxas</code>.
PTX is an assembly target, so your CUDA programs will emit ptx code which can be run on your GPU’s special purpose processing units.
Just as you can use <code class="language-plaintext highlighter-rouge">asm volitile("" : : : "");</code> in C and C++ to write inline assembly, you can also write inline ptx assembly in your programs.
Also like C and C++, it is almost certainly more effective for you to write your code in a higher level language like CUDA C or C++, and write PTX after profiling and testing, when you are sure you need it.</p>

<p>If you’re careful, you might also have noticed that GCC was passed the command line argument <code class="language-plaintext highlighter-rouge">-x c++</code>, even though we’re working in plain CUDA C.
This is because cuda code is <em>by default built on the host as C++</em>.
If you use the oldest CUDA compiler available on Compiler Explorer, you’ll see that it still defaults to building the host code under C++14.</p>

<h3 id="using-the-cuda-runtime">Using the CUDA Runtime</h3>

<p><a href="https://godbolt.org/z/81v3jfehq" target="blank">
In this example, we introduce three aspects of the CUDA programming model:
</a></p>

<ul>
  <li>The special keyword <code class="language-plaintext highlighter-rouge">__global__</code>
</li>
  <li>Device memory management</li>
  <li>Kernel launches</li>
</ul>

<div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// square.cu</span>
<span class="cp">#include &lt;cstdio&gt;
#include &lt;cuda.h&gt;
#include &lt;cuda_runtime.h&gt;
</span>
<span class="k">__global__</span> <span class="kt">void</span> <span class="nf">square</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">ar</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">ar</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">ar</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">ar</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="cp">#define N 10
</span>
  <span class="c1">// Allocate static memory on host</span>
  <span class="kt">int</span> <span class="n">ar</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="n">ar</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>

  <span class="c1">// Allocate memory on device, copy from host to device</span>
  <span class="kt">int</span><span class="o">*</span> <span class="n">d_ar</span><span class="p">;</span>
  <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_ar</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]));</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_ar</span><span class="p">,</span> <span class="n">ar</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

  <span class="c1">// Launch kernel to run on the device</span>
  <span class="n">square</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_ar</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>

  <span class="c1">// Copy memory back from device</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">ar</span><span class="p">,</span> <span class="n">d_ar</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

  <span class="c1">// Display values after kernel</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"%d "</span><span class="p">,</span> <span class="n">ar</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="n">puts</span><span class="p">(</span><span class="s">""</span><span class="p">);</span>

  <span class="c1">// Deallocate memory</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_ar</span><span class="p">);</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>nvcc square.cu <span class="nt">-o</span> square
<span class="gp">$</span><span class="w"> </span>./square
<span class="go">0 1 4 9 16 25 36 49 64 81
</span></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">__global__</code> indicates that the code <em>runs on the device</em> and is <em>called from the host</em>.
Keep in mind that we have to <em>memory spaces</em> and two <em>execution spaces</em>.</p>

<p>The following table enumerates common operations in C along with their CUDA counterpart:</p>

<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0pky">Operation</th>
    <th class="tg-0pky">C</th>
    <th class="tg-0pky">CUDA</th>
    <th class="tg-0pky">Notes</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky">Dynamically allocate memory</td>
    <td class="tg-0pky"><span style="color:#905;background-color:#ddd"><code class="language-plaintext highlighter-rouge">malloc</code></span></td>
    <td class="tg-0pky"><span style="color:#905;background-color:#ddd"><code class="language-plaintext highlighter-rouge">cudaMalloc</code></span></td>
    <td class="tg-0pky">
</td>
  </tr>
  <tr>
    <td class="tg-0pky">Copy memory from one location to another</td>
    <td class="tg-0pky"><span style="color:#905;background-color:#ddd"><code class="language-plaintext highlighter-rouge">memcpy</code></span></td>
    <td class="tg-0pky"><span style="color:#905;background-color:#ddd"><code class="language-plaintext highlighter-rouge">cudaMemcpy</code></span></td>
    <td class="tg-0pky">
</td>
  </tr>
  <tr>
    <td class="tg-0pky">Deallocate memory</td>
    <td class="tg-0pky"><span style="color:#905;background-color:#ddd"><code class="language-plaintext highlighter-rouge">free</code></span></td>
    <td class="tg-0pky"><span style="color:#905;background-color:#ddd"><code class="language-plaintext highlighter-rouge">cudaFree</code></span></td>
    <td class="tg-0pky">
</td>
  </tr>
  <tr>
    <td class="tg-0pky">Call function</td>
    <td class="tg-0pky"><span style="color:#905;background-color:#ddd"><code class="language-plaintext highlighter-rouge">func()</code></span></td>
    <td class="tg-0pky"><span style="color:#905;background-color:#DDD"><code class="language-plaintext highlighter-rouge">func&lt;&lt;&lt;gridSize, blockSize, sharedMemSize, cudaStream&gt;&gt;&gt;()</code></span></td>
    <td class="tg-0pky">Triple angle-brackets indicate you are running in the <span style="font-style:italic"><em>device execution space</em></span>
</td>
  </tr>
</tbody>
</table>

</center>

<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#execution-configuration" target="blank">
The angle brackets surrounding our <em>kernel launch parameters</em> determine how the kernel will be executed by the GPU.
The possible kernel launch parameters are enumerated at this link.
</a></p>

<p>The kernel launch parameters determine how many streaming multiprocessors (SMs) will execute code on the GPU.
The first two parameters are objects of type <code class="language-plaintext highlighter-rouge">dim3</code>, and they can be up to three-dimensional vectors.
The first kernel launch parameter is the <em>grid size</em>, and the second is the <em>block size</em>.</p>

<p>Grids consist of blocks.</p>

<p>Blocks consist of threads.</p>

<p>Therefore, the total number of threads launched by your kernel will be:</p>

<center>
$$
totalthreads \gets gridsize.x \times gridsize.y \times gridsize.z \\
                   \times blocksize.x \times blocksize.y \times blocksize.z \\
$$
</center>

<p>As depicted by the image below, CUDA kernels may be launched with a 1-3 dimensional grid, and a 1-3 dimensional block.</p>

<center>
<img src="http://www.microway.com/wp-content/uploads/CUDA-GridBlockThread-Structure.png" alt="CUDA Grid and Block Depiction">
</center>

<p>You might also notice that we guard our operation with this <code class="language-plaintext highlighter-rouge">if</code> statement.</p>
<div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">ar</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">ar</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">ar</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
</code></pre></div></div>
<p>For performance reasons, it’s usually best to launch your kernels with a multiple of the number of threads in a given block on your GPU, so you may launch with more GPU threads than you need.</p>

<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation" target="blank">
For additional reading on the hardware implementation of the CUDA programming model, please refer to chapter 4 of the NVIDIA CUDA Programming Guide.
</a></p>

<h3 id="shared-memory">Shared Memory</h3>

<p>Although each thread launches with its own stack memory, threads can share memory just like OS threads.
The third kernel launch parameter determines how many bytes will be allocated <em>for each block</em> that is launched.</p>

<p><a href="https://godbolt.org/z/nrbdK9nKj" target="blank">
In the following example, we make use of CUDA shared memory, as indicated by the <code class="language-plaintext highlighter-rouge">__shared__</code> keyword annotating the array in our kernel, as well as our use of the third kernel launch parameter:
</a></p>
<div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include &lt;cstdio&gt;
#include &lt;cuda.h&gt;
#include &lt;cuda_runtime.h&gt;
</span>
<span class="k">__global__</span> <span class="kt">void</span> <span class="nf">mulsum</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">b</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">out</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">extern</span> <span class="k">__shared__</span> <span class="kt">int</span> <span class="n">tmp</span><span class="p">[];</span>
  <span class="cm">/* ▲
   * │     ┌───────────────────────────────┐
   * │     │External and shared, allocated │
   * └─────┤by the cuda runtime when kernel│
   *       │         is launched           │
   *       └───────────────────────────────┘
   */</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&gt;=</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span><span class="p">;</span>

  <span class="n">tmp</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>

  <span class="n">__syncthreads</span><span class="p">();</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
      <span class="n">sum</span> <span class="o">+=</span> <span class="n">tmp</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="o">*</span><span class="n">out</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="cp">#define N 10
</span>
  <span class="kt">int</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>

  <span class="kt">int</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>

  <span class="kt">int</span><span class="o">*</span> <span class="n">d_a</span><span class="p">;</span>
  <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_a</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]));</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

  <span class="kt">int</span><span class="o">*</span> <span class="n">d_b</span><span class="p">;</span>
  <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_b</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]));</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_b</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">[</span><span class="n">N</span><span class="p">]),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

  <span class="kt">int</span><span class="o">*</span> <span class="n">d_out</span><span class="p">;</span>
  <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_out</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>

  <span class="n">mulsum</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="o">*</span><span class="n">N</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span> <span class="n">d_b</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
  <span class="cm">/*             ▲
   *             │    ┌─────────────────────────────────────┐
   *             └────┤Size of shared memory to be allocated│
   *                  │         for kernel launch           │
   *                  └─────────────────────────────────────┘
   */</span>

  <span class="kt">int</span> <span class="n">out</span><span class="p">;</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">out</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
  <span class="n">printf</span><span class="p">(</span><span class="s">"%d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">out</span><span class="p">);</span>

  <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_a</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_b</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_out</span><span class="p">);</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>nvcc mul-sum-reduce.cu <span class="nt">-o</span> msr
<span class="gp">$</span><span class="w"> </span>./msr
<span class="go">285
</span></code></pre></div></div>

<p>Notice how our shared memory is declared:</p>
<div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">extern</span> <span class="k">__shared__</span> <span class="kt">int</span> <span class="n">tmp</span><span class="p">[];</span>
</code></pre></div></div>

<p>It is <code class="language-plaintext highlighter-rouge">external</code> because we are not allocating the memory in our kernel; it’s allocated by the cuda runtime when we pass the third parameter to the kernel launch parameters:</p>

<div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mulsum</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="o">*</span><span class="n">N</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span> <span class="n">d_b</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
               <span class="err">▲</span>
               <span class="err">│</span>      <span class="err">┌─────────────────────────────────────┐</span>
               <span class="err">└──────┤</span><span class="n">Size</span> <span class="n">of</span> <span class="n">shared</span> <span class="n">memory</span> <span class="n">to</span> <span class="n">be</span> <span class="n">allocated</span><span class="err">│</span>
                      <span class="err">│</span>         <span class="k">for</span> <span class="n">kernel</span> <span class="n">launch</span>           <span class="err">│</span>
                      <span class="err">└─────────────────────────────────────┘</span>
</code></pre></div></div>

<p>There can only be one segment of shared memory in a kernel launch, so the shared memory segment will be interpreted as whatever type we declare our shared memory with.
In this case, it’s an array of ints.</p>

<p>We also introduced another CUDA extension to the host language: <code class="language-plaintext highlighter-rouge">__syncthreads()</code>.
<code class="language-plaintext highlighter-rouge">__syncthreads</code> is a <em>fence</em> or <em>barrier</em>, a point which no thread can cross until all threads have reached it.
There are many other synchronization primitives in cuda for primitive atomic operations, such as <code class="language-plaintext highlighter-rouge">atomicAdd</code>.</p>

<h3 id="cuda-dgemv">CUDA <code class="language-plaintext highlighter-rouge">DGEMV</code>
</h3>

<p>We again return to our <code class="language-plaintext highlighter-rouge">dgemv</code> example, this time armed with some knowledge about the CUDA runtime and some software and hardware abstractions.</p>

<h1 id="c-on-the-host-1">C++ on the Host</h1>

<p>There are many layers of abstraction commonly used in HPC for running code on a GPU.
The abstractions used in this post will be limited to those shipped with the most recent stable distribution of the NVIDIA CUDA Toolkit at the time of writing (11.5) along with the abstractions recommended by the lead HPC programming models architect at NVIDIA, Bryce Adelstein Lelbach.
These abstractions are almost entirely <em>compile-time</em> abstractions, which means they incur no runtime cost over the raw C equivilant.
In particular, we will use the structure template <code class="language-plaintext highlighter-rouge">mdspan</code>, which is likely going to be a part of standard ISO C++23.
<code class="language-plaintext highlighter-rouge">mdspan</code> is extremely useful on the host as well, so we’ll use it in our C++ host example.
<a href="https://youtu.be/KK3JXvSiJG4" target="blank"> See Bryce’s talk on C++ standard parallelism here.  </a></p>

<p>In our C++ examples, we’ll use the following type aliases for matrices and vectors:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include &lt;mdspan.hpp&gt; // single-header version from github
</span><span class="k">namespace</span> <span class="n">stdex</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">experimental</span><span class="p">;</span>
<span class="k">using</span> <span class="n">mat_t</span> <span class="o">=</span> <span class="n">stdex</span><span class="o">::</span><span class="n">mdspan</span><span class="o">&lt;</span>
    <span class="kt">double</span><span class="p">,</span>
    <span class="n">stdex</span><span class="o">::</span><span class="n">extents</span><span class="o">&lt;</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="o">&gt;</span>
  <span class="o">&gt;</span><span class="p">;</span>
<span class="k">using</span> <span class="n">vec_t</span> <span class="o">=</span> <span class="n">stdex</span><span class="o">::</span><span class="n">mdspan</span><span class="o">&lt;</span>
    <span class="kt">double</span><span class="p">,</span>
    <span class="n">stdex</span><span class="o">::</span><span class="n">extents</span><span class="o">&lt;</span><span class="mi">3</span><span class="o">&gt;</span>
  <span class="o">&gt;</span><span class="p">;</span>
</code></pre></div></div>

<p>Our setup is then as follows:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="kt">double</span> <span class="n">dm</span><span class="p">[</span><span class="mi">9</span><span class="p">];</span>
  <span class="k">const</span> <span class="k">auto</span> <span class="n">m</span> <span class="o">=</span> <span class="n">mat_t</span><span class="p">(</span><span class="n">dm</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">iota</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">m</span><span class="p">.</span><span class="n">data</span><span class="p">()</span><span class="o">+</span><span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

  <span class="kt">double</span> <span class="n">dv</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span> <span class="p">};</span>
  <span class="k">const</span> <span class="k">auto</span> <span class="n">v</span> <span class="o">=</span> <span class="n">vec_t</span><span class="p">(</span><span class="n">dv</span><span class="p">);</span>

  <span class="kt">double</span> <span class="n">dout</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
  <span class="k">const</span> <span class="k">auto</span> <span class="n">out</span> <span class="o">=</span> <span class="n">vec_t</span><span class="p">(</span><span class="n">dout</span><span class="p">);</span>

  <span class="n">dgemv</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">out</span><span class="p">);</span>

  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>If we only compute the solution on the host, everything works the same way:</p>
<div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">dgemv</span><span class="p">(</span><span class="n">mat_t</span> <span class="n">m</span><span class="p">,</span> <span class="n">vec_t</span> <span class="n">v</span><span class="p">,</span> <span class="n">vec_t</span> <span class="n">out</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">static_extent</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">static_extent</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>
      <span class="n">out</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+=</span> <span class="n">v</span><span class="p">(</span><span class="n">j</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Notice how we can index our matrix as we would expect to: with multidimensional indexing.
Notice also how we can call <code class="language-plaintext highlighter-rouge">static_extent</code> on our matrix - this determines the length of our matrix in a given dimension at <em>compile time</em>, which means we perform <em>less work at runtime then in the pure C version</em> while getting nicer indexing and extent calculations.
The compile-time and zero-cost abstractions are significant reasons why developers opt for C++ over C.
For example, compare <a href="https://godbolt.org/z/ssrnz71hs" target="blank">the assembly in this C example</a> to <a href="https://godbolt.org/z/ra8d6arMf" target="blank"> the assembly in this C++ example.  </a>
This is one reason that most compiler vendors choose to write their compilers in C++ over C (eg GNU’s GCC, LLVM, Cray CC, IBM XL, Intel compiler suite, etc).</p>

<p>Let’s now compute our <code class="language-plaintext highlighter-rouge">dgemv</code> with the work broken up into isolated units, just as before.</p>

<p>We’ll use the following additional type aliases:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">stdex</span><span class="o">::</span><span class="n">full_extent</span><span class="p">;</span>
<span class="k">using</span> <span class="n">stdex</span><span class="o">::</span><span class="n">submdspan</span><span class="p">;</span>
<span class="k">using</span> <span class="n">tuplespace_t</span> <span class="o">=</span> <span class="n">stdex</span><span class="o">::</span><span class="n">mdspan</span><span class="o">&lt;</span>
    <span class="kt">double</span><span class="p">,</span>
    <span class="n">stdex</span><span class="o">::</span><span class="n">extents</span><span class="o">&lt;</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="o">&gt;</span>
  <span class="o">&gt;</span><span class="p">;</span>
<span class="k">using</span> <span class="n">tuplespace_row_t</span> <span class="o">=</span> <span class="n">stdex</span><span class="o">::</span><span class="n">mdspan</span><span class="o">&lt;</span>
    <span class="kt">double</span><span class="p">,</span>
    <span class="n">stdex</span><span class="o">::</span><span class="n">extents</span><span class="o">&lt;</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="o">&gt;</span>
  <span class="o">&gt;</span><span class="p">;</span>
</code></pre></div></div>

<p>The setup for the tuplespace and the units of work also look just about the same:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">double</span> <span class="nf">unit_of_work</span><span class="p">(</span><span class="n">tuplespace_row_t</span> <span class="n">tsr</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">double</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tsr</span><span class="p">.</span><span class="n">static_extent</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">sum</span> <span class="o">+=</span> <span class="n">tsr</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">tsr</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">sum</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">dgemv_on_tuplespace</span><span class="p">(</span><span class="n">mat_t</span> <span class="n">m</span><span class="p">,</span> <span class="n">vec_t</span> <span class="n">v</span><span class="p">,</span> <span class="n">vec_t</span> <span class="n">out</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">double</span> <span class="n">tuplespace_data</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">.</span><span class="n">static_extent</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span><span class="p">.</span><span class="n">static_extent</span><span class="p">(</span><span class="mi">1</span><span class="p">)];</span>
  <span class="k">const</span> <span class="k">auto</span> <span class="n">ts</span> <span class="o">=</span> <span class="n">tuplespace_t</span><span class="p">(</span><span class="n">tuplespace_data</span><span class="p">);</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">static_extent</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">static_extent</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">ts</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">);</span>
      <span class="n">ts</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="n">v</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
    <span class="p">}</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">static_extent</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">out</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">=</span> <span class="n">unit_of_work</span><span class="p">(</span><span class="n">submdspan</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">full_extent</span><span class="p">,</span> <span class="n">full_extent</span><span class="p">));</span>
<span class="p">}</span>
</code></pre></div></div>

<h1 id="cuda-c-1">CUDA C++</h1>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">__global__</span>
<span class="kt">void</span> <span class="nf">unit_of_work</span><span class="p">(</span><span class="n">tuplespace_t</span> <span class="n">ts</span><span class="p">,</span> <span class="n">vec_t</span> <span class="n">out</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// row of tuplespace, and index in output</span>
  <span class="k">const</span> <span class="k">auto</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">const</span> <span class="k">auto</span> <span class="n">tsr</span> <span class="o">=</span> <span class="n">submdspan</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">tid</span><span class="p">,</span> <span class="n">full_extent</span><span class="p">,</span> <span class="n">full_extent</span><span class="p">);</span>
  <span class="kt">double</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tsr</span><span class="p">.</span><span class="n">static_extent</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">sum</span> <span class="o">+=</span> <span class="n">tsr</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">tsr</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
  <span class="n">out</span><span class="p">(</span><span class="n">tid</span><span class="p">)</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">dgemv_on_tuplespace</span><span class="p">(</span><span class="n">mat_t</span> <span class="n">m</span><span class="p">,</span> <span class="n">vec_t</span> <span class="n">v</span><span class="p">,</span> <span class="n">vec_t</span> <span class="n">out</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">constexpr</span> <span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="n">ts_size</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">.</span><span class="n">static_extent</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span><span class="p">.</span><span class="n">static_extent</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
  <span class="kt">double</span> <span class="n">tuplespace_data</span><span class="p">[</span><span class="n">ts_size</span><span class="p">];</span>
  <span class="k">const</span> <span class="k">auto</span> <span class="n">ts</span> <span class="o">=</span> <span class="n">tuplespace_t</span><span class="p">(</span><span class="n">tuplespace_data</span><span class="p">);</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">static_extent</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">static_extent</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">ts</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">);</span>
      <span class="n">ts</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="n">v</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
    <span class="p">}</span>

  <span class="c1">// Allocate memory for tuplespace on device, copy to device</span>
  <span class="kt">double</span> <span class="o">*</span><span class="n">dev_tuplespace_data</span> <span class="o">=</span> <span class="nb">nullptr</span><span class="p">;</span>
  <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dev_tuplespace_data</span><span class="p">,</span> <span class="n">ts_size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dev_tuplespace_data</span><span class="p">,</span> <span class="n">tuplespace_data</span><span class="p">,</span>
    <span class="n">ts_size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
  <span class="k">const</span> <span class="k">auto</span> <span class="n">dev_ts</span> <span class="o">=</span> <span class="n">tuplespace_t</span><span class="p">(</span><span class="n">dev_tuplespace_data</span><span class="p">);</span>

  <span class="c1">// Allocate memory for output vector on device</span>
  <span class="kt">double</span> <span class="o">*</span><span class="n">dev_out_data</span> <span class="o">=</span> <span class="nb">nullptr</span><span class="p">;</span>
  <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dev_out_data</span><span class="p">,</span> <span class="n">m</span><span class="p">.</span><span class="n">static_extent</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span>
  <span class="k">const</span> <span class="k">auto</span> <span class="n">dev_out</span> <span class="o">=</span> <span class="n">vec_t</span><span class="p">(</span><span class="n">dev_out_data</span><span class="p">);</span>

  <span class="c1">// Perform units of work on m.static_extent(0) GPU threads</span>
  <span class="n">cu</span><span class="o">::</span><span class="n">unit_of_work</span><span class="o">&lt;&lt;&lt;</span><span class="n">m</span><span class="p">.</span><span class="n">static_extent</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">dev_ts</span><span class="p">,</span> <span class="n">dev_out</span><span class="p">);</span>

  <span class="c1">// Copy output vector from device to host</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dev_out</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">ts_size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<!---
<center>
<div class="mermaid">
graph TD
  T(tests) -- > C(correctness)
  I(initial impl) -- > C
  C -- > P(perf analysis)
  P -- > A(algorithm analysis)
  P -- > CC(code generation)
  A -- > D(distribution)
</div>
</center>
--->

<h1 id="bqn-example">BQN Example</h1>

<p>Personally, I use BQN to prototype solutions to problems and to better understand the fundamental algorithms at play; you don’t have to know an APL in order to understand this, but it might be helpful.
Feel free to skip this section; it is not critical to understanding the concepts.</p>

<p><a href="https://mlochbaum.github.io/BQN/try.html#code=4oCiU2hvdyBtYXQg4oaQIDPigL8z4qWK4oaVMTAK4oCiU2hvdyB2ZWMg4oaQIDPipYoyCivLneKOiTEgbWF0w5d2ZWMK" target="blank">Here’s a permalink to the BQN snippet.</a></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   # Same matrix as in our C example
   mat ← 3‿3⥊↕10
┌─       
╵ 0 1 2  
  3 4 5  
  6 7 8  
        ┘
   # Same vector as in our C example
   vec ← 3⥊2
⟨ 2 2 2 ⟩

   +˝⎉1 mat×vec
⟨ 6 24 42 ⟩
</code></pre></div></div>

<p>The core algorithm is seen in the final expression:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+˝⎉1 mat×vec
▲    ▲
│    │     ┌───────────────────────────┐
│    └─────┤Multiply rows of mat by vec│
│          │        element-wise       │
│          └───────────────────────────┘
│     ┌─────────────────────────┐
│     │Sum-reduce rows of matrix│
└─────┤ resulting from mat×vec  │
      └─────────────────────────┘
</code></pre></div></div>

<p>Alternatively:</p>

<center>
<img height="300" src="/images/hpc-101-matvec/bqn-dgemv-explain.png" alt="Try BQN explanation of dgemv">
</center>

<h1 id="links-references-additional-reading">Links, References, Additional Reading</h1>

<ul>
  <li><a href="https://godbolt.org/z/45j4hedq8" target="blank"> C dgemv and dgemv on tuplespace example on godbolt </a></li>
  <li><a href="https://mlochbaum.github.io/BQN/try.html#code=4oCiU2hvdyBtYXQg4oaQIDPigL8z4qWK4oaVMTAK4oCiU2hvdyB2ZWMg4oaQIDPipYoyCivLneKOiTEgbWF0w5d2ZWMK" target="blank">BQN dgemv example</a></li>
  <li><a href="https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.2-Multiplying-Matrices-and-Vectors/" target="blank">Matrix-Vector Product image</a></li>
  <li><a href="https://www.cs.utexas.edu/~lin/cs380c/handout27.pdf" target="blank">UT Austin slides on loop-carried dependencies and parallelism</a></li>
  <li><a href="https://www.worldcat.org/title/how-to-write-parallel-programs-a-first-course/oclc/912171709&amp;referer=brief_results" target="blank"><em>How to Write Parallel Programs: A First Course</em></a></li>
  <li><a href="https://thrust.github.io/doc/group__transformed__reductions_ga0d4232a9685675f488c3cc847111e48d.html" target="blank">Thrust parallel algorithms library</a></li>
  <li><a href="https://adspthepodcast.com/2021/11/12/Episode-51.html" target="blank"> ADSP podcast episode from the lead HPC architect at NVIDIA discussing speed vs efficiency</a></li>
  <li><a href="https://youtu.be/KK3JXvSiJG4" target="blank"> Bryce Adelstein Lelbach’s talk on C++ standard parallelism </a></li>
  <li><a href="https://github.com/kokkos/mdspan/blob/single-header/mdspan.hpp" target="blank"> Kokkos <code class="language-plaintext highlighter-rouge">mdspan</code> single header </a></li>
  <li><a href="https://www.nvidia.com/content/GTC-2010/pdfs/2131_GTC2010.pdf" target="blank">CUDA C Introduction Slides</a></li>
</ul>

<font size="-1">
  <em>
    These views do not in any way represent those of Pacific Northwest National Laboratory, the US Department of Energy, Battelle, or any other organization or institution that I, Asher Mancinelli, am professionally associated with.
    These views are entirely my own.
  </em>
</font>


  </div>

  <hr>

  <div class="date">
    Written on Feb 10th, 2022
  </div>

  
</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          
<a href="mailto:ashermancinelli@gmail.com"><i class="svg-icon email"></i></a>


<a href="https://github.com/ashermancinelli"><i class="svg-icon github"></i></a>

<a href="https://www.linkedin.com/in/https://www.linkedin.com/in/asher-mancinelli-bb4a56144/"><i class="svg-icon linkedin"></i></a>




<a href="https://youtube.com/channel/UCZ5sL4E662VP1ZwC4h85ttQ"><i class="svg-icon youtube"></i></a>

        </footer>
      </div>
    </div>
    
    <div>Icons made by <a href="https://www.freepik.com" title="Freepik">Freepik</a> from <a href="https://www.flaticon.com/" title="Flaticon">www.flaticon.com</a>
</div>
    
    

  </body>
</html>
