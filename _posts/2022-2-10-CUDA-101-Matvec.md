---
layout: post
title: "CUDA 101: Matrix-Vector Product"
permalink: /cuda-matvec
category: c, c++, cuda, GPU, HPC
wip: true
---

{% include latex.html %}
{% include mermaid.html %}

The foundation of GPU programming is linear algebra.
This post takes you from a basic linear algebra example problem to a GPU-accelerated example that calculates a matrix-vector product.

{% include disclaimer.html %}

# Key Takeaways

These are the key takeaways from this post.
If you don't read the entire post, at least take these points:

1. Correctness precedes parallelism and performance
1. Identify and understand the underlying algorithms at play
1. Speed is not the same as efficiency

***NOTE: This post is geared towards those without significant experience in linear algebra, high performance computing, or GPU programming.***

# Outline

1. Mathematical Understanding of Algorithm
    1. Short Example in Python
1. C on Host
1. CUDA C
1. CUDA C++
1. Links, References, Additional Reading

# Mathematical Understanding of Algorithm

We'll be performing a matrix-vector dot product several ways in this post.

The operation is depicted below:

<center>
<img
  src="/images/hpc-101-matvec/matvec.png"
  alt="Matvec dot product, credit this post: https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.2-Multiplying-Matrices-and-Vectors/"
  >
</center>

Let `p` be the result of the dot product of matrix `Mat` and vector `v`.
The dot product is calculated like so:

<center>
$$
\\
  p \gets Mat \cdot v

  \\

  =

  v_0 \cdot
  \left[ {\begin{array}{c}
    Mat_{0, 0} \\
    Mat_{1, 0} \\
    Mat_{2, 0} \\
  \end{array} } \right]
  +
  v_1 \cdot
  \left[ {\begin{array}{c}
    Mat_{0, 1} \\
    Mat_{1, 1} \\
    Mat_{2, 1} \\
  \end{array} } \right]
  +
  v_2 \cdot
  \left[ {\begin{array}{c}
    Mat_{0, 2} \\
    Mat_{1, 2} \\
    Mat_{2, 2} \\
  \end{array} } \right]

  \\

  =

  \left[ {\begin{array}{cc}
    (Mat_{0,0} \cdot v_0) + (Mat_{0,1} \cdot v_1) + (Mat_{0,2} \cdot v_2) \\
    (Mat_{1,0} \cdot v_0) + (Mat_{1,1} \cdot v_1) + (Mat_{1,2} \cdot v_2) \\
    (Mat_{2,0} \cdot v_0) + (Mat_{2,1} \cdot v_1) + (Mat_{2,2} \cdot v_2) \\
  \end{array} } \right]
\\
$$
<!---
  =   \left[ {\begin{array}{cc}
    6 \\ 24 \\ 42
  \end{array} } \right]
  --->
</center>

Notice how values of `v` are broadcast to match the shape of `Mat`:

<center>
$$
\\
  \left[ {\begin{array}{c}
    v_{0} & v_{1} & \cdots & v_{n}\\
    v_{0} & v_{1} & \cdots & v_{n}\\
    \vdots & \vdots & \ddots & \vdots\\
    v_{0} & v_{1} & \cdots & v_{n}\\
  \end{array} } \right]
\\
$$
</center>

We can broadcast values of `v` into columns of a matrix with the same shape as the matrix `Mat`, and then pair the `Mat` and `v` element-wise, creating a matrix of tuples (or a 3d matrix if you prefer):

<center>
$$
\\
  tuplespace \gets
  \left[ {\begin{array}{cc}
    (Mat_{0,0}, v_0) & (Mat_{0,1}, v_1) & (Mat_{0,2}, v_2) \\
    (Mat_{1,0}, v_0) & (Mat_{1,1}, v_1) & (Mat_{1,2}, v_2) \\
    (Mat_{2,0}, v_0) & (Mat_{2,1}, v_1) & (Mat_{2,2}, v_2) \\
  \end{array} } \right]
\\
$$
</center>

This is sometimes called a _tuple space_, or the _domain_ of our algorithm.
The book <a href="https://www.worldcat.org/title/how-to-write-parallel-programs-a-first-course/oclc/912171709&referer=brief_results" target="blank">_How to Write Parallel Programs: A First Course_</a> covers tuple spaces in great detail.

Now that we have constructed our tuple space, we might group our computations into self-contained units of work along each row.

Let _tuplespace_ be the 2 dimensional matrix tuple space given above.
We then may form a vector with units of work yielding indices of the output vector:

<center>
$$
\\
  \left[ {\begin{array}{cccc}
    w(0) \gets \sum_{i \gets 0}^{N} tuplespace_{0, i, 0} \cdot tuplespace_{0, i, 1} \\
    w(1) \gets \sum_{i \gets 0}^{N} tuplespace_{1, i, 0} \cdot tuplespace_{1, i, 1} \\
    \vdots \\
    w(M) \gets \sum_{i \gets 0}^{N} tuplespace_{M, i, 0} \cdot tuplespace_{M, i, 1} \\
  \end{array} } \right]
\\
$$
</center>

Equivalently:

<center>
$$
\\
  \left[ {\begin{array}{cccc}
    w(0) \gets \sum_{i \gets 0}^{N} Mat_{0,i} \cdot v_{i} \\
    w(1) \gets \sum_{i \gets 0}^{N} Mat_{1,i} \cdot v_{i} \\
    \vdots \\
    w(M) \gets \sum_{i \gets 0}^{N} Mat_{M,i} \cdot v_{i} \\
  \end{array} } \right]
\\
$$
</center>

Our units of work may independently operate on subsets (rows) of our tuple space.

# Algorithm Analysis

The first question we must ask ourselves when parallelizing code is this: _are any iterations of the algorithm dependent on values calculated in other iterations? Is iteration `N` dependent on calculations in iteration `N-1`?_
In other words, _are the loop bodies entirely_ ***independent*** _of each other?_

If so, our algorithm is _loop independent_ and _trivially parallelizable_.
<a href="https://www.cs.utexas.edu/~lin/cs380c/handout27.pdf" target="blank">This slidedeck from a UT Austin lecture</a> are helpful additional reading on this topic.

The fundamental algorithm at play here is a _reduction_ or a _fold_.
If you see these terms elsewhere in literature, documentation, or algorithms in libraries or programming languages, they almost certainly mean the same thing.
Some collection of values are _reduced_ or _folded_ into a single value.

You might be thinking to yourself, _we are starting with a collection of values (a matrix) and yet we end up with a collection of values (a vector). How is this a reduction/fold?_

This is a good question: the reduction is not performed over the entire matrix, but only the _rows_ of the matrix.
Each row of the matrix is _reduced_ into a single value.

<!---

For the following definitions:

<center>
$$
  \\
  M \gets   \left[ {\begin{array}{cc}
    0 & 1 & 2 \\
    3 & 4 & 5 \\
    6 & 7 & 8 \\
  \end{array} } \right] ,
  v \gets \left[ {\begin{array}{cc} 2 \\ 2 \\ 2  \end{array} } \right]
  \\
$$
</center>

--->

The algorithm each unit of work performs is called _transform-reduce_ (or sometimes _map-reduce_).

Although _transform-reduce_ might seem like two algorithms (it kinda is!), it is such a universal operation that it is often considered it's own algorithm (or at least it's packaged as its own algorithm in libraries).
For example, <a href="https://thrust.github.io/doc/group__transformed__reductions_ga0d4232a9685675f488c3cc847111e48d.html" target="blank">the Thrust abstraction library that ships with NVIDIA's CUDA Toolkit has the _transform-reduce_ algorithm built-in.</a>

In this case, we would like to _transform_ our input tuples by multiplying two elements together, and then _reduce_ our input using the sum operator.

In Python, a given unit of work might look like this:

```python
from functools import reduce
tuplespace_row0 = [
    (0, 2),
    (1, 2),
    (2, 2),
    ]

def work(tupl):
    return reduce(
            lambda a, b: a + b,        # use + to reduce
            map(lambda x: x[0] * x[1], # use * to transform
                tupl                   # input tuple
                )
            )

# Input to map is mat_row
# Input to reduce is [0, 2, 4]
# Final value is 6
print(work(tuplespace_row0)) # yields 6
```

The following formula is a more formal definition of a single unit of work in our example:

<center>
$$
\\
  r \gets current rank \\
  W_{r} \gets \sum_{i \gets 0}^{N} M_{r,i} \cdot v_{i} \\
\\
$$
</center>

In the above case, the summation is the _reduce_ operation, and the multiplication of the matrix elements and vector elements is the _transform_ operation, transforming each tuple into a scalar before the reduction.

The key insight about this reduction is that no unit of work depends on another unit of work.
The domains of each unit of work are non-overlapping.
In other words, this algorithm is _loop independent_ and can be parallelized along the rows of our tuplespace, again given by:

<center>
$$
\\
  \left[ {\begin{array}{ccc}
    (Mat_{0,0}, v_0) & (Mat_{0,1}, v_1) & (Mat_{0,2}, v_2) \\
    \hline \\
    (Mat_{1,0}, v_0) & (Mat_{1,1}, v_1) & (Mat_{1,2}, v_2) \\
    \hline \\
    (Mat_{2,0}, v_0) & (Mat_{2,1}, v_1) & (Mat_{2,2}, v_2) \\
  \end{array} } \right]
\\
$$
</center>

It was by identifying and understanding the underlying algorithms (_broadcast_ and _transform-reduce_) of our higher-level algorithm that we are able to determine if and how it is parallelizable and loop independent.

> Identify and understand the underlying algorithms

_NOTE: Even if your operation seems to be loop dependent, there are sometimes clever tricks you can use to parallelize your code. Perhaps you just haven't been exposed to the correct algorithm yet!_

We now hopefully understand that a matrix-vector product is formally _a broadcasted multiply followed by a series of sum-reductions_ and that we can parallelize our algorithm by breaking it up into self-contained units of work.
We can now move on to implementing and parallelizing the algorithm.

# C on the Host

<a href="https://godbolt.org/z/T3qzr8fve" target="blank">The code for such a calculation might look like this in C</a>:
```c
void matvecmul(int* mat, int* vec, int* out, int m, int n) {
    for (int i=0; i < m; i++)
        for (int j=0; j < n; j++)
            out[i] += vec[j] * mat[j+(i*n)];
}
```

Here's some example data fed into our matrix vector product:
```c
int main() {
    int M = 3;
    int N = 4;

    int mat[M*N];
    for (int i=0; i < M*N; i++) mat[i] = i;

    int vec[N];
    for (int i=0; i < N; i++) vec[i] = i;

    int out[M];

    memset(out, 0, sizeof(int[M]));
    matvecmul(mat, vec, out, M, N);

    memset(out, 0, sizeof(int[M]));
    matvecmul_on_tuplespace(mat, vec, out, M, N);

    return 0;
}
```

The output of this program (with some printing code added in):
```console
Matrix:
  0   1   2   3 
  4   5   6   7 
  8   9  10  11 
Vector:
  0   1   2   3 
Output:
 14  38  62 
Output:
 14  38  62 
```

Feel free to verify these results and play around with other values using <a href="https://keisan.casio.com/exec/system/15052033860538" target="blank">online software like this CASIO calculator website</a>, or a scripting language.
<a href="https://mlochbaum.github.io/BQN/try.html#code=bSDihpAgMwpuIOKGkCA0Ck11bCDihpAgK8ud4oiYw5fijokx4oC/4oieCgptYXQg4oaQIG3igL9u4qWK4oaVMjAwCnZlYyDihpAg4oaVbgoKbWF0IE11bCB2ZWM=" target="blank">
Here's an example of the above problem using BQN, one of my favorite languages to use when understanding an algorithm.
</a>

Demonstrating that we have a _correct_ algorithm with tests is a precondition for optimizing and parallelizing an algorithm:

> Testing for correctness precedes parallelism and performance


We know that a given index in our output vector can be computed independently of any other indices in the output vector from the respective row in our tuple space.
We can then perform the first step in parallelizing our algorithm: pulling out a function that performs a _single unit of work_ as identified above.


Our single unit of work then takes a `tuplespace_t` struct and returns a `double`, which is the value of the output vector at a given index:

```c
double unit_of_work(tuplespace_t ts) {
  double sum = 0;
  for (int i=0; i < ts.size; i++)
    sum += ts.tuples[i].matval * ts.tuples[i].vecval;
  return sum;
}
```

Compare this now with the single unit of work we described above:
<center>
$$
\\
w(row) \gets \sum_{i \gets 0}^{N} tuple_{i, 0} \cdot tuple_{i, 1} \\
\\
$$
</center>

Our `matvecmul` function now must first construct the tuplespace before passing a segment of the tuplespace to the single unit of work:
```c
void matvecmul_on_tuplespace(mat_t m, vec_t v, vec_t out) {
  // allocate memory for our tuplespace
  // one tuple per entry in the matrix
  tuple_t* ts = malloc(sizeof(tuple_t[m.M*m.N]));

  // set up tuplespace

  // for each row in matrix
  for (int i=0; i < m.M; i++)

    // for each column in matrix
    for (int j=0; j < m.N; j++)

      // elements of vector are broadcast to columns of matrix
      ts[j+(i*m.N)] = (tuple_t) {
        .matval = m.d[j+(i*m.N)],
        .vecval = v.d[i]
      };

  // dispatch calculations to unit_of_work for each row of mat
  for (int i=0; i < m.M; i++)

    // element in output vector is determined by passing a row of tuplespace
    // into our unit of work
    out.d[i] = unit_of_work((tuplespace_t) {
      .tuples = &ts[i*m.N],
      .size = m.N,
    });

  free(ts);
}
```

You might have noticed that our new algorithm has significantly more code than our original implementation.
This is okay, and it gets at an important point:

> Speed is not the same as efficiency

<a href="https://adspthepodcast.com/2021/11/12/Episode-51.html" target="blank">
This excellent podcast episode from the lead HPC architect at NVIDIA explains this point in detail.
</a>

If our code performs _more work overall_ it is less _efficient_.
If that additional work means we can perform calculations on multiple threads or additional devices resulting in lower runtime, it is _faster_ and we've increased its _speed_.
The key difference between speed and efficiency is this: speed is a factor of _time_ and efficiency is a factor of _work_.
Sometimes optimizing code means improving speed, other times efficiency.
Most of the time, to run code on a GPU, you do have to perform more work to set up the calculation, so strictly speaking our code will be faster and less efficient.

# CUDA C

CUDA C is the basis of the CUDA runtime, and forms the foundation for all other CUDA-related abstractions.
<a href="https://www.nvidia.com/content/GTC-2010/pdfs/2131_GTC2010.pdf" target="blank">
This CUDA C introductory slide deck is helpful in understanding the basics.
</a>

## Core Concepts

### Host and Device

When working with a GPU, it's important to keep in mind the difference between the _host_ and the _device_.

<center>
<img
  src="https://cis.temple.edu/~giorgio/cis307/readings/CUDA_processing_flow.png"
  alt="GPU-CPU interaction"
  />
</center>

Just like your CPU, your GPU has access to it's own _memory_.
Programming a GPU entails managing your CPU's memory along with your GPU's memory.
If you would like your GPU to have access to some memory you're using on the CPU, you'll have to allocate memory on the GPU and copy it over.

<a href="https://godbolt.org/z/9eeEedhd5" target="blank">
If you don't tell the GPU to perform any work, then your CUDA C code is really just C code:
</a>
```c
#include <cstdio>
int main() {
  puts("Hello!");
  return 0;
}
```

You can then invoke NVIDIA's compiler, NVCC, to compile the program:
```console
$ cat hello.cu
#include <cstdio>
int main() {
  puts("Hello!");
  return 0;
}
$ nvcc hello.cu -o hello && ./hello
Hello!
```

If you invoke `nvcc` with the `-v` flag for extra verbosity, you can see that `nvcc` actually uses a _host_ compiler to build the parts of your program that don't involve running code or manipulating memory on the GPU.
`nvcc` uses multiple passes, where it compiles the CUDA code and generates host-only source for the host compiler to compile.
<a href="https://godbolt.org/z/axTn1ex5x" target="blank">
See this Compiler Explorer link and look at the compilation output window in the bottom right pane to see all the output.
</a>
Notice that GCC is invoked, along with the program `ptxas`.
PTX is an assembly target, so your CUDA programs will emit ptx code which can be run on your GPU's special purpose processing units.
Just as you can use `asm volitile("" : : : "");` in C and C++ to write inline assembly, you can also write inline ptx assembly in your programs.
Also like C and C++, it is almost certainly more effective for you to write your code in a higher level language like CUDA C or C++, and write PTX after profiling and testing, when you are sure you need it.

If you're careful, you might also have noticed that GCC was passed the command line argument `-x c++`, even though we're working in plain CUDA C.
This is because cuda code is _by default built on the host as C++_.
If you use the oldest CUDA compiler available on Compiler Explorer, you'll see that it still defaults to building the host code under C++14.

### Using the CUDA Runtime

<a href="https://godbolt.org/z/81v3jfehq" target="blank">
In this example, we introduce three aspects of the CUDA programming model:
</a>

* The special keyword `__global__`
* Device memory management
* Kernel launches

```cuda
// square.cu
#include <cstdio>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void square(int *ar, int n) {
  int tid = threadIdx.x;
  if (tid < n)
    ar[tid] = ar[tid] * ar[tid];
}

int main() {
  #define N 10

  // Allocate static memory on host
  int ar[N];
  for (int i=0; i < N; i++) ar[i] = i;

  // Allocate memory on device, copy from host to device
  int* d_ar;
  cudaMalloc(&d_ar, sizeof(int[N]));
  cudaMemcpy(d_ar, ar, sizeof(int[N]), cudaMemcpyHostToDevice);

  // Launch kernel to run on the device
  square<<<1, 15>>>(d_ar, N);

  // Copy memory back from device
  cudaMemcpy(ar, d_ar, sizeof(int[N]), cudaMemcpyDeviceToHost);

  // Display values after kernel
  for (int i=0; i < N; i++)
    printf("%d ", ar[i]);
  puts("");

  // Deallocate memory
  cudaFree(d_ar);
  return 0;
}
```

```console
$ nvcc square.cu -o square
$ ./square
0 1 4 9 16 25 36 49 64 81
```

`__global__` indicates that the code _runs on the device_ and is _called from the host_.
Keep in mind that we have to _memory spaces_ and two _execution spaces_.

The following table enumerates common operations in C along with their CUDA counterpart:

<center>
{% include hpc-101-matvec/cuda-c-alloc-table.html %}
</center>

<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#execution-configuration" target="blank">
The angle brackets surrounding our _kernel launch parameters_ determine how the kernel will be executed by the GPU.
The possible kernel launch parameters are enumerated at this link.
</a>

The kernel launch parameters determine how many streaming multiprocessors (SMs) will execute code on the GPU.
The first two parameters are objects of type `dim3`, and they can be up to three-dimensional vectors.
The first kernel launch parameter is the _grid size_, and the second is the _block size_.

Grids consist of blocks.

Blocks consist of threads.

Therefore, the total number of threads launched by your kernel will be:

<center>
$$
totalthreads \gets gridsize.x \times gridsize.y \times gridsize.z \\
                   \times blocksize.x \times blocksize.y \times blocksize.z \\
$$
</center>

As depicted by the image below, CUDA kernels may be launched with a 1-3 dimensional grid, and a 1-3 dimensional block.

<center>
<img
  src="http://www.microway.com/wp-content/uploads/CUDA-GridBlockThread-Structure.png"
  alt="CUDA Grid and Block Depiction"
  />
</center>

You might also notice that we guard our operation with this `if` statement.
```cuda
  if (tid < n)
    ar[tid] = ar[tid] * ar[tid];
```
For performance reasons, it's usually best to launch your kernels with a multiple of the number of threads in a given block on your GPU, so you may launch with more GPU threads than you need.

<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation" target="blank">
For additional reading on the hardware implementation of the CUDA programming model, please refer to chapter 4 of the NVIDIA CUDA Programming Guide.
</a>

### Shared Memory

Although each thread launches with its own stack memory, threads can share memory just like OS threads.
The third kernel launch parameter determines how many bytes will be allocated _for each block_ that is launched.

<a href="https://godbolt.org/z/nrbdK9nKj" target="blank">
In the following example, we make use of CUDA shared memory, as indicated by the `__shared__` keyword annotating the array in our kernel, as well as our use of the third kernel launch parameter:
</a>
```cuda
#include <cstdio>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void mulsum(int* a, int* b, int* out, int n) {
  int tid = threadIdx.x;
  extern __shared__ int tmp[];
  /* ▲
   * │     ┌───────────────────────────────┐
   * │     │External and shared, allocated │
   * └─────┤by the cuda runtime when kernel│
   *       │         is launched           │
   *       └───────────────────────────────┘
   */
  if (tid >= n)
    return;

  tmp[tid] = a[tid] * b[tid];

  __syncthreads();

  if (tid == 0) {
    int sum = 0;
    for (int i=0; i < n; i++)
      sum += tmp[i];
    *out = sum;
  }
}

int main() {
  #define N 10

  int a[N];
  for (int i=0; i < N; i++) a[i] = i;

  int b[N];
  for (int i=0; i < N; i++) b[i] = i;

  int* d_a;
  cudaMalloc(&d_a, sizeof(int[N]));
  cudaMemcpy(d_a, a, sizeof(int[N]), cudaMemcpyHostToDevice);

  int* d_b;
  cudaMalloc(&d_b, sizeof(int[N]));
  cudaMemcpy(d_b, b, sizeof(int[N]), cudaMemcpyHostToDevice);

  int* d_out;
  cudaMalloc(&d_out, sizeof(int));

  mulsum<<<1, N, sizeof(int)*N>>>(d_a, d_b, d_out, N);
  /*             ▲
   *             │    ┌─────────────────────────────────────┐
   *             └────┤Size of shared memory to be allocated│
   *                  │         for kernel launch           │
   *                  └─────────────────────────────────────┘
   */

  int out;
  cudaMemcpy(&out, d_out, sizeof(int), cudaMemcpyDeviceToHost);
  printf("%d\n", out);

  cudaFree(d_a);
  cudaFree(d_b);
  cudaFree(d_out);
  return 0;
}
```

```console
$ nvcc mul-sum-reduce.cu -o msr
$ ./msr
285
```

Notice how our shared memory is declared:
```cuda
  extern __shared__ int tmp[];
```

It is `external` because we are not allocating the memory in our kernel; it's allocated by the cuda runtime when we pass the third parameter to the kernel launch parameters:

```cuda
mulsum<<<1, N, sizeof(int)*N>>>(d_a, d_b, d_out, N);
               ▲
               │      ┌─────────────────────────────────────┐
               └──────┤Size of shared memory to be allocated│
                      │         for kernel launch           │
                      └─────────────────────────────────────┘
```

There can only be one segment of shared memory in a kernel launch, so the shared memory segment will be interpreted as whatever type we declare our shared memory with.
In this case, it's an array of ints.
Although there is strictly one _segment_ of shared memory in a kernel launch, you can still declare multiple variables as `__shared__`, so long as they all fit in the allocated shared memroy.

We also introduced another CUDA extension to the host language: `__syncthreads()`.
`__syncthreads()` is a _fence_ or _barrier_, a point which no thread in that block can cross until all threads have reached it.
`__syncthreads()`
There are many other CUDA primitives for primitive atomic/parallel/synchronization operations, such as `atomicAdd`.

### CUDA Mat-Vec Multiply

We again return to our `matvecmul` example, this time armed with some knowledge about the CUDA runtime and some software and hardware abstractions.

```cuda
#include <cstdio>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void matvecmul(int* mat, int* vec, int* outv,
                          int m, int n) {

  int rowidx = blockIdx.x;
  int colidx = threadIdx.x;

  extern __shared__ int tmp[];

  if (colidx < n && rowidx < m) {
    tmp[colidx] = mat[colidx + (rowidx * n)] * vec[colidx];

    __syncthreads();

    if (colidx == 0) {
      int sum = 0;
      for (int i=0; i < n; i++)
        sum += tmp[i];
      outv[rowidx] = sum;
    }
  }
}

int main() {
  #define M 10
  #define N 15

  int a[M*N];
  for (int i=0; i < M*N; i++) a[i] = i;

  int b[N];
  for (int i=0; i < N; i++) b[i] = i;

  int* d_a;
  cudaMalloc(&d_a, sizeof(int[M*N]));
  cudaMemcpy(d_a, a, sizeof(int[M*N]), cudaMemcpyHostToDevice);

  int* d_b;
  cudaMalloc(&d_b, sizeof(int[N]));
  cudaMemcpy(d_b, b, sizeof(int[N]), cudaMemcpyHostToDevice);

  int* d_c;
  cudaMalloc(&d_c, sizeof(int[M]));

  matvecmul<<<M, N, sizeof(int[N])>>>(d_a, d_b, d_c, M, N);

  int c[M];
  cudaMemcpy(c, d_c, sizeof(int[M]), cudaMemcpyDeviceToHost);

  cudaFree(d_a);
  cudaFree(d_b);
  cudaFree(d_c);
  return 0;
}
```

After adding some printing code to our example above, we get the following:
```console
$ nvcc matvecmul.cu && ./a.out
Matrix:
   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14
  15   16   17   18   19   20   21   22   23   24   25   26   27   28   29
  30   31   32   33   34   35   36   37   38   39   40   41   42   43   44
  45   46   47   48   49   50   51   52   53   54   55   56   57   58   59
  60   61   62   63   64   65   66   67   68   69   70   71   72   73   74
  75   76   77   78   79   80   81   82   83   84   85   86   87   88   89
  90   91   92   93   94   95   96   97   98   99  100  101  102  103  104
 105  106  107  108  109  110  111  112  113  114  115  116  117  118  119
 120  121  122  123  124  125  126  127  128  129  130  131  132  133  134
 135  136  137  138  139  140  141  142  143  144  145  146  147  148  149

Vector:
   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14

Output:
1015 2590 4165 5740 7315 8890 10465 12040 13615 15190
```

<a href="https://mlochbaum.github.io/BQN/try.html#code=TXVsIOKGkCAry53iiJjDl+KOiTHigL/iiJ4KCm3ihpAxMOKAvzE14qWK4oaVMjAwCnbihpDihpUxNQoKbSBNdWwgdg==" target="blank">
This BQN example verifies the output from our CUDA program:
</a>
```
   Mul ← +˝∘×⎉1‿∞
   m←10‿15⥊↕200
   v←↕15
   m Mul v
⟨ 1015 2590 4165 5740 7315 8890 10465 12040 13615 15190 ⟩
```

In our CUDA C example, we launch a block for each row of our matrix.
This way, we can share memory between each thread operating on a given row of the matrix.
A single thread per row can then perform the sum reduction and assign the value to the index in the output vector.

# C++ on the Host

There are many layers of abstraction commonly used in HPC for running code on a GPU.
The abstractions used in this post will be limited to those shipped with the most recent stable distribution of the NVIDIA CUDA Toolkit at the time of writing (11.5) along with the abstractions recommended by the lead HPC programming models architect at NVIDIA, Bryce Adelstein Lelbach.
These abstractions are almost entirely _compile-time_ abstractions, which means they incur no runtime cost over the raw C equivilant.
In particular, we will use the structure template `mdspan`, which is likely going to be a part of standard ISO C++23.
`mdspan` is extremely useful on the host as well, so we'll use it in our C++ host example.
<a href="https://youtu.be/KK3JXvSiJG4" target="blank"> See Bryce's talk on C++ standard parallelism here.  </a>

In our C++ examples, we'll use the following type aliases for matrices and vectors:
```cpp
#include <mdspan.hpp> // single-header version from github
namespace stdex = std::experimental;
using mat_t = stdex::mdspan<
    double,
    stdex::extents<3, 3>
  >;
using vec_t = stdex::mdspan<
    double,
    stdex::extents<3>
  >;
```

Our setup is then as follows:
```cpp
int main() {
  double dm[9];
  const auto m = mat_t(dm);
  std::iota(m.data(), m.data()+9, 0);

  double dv[] = { 2., 2., 2. };
  const auto v = vec_t(dv);

  double dout[3] = {0};
  const auto out = vec_t(dout);

  matvecmul(m, v, out);

  return 0;
}
```

If we only compute the solution on the host, everything works the same way:
```cuda
void matvecmul(mat_t m, vec_t v, vec_t out) {
  for (int i=0; i < m.static_extent(0); i++)
    for (int j=0; j < m.static_extent(1); j++)
      out(i) += v(j) * m(i, j);
}
```

Notice how we can index our matrix as we would expect to: with multidimensional indexing.
Notice also how we can call `static_extent` on our matrix - this determines the length of our matrix in a given dimension at _compile time_, which means we perform _less work at runtime then in the pure C version_ while getting nicer indexing and extent calculations.
The compile-time and zero-cost abstractions are significant reasons why developers opt for C++ over C.
For example, compare <a href="https://godbolt.org/z/ssrnz71hs" target="blank">the assembly in this C example</a> to <a href="https://godbolt.org/z/ra8d6arMf" target="blank"> the assembly in this C++ example.  </a>
This is one reason that most compiler vendors choose to write their compilers in C++ over C (eg GNU's GCC, LLVM, Cray CC, IBM XL, Intel compiler suite, etc).

Let's now compute our `matvecmul` with the work broken up into isolated units, just as before.

We'll use the following additional type aliases:
```cpp
using stdex::full_extent;
using stdex::submdspan;
using tuplespace_t = stdex::mdspan<
    double,
    stdex::extents<3, 3, 2>
  >;
using tuplespace_row_t = stdex::mdspan<
    double,
    stdex::extents<3, 2>
  >;
```

The setup for the tuplespace and the units of work also look just about the same:
```cpp
double unit_of_work(tuplespace_row_t tsr) {
  double sum = 0;
  for (int i=0; i < tsr.static_extent(0); i++)
    sum += tsr(i, 0) * tsr(i, 1);
  return sum;
}

void matvecmul_on_tuplespace(mat_t m, vec_t v, vec_t out) {
  double tuplespace_data[2 * m.static_extent(0) * m.static_extent(1)];
  const auto ts = tuplespace_t(tuplespace_data);

  for (int i=0; i < m.static_extent(0); i++)
    for (int j=0; j < m.static_extent(1); j++) {
      ts(i, j, 0) = m(i, j);
      ts(i, j, 1) = v(i);
    }

  for (int i=0; i < m.static_extent(0); i++)
    out(i) = unit_of_work(submdspan(ts, i, full_extent, full_extent));
}
```

# CUDA C++

```cpp

__global__
void unit_of_work(tuplespace_t ts, vec_t out) {
  // row of tuplespace, and index in output
  const auto tid = threadIdx.x;
  const auto tsr = submdspan(ts, tid, full_extent, full_extent);
  double sum = 0;
  for (int i=0; i < tsr.static_extent(0); i++)
    sum += tsr(i, 0) * tsr(i, 1);
  out(tid) = sum;
}

void matvecmul_on_tuplespace(mat_t m, vec_t v, vec_t out) {
  constexpr std::size_t ts_size = 2 * m.static_extent(0) * m.static_extent(1);
  double tuplespace_data[ts_size];
  const auto ts = tuplespace_t(tuplespace_data);

  for (int i=0; i < m.static_extent(0); i++)
    for (int j=0; j < m.static_extent(1); j++) {
      ts(i, j, 0) = m(i, j);
      ts(i, j, 1) = v(i);
    }

  // Allocate memory for tuplespace on device, copy to device
  double *dev_tuplespace_data = nullptr;
  cudaMalloc(&dev_tuplespace_data, ts_size * sizeof(double));
  cudaMemcpy(dev_tuplespace_data, tuplespace_data,
    ts_size * sizeof(double), cudaMemcpyHostToDevice);
  const auto dev_ts = tuplespace_t(dev_tuplespace_data);

  // Allocate memory for output vector on device
  double *dev_out_data = nullptr;
  cudaMalloc(&dev_out_data, m.static_extent(0) * sizeof(double));
  const auto dev_out = vec_t(dev_out_data);

  // Perform units of work on m.static_extent(0) GPU threads
  cu::unit_of_work<<<m.static_extent(0), 1>>>(dev_ts, dev_out);

  // Copy output vector from device to host
  cudaMemcpy(out.data(), dev_out.data(), ts_size * sizeof(double), cudaMemcpyDeviceToHost);
}
```

<!---
<center>
<div class="mermaid">
graph TD
  T(tests) -- > C(correctness)
  I(initial impl) -- > C
  C -- > P(perf analysis)
  P -- > A(algorithm analysis)
  P -- > CC(code generation)
  A -- > D(distribution)
</div>
</center>
--->

# BQN Example

Personally, I use BQN to prototype solutions to problems and to better understand the fundamental algorithms at play; you don't have to know an APL in order to understand this, but it might be helpful.
Feel free to skip this section; it is not critical to understanding the concepts.

<a href="https://mlochbaum.github.io/BQN/try.html#code=4oCiU2hvdyBtYXQg4oaQIDPigL8z4qWK4oaVMTAK4oCiU2hvdyB2ZWMg4oaQIDPipYoyCivLneKOiTEgbWF0w5d2ZWMK" target="blank">Here's a permalink to the BQN snippet.</a>

```
   # Same matrix as in our C example
   mat ← 3‿3⥊↕10
┌─       
╵ 0 1 2  
  3 4 5  
  6 7 8  
        ┘
   # Same vector as in our C example
   vec ← 3⥊2
⟨ 2 2 2 ⟩

   +˝⎉1 mat×vec
⟨ 6 24 42 ⟩
```

The core algorithm is seen in the final expression:

```
+˝⎉1 mat×vec
▲    ▲
│    │     ┌───────────────────────────┐
│    └─────┤Multiply rows of mat by vec│
│          │        element-wise       │
│          └───────────────────────────┘
│     ┌─────────────────────────┐
│     │Sum-reduce rows of matrix│
└─────┤ resulting from mat×vec  │
      └─────────────────────────┘
```

Alternatively:

<center>
<img height=300 src="/images/hpc-101-matvec/bqn-matvecmul-explain.png" alt="Try BQN explanation of matvecmul"/>
</center>

# Links, References, Additional Reading

* <a href="https://mlochbaum.github.io/BQN/try.html#code=4oCiU2hvdyBtYXQg4oaQIDPigL8z4qWK4oaVMTAK4oCiU2hvdyB2ZWMg4oaQIDPipYoyCivLneKOiTEgbWF0w5d2ZWMK" target="blank">BQN matvecmul example</a>
* <a href="https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.2-Multiplying-Matrices-and-Vectors/" target="blank">Matrix-Vector Product image</a>
* <a href="https://www.cs.utexas.edu/~lin/cs380c/handout27.pdf" target="blank">UT Austin slides on loop-carried dependencies and parallelism</a>
* <a href="https://www.worldcat.org/title/how-to-write-parallel-programs-a-first-course/oclc/912171709&referer=brief_results" target="blank">_How to Write Parallel Programs: A First Course_</a>
* <a href="https://thrust.github.io/doc/group__transformed__reductions_ga0d4232a9685675f488c3cc847111e48d.html" target="blank">Thrust parallel algorithms library</a>
* <a href="https://adspthepodcast.com/2021/11/12/Episode-51.html" target="blank"> ADSP podcast episode from the lead HPC architect at NVIDIA discussing speed vs efficiency</a>
* <a href="https://youtu.be/KK3JXvSiJG4" target="blank"> Bryce Adelstein Lelbach's talk on C++ standard parallelism </a>
* <a href="https://github.com/kokkos/mdspan/blob/single-header/mdspan.hpp" target="blank"> Kokkos `mdspan` single header </a>
* <a href="https://www.nvidia.com/content/GTC-2010/pdfs/2131_GTC2010.pdf" target="blank">CUDA C Introduction Slides</a>
* <a href="https://github.com/uysalere/cuda-matrix-vector-multiplication" target="blank"> More sophisticated CUDA matrix-vector product implementations </a>


{% include disclaimer.html %}
