<!DOCTYPE HTML>
<html lang="en" class="ayu" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Notes</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="./mdbook-admonish.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "coal" : "ayu";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('ayu')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="about.html">About</a></li><li class="chapter-item expanded affix "><li class="part-title">Blog</li><li class="chapter-item expanded "><a href="csblog/2024-9-4-Debugging-In-Parallel.html">Debugging in Parallel 9/4/2024</a></li><li class="chapter-item expanded "><a href="notes/2024-8-31-Linux-Perf-Notes.html">The Linux Perf Tool 8/31/2024</a></li><li class="chapter-item expanded "><a href="notes/values.html">Values 8/26/2024</a></li><li class="chapter-item expanded "><a href="notes/2024-8-30-Shell.html">Shell+Scripting 8/24/2024</a></li><li class="chapter-item expanded "><a href="notes/editors.html">Editors+Tools 8/24/2024</a></li><li class="chapter-item expanded "><a href="csblog/2023-6-1-C-VLA-Implementation.html">Understanding VLA 6/1/2023</a></li><li class="chapter-item expanded "><a href="csblog/2022-5-2-BQN-reflections.html">BQN and Reflections on the Joy of Programming 5/2/2022</a></li><li class="chapter-item expanded "><a href="csblog/2022-2-2-LLVM-Development-On-NixOS.html">LLVM Development on NixOS 2/2/2022</a></li><li class="chapter-item expanded "><a href="csblog/2022-2-10-CUDA-101-Matvec.html">CUDA 101: Matrix-Vector Product 2/10/2022</a></li><li class="chapter-item expanded "><a href="csblog/2022-12-12-Compiler-Perf-Debugging.html">Debugging Performance in Compilers 12/12/2022</a></li><li class="chapter-item expanded "><a href="csblog/2022-1-15-Std-Expected.html">std::expected And Why It's Awesome 1/15/2022</a></li><li class="chapter-item expanded "><a href="csblog/2021-3-7-GTest-Type-Value-Params.html">GTest Type and Value Parameterized Tests 3/7/2021</a></li><li class="chapter-item expanded "><a href="csblog/2021-3-6-Spack-Development-3.html">Spack for Package Development Part 3 3/6/2021</a></li><li class="chapter-item expanded "><a href="csblog/2021-3-6-Clang-Tools-Lambda.html">Clang Tools for Checking Domain-Specific Errors 3/6/2021</a></li><li class="chapter-item expanded "><a href="csblog/2021-3-5-Spack-Development-2.html">Spack for Package Development Part 2 3/5/2021</a></li><li class="chapter-item expanded "><a href="csblog/2021-3-4-Spack-Development-1.html">Spack for Package Development Part 1 3/4/2021</a></li><li class="chapter-item expanded "><a href="csblog/2021-12-23-std-mdspan-Response.html">A Look at std::mdspan 12/23/2021</a></li><li class="chapter-item expanded "><a href="csblog/2021-10-24-Popular-Languages-1965.html">Using the Most Popular Programming Languages of the '60s 10/24/2021</a></li><li class="chapter-item expanded "><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html">One Problem, Four Languages, Two Paradigms 10/19/2021</a></li><li class="chapter-item expanded "><a href="csblog/2021-10-11-BQN-Cpp-CUDA.html">BQN and CUDA C++ LeetCode Solutions 10/11/2021</a></li><li class="chapter-item expanded affix "><li class="part-title">Coffee</li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-11-Best-Espresso-In-Portland.html">Best Espresso In Portland (6/11/2023)</a></li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-22-Sterling.html">Sterling (6/22/2023)</a></li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-14-Deadstock.html">Deadstock (6/14/2023)</a></li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-22-Barista.html">Barista (6/22/2023)</a></li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-13-Never-Coffee.html">Never Coffee (6/13/2023)</a></li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-15-Upper-Left-Roasters.html">Upper Left Roasters (6/15/2023)</a></li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-14-Abba.html">Abba (6/14/2023)</a></li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-15-Rose-City-Coffee.html">Rose City Coffee (6/15/2023)</a></li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-14-Sterling.html">Sterling (6/14/2023)</a></li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-21-Superjoy.html">Superjoy (6/21/2023)</a></li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-13-Beginners-Guide.html">Beginners Guide (6/13/2023)</a></li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-19-Seattle-Trip-Report.html">Seattle Trip Report (6/19/2023)</a></li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-15-Adapt-Coffee.html">Adapt Coffee (6/15/2023)</a></li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-13-Coava.html">Coava (6/13/2023)</a></li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-14-PDX-Espresso-Research.html">PDX Espresso Research (6/14/2023)</a></li><li class="chapter-item expanded "><a href="coffeeblog/2023-6-15-Nossa-Familia-Coffee.html">Nossa Familia Coffee (6/15/2023)</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Notes</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="welcome"><a class="header" href="#welcome">Welcome</a></h1>
<p>I find compilers, programming languages and performance really interesting.</p>
<p>My personal notes and blog posts are enumerated on the left, or you can use the search bar at the top.</p>
<p>I work on <a href="https://developer.nvidia.com/hpc-sdk">the NVHPC C, C++ and Fortran compilers at NVIDIA</a>.</p>
<!-- ```admonish tip title="Heads up!" -->
<!-- For code blocks with shell commands, I use `;` instead of the typical `$`. -->
<!-- This way, if you use the button in the top-right of the code block to copy the commands, -->
<!-- the semicolon doesn't mess up the commands like `$` does. -->
<!-- ``` -->
<hr />
<ul>
<li><a href="https://www.youtube.com/@AsherMancinelli">Youtube</a></li>
<li><a href="https://www.linkedin.com/in/asher-mancinelli-bb4a56144/">LinkedIn</a></li>
</ul>
<hr />
<font size="-1">
  <em>
    These views do not in any way represent those of NVIDIA or any other organization or institution that I am professionally associated with.
    These views are entirely my own.
  </em>
</font>
<h1 id="debugging-in-parallel"><a class="header" href="#debugging-in-parallel">Debugging in Parallel</a></h1>
<p>Let’s say you have a compiler pass that performs some transformation.
You check in a change to the compiler…</p>
<pre><code class="language-c++">void my_pass(ast_t *ast) {
    // perform some transformation
}
</code></pre>
<p>…and you get a bug report back.</p>
<p>Something is broken, but it only shows up in a huge translation unit, and your pass runs thousands of times.
How do you reduce the problem?</p>
<h2 id="constrain-the-problem"><a class="header" href="#constrain-the-problem">Constrain the Problem</a></h2>
<p>I break the code before and after my patch into two new functions, each called from the old depending on some environment variable:</p>
<pre><code class="language-c++">void old_pass(ast_t *ast) { /* ... */ }
void new_pass(ast_t *ast) { /* ... */ }

void my_pass(ast_t *ast) {
    static int num_calls = 1;
    char *env;
    fprintf(stderr, "num_calls=%d\n", num_calls);

    if (num_calls == std::atoi(std::getenv("USE_NEW_CODE"))) {
        new_pass(ast);
    } else {
        old_pass(ast);
    }

    num_calls++;
}
</code></pre>
<p>I can then change from the command line which code path the compiler will use when my pass is run:</p>
<pre><code class="language-bash"># Disable my new change
USE_NEW_CODE=0 build/bin/clang ./bug.c -O3

# Enable my new change only on the first call
USE_NEW_CODE=1 build/bin/clang ./bug.c -O3
</code></pre>
<div id="admonition-tip" class="admonition admonish-tip" role="note" aria-labelledby="admonition-tip-title">
<div class="admonition-title">
<div id="admonition-tip-title">
<p>Tip</p>
</div>
<a class="admonition-anchor-link" href="csblog/2024-9-4-Debugging-In-Parallel.html#admonition-tip"></a>
</div>
<div>
<p>Rather than using environment variables, the same can be accomplished with clang’s <code>cl::opt</code> command line options.
<code>opt</code> has the command line flag <code>-opt-bisect-limit=&lt;limit&gt;</code> for bisecting LLVM passes, and you can do the same thing in your own pass.</p>
</div>
</div>
<p>If we then turn this build and run step into a script that runs in a temporary directory,
we’re almost ready to test the entire problem space in parallel:</p>
<pre><code class="language-bash">$ cat &gt;test.sh &lt;&lt;EDO
#!/usr/bin/env bash
start=$PWD
pushd $(mktemp -d)
USE_NEW_CODE=$1 $start/build/bin/clang $start/bug.c -O3
./a.out &amp;&amp; echo $* pass || echo $* fail
EOD
</code></pre>
<p>Now, we could run this script from <code>1</code> up to the number of times your optimization kicks in for the failing test case, but we can do better.
We can use GNU parallel<sup class="footnote-reference"><a href="#gnu_par">1</a></sup> to test the entire problem space on all our cores:</p>
<pre><code class="language-bash">$ seq 1000 | parallel --bar -j `nproc` ./test.sh {} '&amp;&gt;' logs/{}.txt
100% 400:0=0s 400
$ grep -r fail logs/
out-314.sh:314: fail
out-501.sh:501: fail
</code></pre>
<p>This gives you every individual instance in which your new code caused the failure (in case there were multiple).
You can also bisect the failures by using a minimum and maximum instead of only enabling your new code for one single instance, in case this does not work.</p>
<div id="admonition-tip-1" class="admonition admonish-tip" role="note" aria-labelledby="admonition-tip-1-title">
<div class="admonition-title">
<div id="admonition-tip-1-title">
<p>Tip</p>
</div>
<a class="admonition-anchor-link" href="csblog/2024-9-4-Debugging-In-Parallel.html#admonition-tip-1"></a>
</div>
<div>
<p><code>creduce</code> along with LLVM’s <code>bugpoint</code> and <code>llvm-reduce</code> can also be helpful, but not when your test application is segfaulting.
<code>creduce</code> tends to create all kinds of segfaults and works best when you have more specific output in the failure case you can grep for.</p>
</div>
</div>
<div class="footnote-definition" id="gnu_par"><sup class="footnote-definition-label">1</sup>
<p><a href="https://www.gnu.org/software/parallel/">GNU parallel is a shell tool for executing jobs in parallel using one or more computers. A job can be a single command or a small script that has to be run for each of the lines in the input.</a></p>
</div>
<!--
layout: post
title: Linux Perf Notes
permalink: /perf
category: linux, c++, perfanalysis
wip: false
cat: cs
-->
<div id="admonition-toc" class="admonition admonish-example" role="note" aria-labelledby="admonition-toc-title">
<div class="admonition-title">
<div id="admonition-toc-title">
<p>TOC</p>
</div>
<a class="admonition-anchor-link" href="notes/2024-8-31-Linux-Perf-Notes.html#admonition-toc"></a>
</div>
<div>
<ul>
<li><a href="notes/2024-8-31-Linux-Perf-Notes.html#linux-performance-analysis">Linux Performance Analysis</a>
<ul>
<li><a href="notes/2024-8-31-Linux-Perf-Notes.html#approaches">Approaches</a></li>
</ul>
</li>
<li><a href="notes/2024-8-31-Linux-Perf-Notes.html#the-linux-perf-tool">The Linux Perf Tool</a>
<ul>
<li><a href="notes/2024-8-31-Linux-Perf-Notes.html#perf-events-and-perf-list">Perf Events and Perf List</a></li>
<li><a href="notes/2024-8-31-Linux-Perf-Notes.html#perf-stat">Perf Stat</a></li>
<li><a href="notes/2024-8-31-Linux-Perf-Notes.html#perf-record">Perf Record</a></li>
<li><a href="notes/2024-8-31-Linux-Perf-Notes.html#perf-report">Perf Report</a></li>
<li><a href="notes/2024-8-31-Linux-Perf-Notes.html#flamegraph">FlameGraph</a></li>
</ul>
</li>
<li><a href="notes/2024-8-31-Linux-Perf-Notes.html#pov-ray">POV-Ray</a>
<ul>
<li><a href="notes/2024-8-31-Linux-Perf-Notes.html#building-pov-ray">Building POV-Ray</a></li>
</ul>
</li>
<li><a href="notes/2024-8-31-Linux-Perf-Notes.html#further-reading">Further Reading</a>
<ul>
<li><a href="notes/2024-8-31-Linux-Perf-Notes.html#references">References</a></li>
</ul>
</li>
</ul>
</div>
</div>
<h1 id="linux-performance-analysis"><a class="header" href="#linux-performance-analysis">Linux Performance Analysis</a></h1>
<p>Perf analysis is <em>super</em> interesting to me - why does an application run faster or slower under certain conditions?
Why does one compiler (or compiler switch) produce a faster application than any other?
I want to know what tricks my compiler is doing to speed up my app.</p>
<p>This post is an example performance analysis of an application called <em>POV-Ray</em>.
I explain my benchmark choice in the <a href="notes/2024-8-31-Linux-Perf-Notes.html#pov-ray">section on POV-Ray</a>.</p>
<h2 id="approaches"><a class="header" href="#approaches">Approaches</a></h2>
<p>There are two ways I think about approaching performance analysis:
a <em>top-down</em> approach and a <em>bottom-up</em> approach.
I use perf for both of these approaches, so we’ll start with an overview of perf
and then apply these approaches to povray.</p>
<div id="admonition-key-terms" class="admonition admonish-tip" role="note" aria-labelledby="admonition-key-terms-title">
<div class="admonition-title">
<div id="admonition-key-terms-title">
<p>Key Terms</p>
</div>
<a class="admonition-anchor-link" href="notes/2024-8-31-Linux-Perf-Notes.html#admonition-key-terms"></a>
</div>
<div>
<p><strong>Top-down approach:</strong> look at the application starting at the root of the call stack.
What does <code>main()</code> look like? What is the application doing at an extremely high level?</p>
<p><strong>Bottom-up approach:</strong> Look at the fine-grained details of the application.
What instructions are being executed? Is the application memory-, network-, or compute-bound?
Where are these instructions coming from in the source?</p>
</div>
</div>
<h1 id="the-linux-perf-tool"><a class="header" href="#the-linux-perf-tool">The Linux Perf Tool</a></h1>
<p>So how do we see into the guts of this app as it’s running?</p>
<p>IMO the best place to start (and often finish) is with the <code>perf</code> tool<sup class="footnote-reference"><a href="#man_perf">1</a></sup>.
Perf is a part of the linux project, so it’s supported on all linux platforms.</p>
<div id="admonition-default" class="admonition admonish-tip" role="note">
<div>
<p>If you don’t already have it, you can <em>probably</em> install it from your package manager as <code>linux-tools-common</code>:</p>
<pre><code class="language-bash">sudo apt install linux-tools-common linux-tools-`uname -r`
</code></pre>
</div>
</div>
<p>Perf has lots of commands, but the main two you’ll need to interact with are <code>perf-record</code> and <code>perf-report</code>.
The workflow is generally:</p>
<pre><code class="language-bash">; perf stat -- ./a.out

# This leaves the recorded data in ./perf.data
; perf record -- ./a.out
; perf report
</code></pre>
<p>Perf report helps you drill into the call stack to see <strong>where samples were recorded</strong> in the application,
even down to the assembly instructions that corresponded to samples.</p>
<h2 id="perf-events-and-perf-list"><a class="header" href="#perf-events-and-perf-list">Perf Events and Perf List</a></h2>
<p>Note that in the previous section I said <code>perf report</code> helps you view
<em>where samples were recorded</em> and not <em>where time was spent</em>;
perf watches for <em>events</em> and takes periodic samples of what’s happening on the system when it wakes up.
These samples do not necessarily indicate where user-time is being spent.</p>
<p>Depending on your system, kernel configuration, and the configuration of perf itself, you’ll have different events available to profile.</p>
<p>Run <code>perf list</code><sup class="footnote-reference"><a href="#man_perf_list">2</a></sup> to get a view of all the sampling events you can use on your system:</p>
<pre><code class="language-bash">; perf list
List of pre-defined events (to be used in -e):

  branch-instructions OR branches                    [Hardware event]
  branch-misses                                      [Hardware event]
  bus-cycles                                         [Hardware event]
  cache-misses                                       [Hardware event]
...
</code></pre>
<p>The list of samplable events is rather long and often has architecture- and cpu-specific entries,
so I’ll leave it as an excercise for the reader to see what perf events are
available to you on <em>your</em> system, and learn what they all mean.</p>
<p>The <code>-F</code> flag tells perf what observation frequency it should use when recording samples -
often <code>-F 99</code> (for 99 hertz) is a good place to start; you get enough data to gain insights without being overwhelmed.
You can always turn it down for longer-running applications or when you’re sampling many different events.</p>
<h2 id="perf-stat"><a class="header" href="#perf-stat">Perf Stat</a></h2>
<p>The best place to start with perf is often <code>perf stat</code>.
This command gives a brief overview of total samples of events.
If something from perf stat’s report stands out, you can use perf record with that
event to drill into the sources of those samples.</p>
<p>A perf stat run might look like this:</p>
<pre><code class="language-bash">; perf stat -- ./a.out
 Performance counter stats for './a.out':

         21,829.89 msec task-clock                #    0.963 CPUs utilized          
             7,097      context-switches          #  325.105 /sec                   
                 1      cpu-migrations            #    0.046 /sec                   
             5,062      page-faults               #  231.884 /sec                   
    70,001,621,188      cycles                    #    3.207 GHz                    
   155,086,020,805      instructions              #    2.22  insn per cycle         
     9,013,464,722      branches                  #  412.896 M/sec                  
        49,795,347      branch-misses             #    0.55% of all branches        

      22.661088635 seconds time elapsed

      21.785643000 seconds user
       0.051956000 seconds sys
</code></pre>
<h2 id="perf-record"><a class="header" href="#perf-record">Perf Record</a></h2>
<p><code>perf record</code> is the primary command for recording samples about your application or system.</p>
<p>My perf record commands usually look like this:</p>
<pre><code class="language-bash">; export \
    APP=./a.out \
    FREQ=99 \
    EVENTS="cycles,instructions,branches,loads,task-clock"
; perf record \
    --output perf-$APP.data \
    --call-graph fp \
    -F $FREQ -e $EVENTS \
    -- taskset 0x2 ./a.out &gt;/dev/null
</code></pre>
<p>I’m using <code>--call-graph fp</code> because I want perf to record callgraph information
using the frame pointer - this is why you must often build your application with
the <code>-fno-omit-frame-pointer</code> compiler flag (more on that later).</p>
<p>I’m also using <code>taskset 0x2</code> because I only want the app to run on a single core
in this example; perf can also record data for <em>everything running on your entire system</em>
if you would like it to - or just on a specific core or for a specific application.</p>
<h2 id="perf-report"><a class="header" href="#perf-report">Perf Report</a></h2>
<p><code>perf report</code> will give you a TUI report like this by default:</p>
<pre><code class="language-bash">Samples: 88K of event 'cycles', Event count (approx.): 72137516526
  Children      Self  Command  Shared Object              Symbol
+   99.61%     0.00%  povray   libboost_thread.so.1.74.0  [.] 0x00007f61e2d6f0cb
+   99.54%     0.00%  povray   povray                     [.] pov::Task::TaskThread
+   97.41%     0.03%  povray   povray                     [.] pov::Trace::ComputeTextureColour
+   97.40%     0.06%  povray   povray                     [.] pov::Trace::ComputeOneTextureColour
...
</code></pre>
<p>Notice the event used for the report is given in the first line.</p>
<p><code>perf report --stdio</code> gives the same information initially, but with all the call stacks expanded;
this may get overwhelming.
For a the 20 second recording I took for this example, the stdio output of
perf report was over 10k lines long:</p>
<pre><code class="language-bash">; perf report --stdio|wc -l
10010
</code></pre>
<p>From inside the TUI you can press <code>h</code> to get a list of all the available commands,
so I won’t enumerate them here.</p>
<p>I usually run perf report with the <code>-G</code> flag, which is shorthand for <code>--inverted</code>,
meaning the callgraph representation is inverted.</p>
<p>You may have noticed that the snippet from perf report I pasted above starts
with two columns: <code>Self</code> and <code>Children</code>.</p>
<div id="admonition-the-self-and-children-columns" class="admonition admonish-tip" role="note" aria-labelledby="admonition-the-self-and-children-columns-title">
<div class="admonition-title">
<div id="admonition-the-self-and-children-columns-title">
<p>The <code>Self</code> and <code>Children</code> columns</p>
</div>
<a class="admonition-anchor-link" href="notes/2024-8-31-Linux-Perf-Notes.html#admonition-the-self-and-children-columns"></a>
</div>
<div>
<p>The <em>Children</em> indicates the percentage of samples taken in that stack frame
<em>or any of its children</em> - meaning any samples recorded while in this stack
frame or that of any functions called from the current stack frame.</p>
<p>The <em>Self</em> column is more significant: it indicates what percentage of samples
were taken <em>in the given stack frame only</em> - meaning instructions coming from
that function alone, and not any functions it calls.</p>
<p>The <code>main()</code> function is always at the top, since it calls all other function.
However, unless your entire program was inlined into the main routine, its <em>Self</em>
column is likely very low since most of the work being done is probably happening
elsewhere.</p>
</div>
</div>
<h2 id="flamegraph"><a class="header" href="#flamegraph">FlameGraph</a></h2>
<p>I mention Brendan Gregg<sup class="footnote-reference"><a href="#brendan_gregg_blog">3</a></sup> a few times in this post, and you should get familiar with
him and his work.
His blog has many pearls and he might have a one-liner for exactly your use case.</p>
<p>One of his other contributions is the FlameGraph repo<sup class="footnote-reference"><a href="#brendan_gregg_flamegraph">4</a></sup>.</p>
<p>Remember how our perf report contains over 10k lines of reporting for just a single application running for ~20 seconds?
His flamegraph repo gives us a way to visualize and gain insights from <em>all</em> of that data at a very high level
by creating a flamegraph from perf’s recorded data.</p>
<div id="admonition-note" class="admonition admonish-tip" role="note" aria-labelledby="admonition-note-title">
<div class="admonition-title">
<div id="admonition-note-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="notes/2024-8-31-Linux-Perf-Notes.html#admonition-note"></a>
</div>
<div>
<p>The FlameGraph repo actually knows how to deal with other profilers too, like DTrace and SystemTap.</p>
</div>
</div>
<p>A workflow for generating a flamegraph might look like this:</p>
<pre><code class="language-bash"># build and profile your application
; make
; perf record --call-graph fp -- ./a.out

; git clone https://github.com/brendangregg/FlameGraph ../FlameGraph

; perf script \
    | ../FlameGraph/stackcollapse-perf.pl \
    | ../FlameGraph/flamegraph.pl \
    &gt; flamegraph.svg
</code></pre>
<div id="admonition-note-1" class="admonition admonish-tip" role="note" aria-labelledby="admonition-note-1-title">
<div class="admonition-title">
<div id="admonition-note-1-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="notes/2024-8-31-Linux-Perf-Notes.html#admonition-note-1"></a>
</div>
<div>
<p>The FlameGraph scripts have actally been merged into the linux kernel’s repo,
so perf built for a newer kernel has FlameGraph as a built-in script, used like so:</p>
<pre><code class="language-bash">; perf script flamegraph -- ./a.out

# alternatively...
; perf record -- ./a.out
; perf script report flamegraph
</code></pre>
<p>This requires python scripting support built into perf, which my perf build does
not have, so I can’t test it myself. I still use the scripts from Brendan’s repo.</p>
</div>
</div>
<h1 id="pov-ray"><a class="header" href="#pov-ray">POV-Ray</a></h1>
<p>Povray<sup class="footnote-reference"><a href="#povray">5</a></sup> is a 3d graphics code commonly used for benchmarking - it’s part of CPU benchmarking suites from OpenBenchmarking<sup class="footnote-reference"><a href="#openbench_povray">6</a></sup> and spec2017<sup class="footnote-reference"><a href="#spec2017_povray">7</a></sup>, which means a few things:</p>
<ol>
<li>
<p>It’s reasonably well-optimized.</p>
<p>Compiler writers and hardware vendors don’t care too much about benchmarking
silly code that doesn’t represent what users will actually be running.</p>
</li>
<li>
<p>It’s cross-platform</p>
<p>Part of its utility is that we can compare performance across hardware vendors</p>
</li>
<li>
<p>It’s well-supported by most/all compilers</p>
<p>compiler authors and hardware vendors care about how well POV-Ray runs on their tech,
so we can assume they’ve put effort into handling povray’s code well and ensuring
it builds with their compilers.</p>
</li>
<li>
<p>It doesn’t rely <em>too</em> much on libraries.</p>
<p>OpenBenchmarking and SPEC suites are especially useful for benchmarking
because they are mostly self-contained.</p>
</li>
</ol>
<h2 id="building-pov-ray"><a class="header" href="#building-pov-ray">Building POV-Ray</a></h2>
<p>POV-Ray is opensource, so we can download it and built it ourselves:</p>
<pre><code class="language-bash">; git clone --branch latest-stable git@github.com:POV-Ray/povray.git
; cd povray
; (cd unix; ./preinstall.sh)
</code></pre>
<p>We will build the app with come debug information enabled so we have more visibility into the app’s behavior as it runs:</p>
<pre><code class="language-bash">; ./configure \
    --disable-strip \
    --prefix=$PWD/../povray-gcc-12/ \
    COMPILED_BY="Asher Mancinelli on $(date)" \
    CFLAGS='-fno-omit-frame-pointer' CXXFLAGS='-fno-omit-frame-pointer' \
    CC=gcc-12 CXX=g++-12
; ./unix/povray --version |&amp; grep flags
  Compiler flags:      -pipe -Wno-multichar -Wno-write-strings -fno-enforce-eh-specs -Wno-non-template-friend -g -pg -O3 -ffast-math -march=native -fno-omit-frame-pointer
</code></pre>
<div id="admonition-frame-pointer" class="admonition admonish-tip" role="note" aria-labelledby="admonition-frame-pointer-title">
<div class="admonition-title">
<div id="admonition-frame-pointer-title">
<p>Frame Pointer</p>
</div>
<a class="admonition-anchor-link" href="notes/2024-8-31-Linux-Perf-Notes.html#admonition-frame-pointer"></a>
</div>
<div>
<p>You’ll notice I used the unfortunately-named <code>-fno-omit-frame-pointer</code>.
This tells the compiler to maintain the frame pointer in the frame pointer register (<code>ebp</code> on x86_64 systems);
the compiler might otherwise reuse the register as a general-purpose register,
but we’re going to tell the perf tool to use the frame pointer register for building analyses,
so we need to keep it around.</p>
</div>
</div>
<p>Once we have the app built, we can run the standard benchmark (this takes a while):</p>
<pre><code class="language-bash">; make -j `nproc` install
; ./unix/povray --benchmark &lt;/dev/null
...
Render Options
  Quality:  9
  Bounding boxes.......On   Bounding threshold: 3
  Antialiasing.........On  (Method 1, Threshold 0.300, Depth 3, Jitter 0.30,
 Gamma 2.50)
==== [Rendering...] ========================================================
Rendered 15360 of 262144 pixels (5%)
</code></pre>
<!--

This script watches most of the events I care about and generates all the reports in one place.

```bash
set -x

events="cycles:u,instructions,user_time,cache-misses,branch-misses,task-clock"
freq=99 # sampling frequency
app=$PWD/a.out
config="$*"
name=$(echo "$*" | sed -e 's/ /_/g')

ulimit -Ss unlimited
test -d $name || mkdir $name
pushd $name

perf record \
    --output perf.data \
    --call-graph fp \
    -F $freq -e "$events" \
    -- taskset 0x2 $app >/dev/null

perf report \
    --stdio -G \
    --inline --itrace=i \
    > perf.report

perf stat record \
    -o perf-stat.data \
    -e "$events" \
    -- taskset 0x2 $app >/dev/null

# --stdio much preferred to --stdio2
perf annotate -i perf.data --stdio > perf.annotate

popd
```

I like to create separate directories for all the data on a per-flag basis because I'm trying lots of different flags when investigating a performance change.
This way, each time I want to try another combination of flags, my history is preserved in its own directory and I don't have to wait to look at any reports:

```bash
# whatever directory was created by the above script
d="flags"
perf report -i $d/perf.data
perf stat report $d/perf-stat.data
$PAGER $d/perf.annotate
$PAGER $d/perf.report
```

NOTE: make sure you build with `-fno-omit-frame-pointer` so perf can give you reasonable traces.
Debug info works _okayyy_ but you'll end up with _massive_ data dumps that take forever to load into perf-report and other tools.

## Why is my app slower when I X?

```admonish note
`perf-diff` is worse when name mangling is different (e.g. with Fortran apps) because perf can't match the events up.
```

```bash
make FLAGS="-O0"
perf record ...
make FLAGS="-O3"
perf record ...
perf diff
```

-->
<h1 id="further-reading"><a class="header" href="#further-reading">Further Reading</a></h1>
<p>Truly, read the manpages.
The perf man pages could be more thorough and some commands are not exceptionally
well-documented (looking at you, <code>perf diff</code>), but they are invaluable resources.</p>
<p>Search for Brendan Gregg on YouTube, he has plenty of great talks there.
For example:
<a href="https://www.youtube.com/watch?v=GsMs3n8CB6g"><em>Give me 15 minutes and I’ll change your view of Linux tracing</em></a></p>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<div class="footnote-definition" id="brendan_gregg_blog"><sup class="footnote-definition-label">3</sup>
<p><a href="https://www.brendangregg.com/perf.html">Brendan Gregg’s blog post with perf one-liners. Reread this list several times. What you want is probably already here.</a></p>
</div>
<div class="footnote-definition" id="brendan_gregg_flamegraph"><sup class="footnote-definition-label">4</sup>
<p><a href="https://github.com/brendangregg/FlameGraph">FlameGraph repo</a>. <a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html">See also: his blog post on flamegraphs</a></p>
</div>
<div class="footnote-definition" id="povray"><sup class="footnote-definition-label">5</sup>
<p><a href="https://www.povray.org/download/benchmark.php">  POVRay.org: Benchmarking with POV-Ray  </a></p>
</div>
<div class="footnote-definition" id="openbench_povray"><sup class="footnote-definition-label">6</sup>
<p><a href="https://openbenchmarking.org/test/system/povray">OpenBenchmarking POV-Ray</a></p>
</div>
<div class="footnote-definition" id="spec2017_povray"><sup class="footnote-definition-label">7</sup>
<p><a href="https://www.spec.org/cpu2017/Docs/benchmarks/511.povray_r.html">511.povray_r: SPEC CPU®2017 Benchmark Description</a></p>
</div>
<div class="footnote-definition" id="man_perf"><sup class="footnote-definition-label">1</sup>
<p><a href="https://www.man7.org/linux/man-pages/man1/perf.1.html">man perf</a></p>
</div>
<div class="footnote-definition" id="man_perf_list"><sup class="footnote-definition-label">2</sup>
<p><a href="https://www.man7.org/linux/man-pages/man1/perf-list.1.html">man perf-list</a></p>
</div>
<div class="footnote-definition" id="man_perf_diff"><sup class="footnote-definition-label">8</sup>
<p><a href="https://www.man7.org/linux/man-pages/man1/perf-diff.1.html">man perf-diff</a></p>
</div>
<div class="footnote-definition" id="redhat_flamegraph"><sup class="footnote-definition-label">9</sup>
<p><a href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-flamegraphs_monitoring-and-managing-system-status-and-performance#creating-flamegraphs-over-the-entire-system_getting-started-with-flamegraphs">RedHat blog post on flamegraphs</a></p>
</div>
<h1 id="values-statement"><a class="header" href="#values-statement">Values Statement</a></h1>
<div id="admonition-tldr" class="admonition admonish-abstract" role="note" aria-labelledby="admonition-tldr-title">
<div class="admonition-title">
<div id="admonition-tldr-title">
<p>TL;DR</p>
</div>
<a class="admonition-anchor-link" href="notes/values.html#admonition-tldr"></a>
</div>
<div>
<p>These values motivate and inform how I work.</p>
<p>They are <em>values</em>, meaning I <em>aspire</em> to live up to them and they are at times in tension with each other.</p>
<p>They are: <strong>curiosity, honesty, rigor, communication, empathy.</strong></p>
</div>
</div>
<h2 id="curiosity"><a class="header" href="#curiosity">Curiosity</a></h2>
<ul>
<li>I <em>chose</em> to take an orientation of curiosity towards any problem I work on.
Replace feelings of frustration, anger and disappointment with curiosity when possible.</li>
</ul>
<h2 id="honesty"><a class="header" href="#honesty">Honesty</a></h2>
<ul>
<li>My coworkers should expect everything I say to be my complete and honest understanding.</li>
<li>I ask when I don’t understand something.</li>
<li>When I present a solution, I point out what I do <em>and</em> don’t understand.
I am not afraid to be wrong or change my opinion, publicly.</li>
<li>This includes times when honesty is uncomfortable or inconvenient, including when I have made a mistake.</li>
</ul>
<h2 id="rigor"><a class="header" href="#rigor">Rigor</a></h2>
<ul>
<li>
<p>Any artifact of my work should clearly demonstrate rigor and thoughtfulness.</p>
</li>
<li>
<p>When describing issues or summarizing an investigation, I should include the possible solutions that I see, and give a recommendation.
If someone else must read my report, come up with possible solutions and give a recommendation, that report was unfinished.
When assigning me a problem, my assigner should expect a rigorous investigation, a range of possible solutions, and a preferred recommendation.</p>
</li>
<li>
<p>Rigor implies good engineering. I write quality code.</p>
</li>
</ul>
<h2 id="effective-communication"><a class="header" href="#effective-communication">Effective Communication</a></h2>
<ul>
<li>Effective written and spoken communication must be a core competency.
Emails, comments, code review, personal messages and official documents convey the appropriate tone and level of detail.</li>
<li>Comments in software are the guideposts for future developers, most importantly myself.</li>
<li>When speaking, I am either prepared to speak or explain that I will speak more at a later time when I <em>am</em> prepared.</li>
</ul>
<h2 id="empathy"><a class="header" href="#empathy">Empathy</a></h2>
<ul>
<li>
<p>In any communication, I consider how it will be received, who is receiving it, and how they may feel about it.</p>
</li>
<li>
<p>The code I write considers the user and the next developer to read or modify it.
Have empathy on the developer that maintains your code, because it will most likely be you.</p>
</li>
<li>
<p>This includes writing tools for tasks that others also perform.
Share the value of your work with others and deduplicate where possible.
Make other’s lives better.</p>
</li>
</ul>
<h2 id=""><a class="header" href="#"></a></h2>
<hr />
<div class="footnote-definition" id="nvda_values"><sup class="footnote-definition-label">1</sup>
<p><a href="https://www.nvidia.com/en-in/about-nvidia/culture-at-nvidia/">Link to NVIDIA’s core values page</a></p>
</div>
<div class="footnote-definition" id="bcantrill_yt"><sup class="footnote-definition-label">2</sup>
<p><a href="https://www.youtube.com/watch?v=9QMGAtxUlAc">Principles of Technology Leadership | Bryan Cantrill | Monktoberfest 2017</a></p>
</div>
<div class="footnote-definition" id="bcantrill_changelog"><sup class="footnote-definition-label">3</sup>
<p><a href="https://changelog.com/podcast/592">changelog ep with Bryan Cantrill</a></p>
</div>
<div class="footnote-definition" id="oxide_principles"><sup class="footnote-definition-label">4</sup>
<p><a href="https://oxide.computer/principles">https://oxide.computer/principles</a></p>
</div>
<!--
layout: post
title: Shell and Scripting Notes
permalink: /shell
category: linux, shell, scripting
wip: false
cat: cs
-->
<p>What I need to remember when scripting</p>
<h2 id="shellscripting"><a class="header" href="#shellscripting">Shell/Scripting</a></h2>
<h3 id="gnu-parallel"><a class="header" href="#gnu-parallel">GNU Parallel</a></h3>
<pre><code>  seq 200 | parallel --bar ./t.sh {} '&gt;' ajm-{}.txt
</code></pre>
<p>Used with some test script like this to find corner cases:</p>
<pre><code>  $ cat t.sh
  init=$PWD
  cd $(mktemp -d)
  echo $1
  ./exe $1 &amp;&gt; stdout
  diff stdout $init/correct &amp;&amp; echo pass
</code></pre>
<p>This way I can run the executable N times and then recursively grep the output directory to find any cases that passed/failed.</p>
<h3 id="bash-regex"><a class="header" href="#bash-regex">Bash REGEX</a></h3>
<p>Use <code>$BASH_REMATCH</code> or <code>${BASH_REMATCH[@]}</code> with the regex match operator in a test expression:</p>
<pre><code>$ [[ " -c -fPIC /path/to/file.F90 " =~ [^\ ]+/file.F90 ]] &amp;&amp; echo ${BASH_REMATCH[@]}
/path/to/file.F90
</code></pre>
<p><a href="http://molk.ch/tips/gnu/bash/rematch.html">more bash regex stuff</a></p>
<h3 id="cli"><a class="header" href="#cli">CLI</a></h3>
<pre><code class="language-shell">exe=$0
usage() {
    cat &lt;&lt;EOD
EOD
    exit 1
}

port=6666
cmd=""
host=""
while [[ $# -ne 0 ]]; do
  case $1 in
    -p) port=$2; shift;;
    -h) host=$2; shift;;
    --) shift;cmd="$*";shift $#;;
    *) usage;;
  esac
  shift
done
</code></pre>
<h3 id="strings"><a class="header" href="#strings">Strings</a></h3>
<p>global sub</p>
<pre><code>  $ var="some text to work with, more text"
  $ echo ${var//text/content}
  some content to work with, more content
</code></pre>
<p>previous command with some replacement</p>
<pre><code>  $ echo one two three one
  one two three one
  $ !!:gs/one/five
  $ echo five two three five
  five two three five
</code></pre>
<p>local sub</p>
<pre><code>  $ echo ${var/text/content}
  some content to work with, more text
</code></pre>
<p>Deletes LONGEST match, be careful of ./file.txt as the ./ component will get the whole thing removed.</p>
<pre><code>  $ file=log.txt
  $ echo ${file%%.*}
  log

  $ file=./log.txt
  $ echo ${file%.*}
  ./log
</code></pre>
<p>string casing</p>
<pre><code>  x="HELLO"
  echo $x  # HELLO
</code></pre>
<p>tolower</p>
<pre><code>  y=${x,,}
  echo $y  # hello
</code></pre>
<p>toupper</p>
<pre><code>  z=${y^^}
  echo $z  # HELLO
</code></pre>
<p><a href="https://tldp.org/LDP/abs/html/string-manipulation.html">Linux documentation on string manipulation</a></p>
<h2 id="perl"><a class="header" href="#perl">Perl</a></h2>
<h3 id="one-liners"><a class="header" href="#one-liners">One-Liners</a></h3>
<p>(Other perl one-liners)[https://learnbyexample.github.io/learn_perl_oneliners/one-liner-introduction.html]</p>
<p>print line number and line of matching regex</p>
<pre><code>perl -ne 'printf "%8d %s", $., $_ if /pattern/'
</code></pre>
<p>in-place replacement of input</p>
<pre><code>perl -pi.bk -E 's/, !dbg !\d+//' good.llvm
      |      |                 | input file
      |      | regex/code/replacement
      | backup file extension to use
</code></pre>
<h3 id="getopt"><a class="header" href="#getopt">Getopt</a></h3>
<pre><code class="language-perl">
</code></pre>
<h3 id="misc"><a class="header" href="#misc">Misc</a></h3>
<p>Print the command to be run, run the arguments in a shell, and print all the lines of stderr/stdout:</p>
<pre><code class="language-perl">sub sh {
    my $cmd="@_";
    chomp($cmd);
    say $cmd;
    open my $fh, "$cmd|";
    print while (&lt;$fh&gt;);
    close $fh;
    say "ec=$?";
}
</code></pre>
<h1 id="editors-and-tools"><a class="header" href="#editors-and-tools">Editors and Tools</a></h1>
<h2 id="editor"><a class="header" href="#editor">Editor</a></h2>
<p>My editing workflow starts with <code>ssh</code>ing into a box and reattaching to a <code>tmux</code> session:</p>
<pre><code class="language-bash">ssh myhost
tmux attach
</code></pre>
<p>I have an ssh alias set up to forward certain ports as well:</p>
<pre><code class="language-bash">$ cat ~/.ssh/config
Host myhost
  User ashermancinelli
  HostName myhost.domain.com
  LocalForward 6666 localhost:6666
  LocalForward 6667 localhost:6667
  LocalForward 6668 localhost:6668
  LocalForward 6669 localhost:6669
</code></pre>
<p>From inside the tmux session, I usually have several sessions each with a few shell windows and a <code>neovim</code> server running:</p>
<pre><code class="language-bash">nvim --headless --listen localhost:6666
</code></pre>
<p>Then I can attach my editor<sup class="footnote-reference"><a href="#neovide">1</a></sup> to the remote editing session from my local terminal:</p>
<pre><code class="language-bash">neovide --no-multigrid --fork --server localhost:6666
</code></pre>
<h2 id="tools"><a class="header" href="#tools">Tools</a></h2>
<p>I use Spack<sup class="footnote-reference"><a href="#spack">2</a></sup> for nearly everything.</p>
<p>I have a set of tools I need on every machine, which I install with Spack.
At the time of writing, this is the list:</p>
<ul>
<li><code>ripgrep</code></li>
<li><code>the-silver-searcher</code></li>
<li><code>bat</code></li>
<li><code>fzf</code></li>
<li><code>fd</code></li>
<li><code>rlwrap</code></li>
<li><code>neovim@master</code></li>
<li><code>lua</code></li>
<li><code>vim</code></li>
<li><code>perl</code></li>
<li><code>python</code></li>
<li><code>py-pip</code></li>
</ul>
<p>I install all of these like so:</p>
<pre><code class="language-bash">spack \
  --config concretizer:targets:granularity:generic \
  install -j `nproc` \
  ripgrep the-silver-searcher bat fzf fd rlwrap neovim@master lua vim perl python py-pip \
  --fresh
</code></pre>
<p>Setting the target granularity to <code>generic</code> means I can <em>usually</em> install once for each architecture in the cluster I’m working on, however sometimes OS incompatibilities mean I need to reinstall a few times.</p>
<p>From a script, I load all the default packages I need by just choosing the first one that matches my current generic architecture.</p>
<pre><code class="language-bash">garch=`spack arch --platform`-`spack arch --operating-system`-`spack arch --generic-target`
for pkg in ${packages[@]}; do eval `spack load --sh --first $pkg arch=$garch`; done
</code></pre>
<p>The better way to do this would be to use environments<sup class="footnote-reference"><a href="#spack_env">3</a></sup>, but installing them as individual packages makes it easier to reuse all my scripts for different architectures and operating systems rather than creating environments for each.
I can just update my list of default packages and reinstall as-needed without activating an environment, reconcretizing and reinstalling every time.</p>
<div class="footnote-definition" id="neovide"><sup class="footnote-definition-label">1</sup>
<p><a href="https://github.com/neovide/neovide">NeoVide, neovim gui client</a></p>
</div>
<div class="footnote-definition" id="spack"><sup class="footnote-definition-label">2</sup>
<p><a href="https://spack.io/">Spack | A flexible package manager supporting multiple versions, configurations, platforms, and compilers.</a></p>
</div>
<div class="footnote-definition" id="spack_env"><sup class="footnote-definition-label">3</sup>
<p><a href="https://spack.readthedocs.io/en/latest/environments.html">https://spack.readthedocs.io/en/latest/environments.html</a></p>
</div>
<!--
layout: post
title: Understanding VLA
permalink: /vla-c
cat: cs
tags: c++
-->
<p>Scattered notes from learning about the implementation of VLA.</p>
<h2 id="what-is-vla"><a class="header" href="#what-is-vla">What is VLA?</a></h2>
<p>Variable-length arrays are dynamic, stack-allocated arrays.
The compiler needs to increase the stack size in the current stack frame to allocate enough space for the array.
Assuming negative stack-growth like on x86, the compiler will decrease the stack pointer sufficiently to store the array.</p>
<p>This is almost identical to <code>alloca</code>.
Both <code>alloca</code> and VLAs are essentially primitives to modify the stack pointer.</p>
<p>Eg:</p>
<pre><code class="language-c">  // Subtracts N from current stack pointer returns sp 
  int *curr_sp = alloca(N * sizeof(int));

  // equivilant to
  int curr_sp[N];
</code></pre>
<p><a href="https://stackoverflow.com/questions/3488821/is-alloca-completely-replaceable">One key difference between the two:</a></p>
<div id="admonition-quote" class="admonition admonish-quote" role="note" aria-labelledby="admonition-quote-title">
<div class="admonition-title">
<div id="admonition-quote-title">
<p>Quote</p>
</div>
<a class="admonition-anchor-link" href="csblog/2023-6-1-C-VLA-Implementation.html#admonition-quote"></a>
</div>
<div>
<p>The memory alloca() returns is valid as long as the current function persists. The lifetime of the memory occupied by a VLA is valid as long as the VLA’s identifier remains in scope. You can <code>alloca</code> memory in a loop for example and use the memory outside the loop, a VLA would be gone because the identifier goes out of scope when the loop terminates.</p>
</div>
</div>
<h2 id="memory-layout"><a class="header" href="#memory-layout">Memory Layout</a></h2>
<p>Because the stack grows down on most platforms, the stack pointer after an <code>alloca</code> or VLA allocation but arrays are addressed sequentially upwards, the address of the first element of a VLA array (or the pointer returned by <code>alloca</code>) will be the value of the stack pointer <em>after</em> it’s modified.</p>
<center>
  <img
    style="background-color:#240057;"
    src="/images/vla/vla-stack-pointer-viz.drawio.png"
    />
</center>
<p>Element 0 of the array or <code>alloca</code>-allocated memory is therefore immediately above the stack pointer after allocation, and is addressed by increasing sequentially until the end of the array.
Accessing past the array will then run into previously declared stack variables.</p>
<p>When the function returns, the stack space will be available for subsequent function calls to use automatically, so there is no need to explicitly free memory allocated by VLA/<code>alloca</code>.</p>
<h2 id="examples"><a class="header" href="#examples">Examples</a></h2>
<div id="admonition-gcc-documentation" class="admonition admonish-quote" role="note" aria-labelledby="admonition-gcc-documentation-title">
<div class="admonition-title">
<div id="admonition-gcc-documentation-title">
<p>GCC Documentation</p>
</div>
<a class="admonition-anchor-link" href="csblog/2023-6-1-C-VLA-Implementation.html#admonition-gcc-documentation"></a>
</div>
<div>
<p>These arrays are declared like any other automatic arrays, but with a length that is not a constant expression. The storage is allocated at the point of declaration and deallocated when the block scope containing the declaration exits.</p>
</div>
</div>
<pre><code class="language-c">// ./vla &lt;size&gt;
int main(int argc, char** argv) {
  int len = atoi(argv[1]);
  int array[len];
  for (int i=0; i&lt;len; i++)
    array[i] = i;
}
</code></pre>
<p>Declaring the array decrements the stack pointer enough to provide memory for the array:</p>
<!--
gcc _includes/vla/inspect-stack.c && LEN=10 IDX=4 ./a.out
-->
<pre><code class="language-c">{% include vla/inspect-stack.c %}
</code></pre>
<pre><code class="language-shell">$ uname -a
Linux carbon 5.15.0-71-generic #78-Ubuntu SMP Tue Apr 18 09:00:29 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
$ gcc inspect-stack-vla.c &amp;&amp; LEN=10 IDX=4 ./a.out
&amp;vla[0]: 140737151458112
before: 140737151458160
after: 140737151458112
diff: 48
</code></pre>
<p>Notice that the address stored in the stack pointer after declaring the VLA array is the same as the address of the first element of the VLA array as depicted in the diagram above.</p>
<h2 id="alloca"><a class="header" href="#alloca"><code>alloca</code></a></h2>
<p>Instead of declaring a VLA array, we can create a pointer to memory allocated by <code>alloca</code> to produce the same effect:</p>
<!--
gcc _includes/vla/inspect-stack-alloca.c && LEN=10 IDX=4 ./a.out
-->
<pre><code class="language-c">{% include vla/inspect-stack-alloca.c %}
</code></pre>
<pre><code class="language-shell">$ gcc inspect-stack-alloca.c &amp;&amp; LEN=10 IDX=4 ./a.out
&amp;vla[0]: 140728646054592
before: 140728646054640
after: 140728646054592
diff: 48
</code></pre>
<p>Compare the GCC docs for <code>alloca</code> with that of variable length arrays and notice the similarities:</p>
<div id="admonition-gcc-documentation-1" class="admonition admonish-quote" role="note" aria-labelledby="admonition-gcc-documentation-1-title">
<div class="admonition-title">
<div id="admonition-gcc-documentation-1-title">
<p>GCC Documentation</p>
</div>
<a class="admonition-anchor-link" href="csblog/2023-6-1-C-VLA-Implementation.html#admonition-gcc-documentation-1"></a>
</div>
<div>
<p>The function alloca supports a kind of half-dynamic allocation in which blocks are allocated dynamically but freed automatically.</p>
<p>Allocating a block with alloca is an explicit action; you can allocate as many blocks as you wish, and compute the size at run time. But all the blocks are freed when you exit the function that alloca was called from, just as if they were automatic variables declared in that function. There is no way to free the space explicitly.</p>
</div>
</div>
<h2 id="why-might-this-be-a-bad-idea"><a class="header" href="#why-might-this-be-a-bad-idea">Why Might This Be a Bad Idea?</a></h2>
<p>The dynamic nature of VLAs means the offset of stack variables declared after the VLA into the stack frame of the function is <strong>also dynamic</strong> - which means the function will need extra instructions to calculate the address of these variables whenever they are referenced in the body of the function.</p>
<p>This <em>may</em> be a worthwhile tradeoff, but know that use of VLAs means your code may need a few extra instructions every time you use stack variables.</p>
<!--
## LLVM IR

Docs explanation of alloca:

> The ‘alloca’ instruction allocates memory on the stack frame of the currently executing function, to be automatically released when this function returns to its caller

< !--
clang -S -emit-llvm -o - _includes/vla/simple.c
-- >
```c
{% include vla/simple.c %}
```
```llvm
@.str = private unnamed_addr constant [4 x i8] c"LEN\00", align 1
@.str.1 = private unnamed_addr constant [4 x i8] c"IDX\00", align 1

define dso_local i32 @main(i32 noundef %0, i8** noundef %1) #0 {
  %3 = alloca i32, align 4
  %4 = alloca i32, align 4
  %5 = alloca i8**, align 8
  %6 = alloca i32, align 4
  %7 = alloca i32, align 4
  %8 = alloca i8*, align 8
  %9 = alloca i64, align 8
  store i32 0, i32* %3, align 4
  store i32 %0, i32* %4, align 4
  store i8** %1, i8*** %5, align 8
  %10 = call i8* @getenv(i8* noundef getelementptr inbounds ([4 x i8], [4 x i8]* @.str, i64 0, i64 0)) #4
  %11 = call i32 @atoi(i8* noundef %10) #5
  store i32 %11, i32* %6, align 4
  %12 = call i8* @getenv(i8* noundef getelementptr inbounds ([4 x i8], [4 x i8]* @.str.1, i64 0, i64 0)) #4
  %13 = call i32 @atoi(i8* noundef %12) #5
  store i32 %13, i32* %7, align 4
  %14 = load i32, i32* %6, align 4
  %15 = zext i32 %14 to i64

  %16 = call i8* @llvm.stacksave()

  store i8* %16, i8** %8, align 8
  %17 = alloca i32, i64 %15, align 16
        ^^^^^^^^^^ Dynamically allocate more memory on the stack by decrementing
                   the stack pointer, giving sufficient space for the array

  store i64 %15, i64* %9, align 8
  %18 = load i32, i32* %7, align 4
  %19 = sext i32 %18 to i64
  %20 = getelementptr inbounds i32, i32* %17, i64 %19
  %21 = load i32, i32* %20, align 4
  store i32 %21, i32* %3, align 4
  %22 = load i8*, i8** %8, align 8
  call void @llvm.stackrestore(i8* %22)
  %23 = load i32, i32* %3, align 4
  ret i32 %23
}
```
-->
<h2 id="conclusion--links"><a class="header" href="#conclusion--links">Conclusion &amp; Links</a></h2>
<ol>
<li><a href="https://gcc.gnu.org/onlinedocs/gcc/Variable-Length.html">GCC VLA docs</a></li>
<li><a href="https://www.gnu.org/software/libc/manual/html_node/Alloca-Example.html">GCC <code>alloca</code> docs</a></li>
<li><a href="https://llvm.org/docs/LangRef.html#alloca-instruction">LLVM IR docs for <code>alloca</code> instruction</a></li>
<li><a href="https://llvm.org/doxygen/Instructions_8cpp_source.html">LLVM source for <code>alloca</code> instruction</a></li>
<li><a href="https://en.cppreference.com/w/c/language/array">cppreference docs on VLA</a></li>
<li><a href="https://www.tenouk.com/Bufferoverflowc/Bufferoverflow2a.html">Buffer overflow and stack frame visualization</a></li>
</ol>
<!--
layout: post
title: BQN and Reflections on the Joy of Programming
permalink: /bqn-reflections
category: BQN, c++
wip: false
cat: cs
-->
<p>Solve a leetcode problem in BQN and I rant about the joy of programming.</p>
<h2 id="leetcode"><a class="header" href="#leetcode">Leetcode</a></h2>
<a href="https://leetcode.com/problems/set-matrix-zeroes" target="blank">
  The Leetcode problem is "Set Matrix Zeroes"
</a>
where we're tasked with setting rows and columns of a matrix that contain zero to be all zeroes.
<h2 id="bqn-solution"><a class="header" href="#bqn-solution">BQN Solution</a></h2>
<pre><code>   i←⟨
     3‿4⥊⟨0,1,3,0,3,4,5,2,1,3,1,5⟩
     3‿3⥊⟨1,1,1,1,0,1,1,1,1⟩
   ⟩

   Z ← {
     bm←0=𝕩
     a←∨` ∨`⌾⌽ bm
     b←(∨`˘) ((∨`˘)⌾(⌽˘)) bm
     𝕩×a¬∘∨b
   }
   
   ⟨"#1","#2"⟩∾i≍Z¨i
┌─                       
╵ "#1"        "#2"       
  ┌─          ┌─         
  ╵ 0 1 3 0   ╵ 1 1 1    
    3 4 5 2     1 0 1    
    1 3 1 5     1 1 1    
            ┘         ┘  
  ┌─          ┌─         
  ╵ 0 0 0 0   ╵ 1 0 1    
    0 4 5 0     0 0 0    
    0 3 1 0     1 0 1    
            ┘         ┘  
                        ┘
</code></pre>
<p>Some other solutions from the BQN Matrix chat room:</p>
<pre><code>   ⊢×0≠∧˝˘∧⌜∧˝           # Marshall &amp; Dzaima (tacit!)
   (≠⥊∧´)˘{𝕩×(𝔽⌾⍉∧𝔽)0≠𝕩} # Dzaima &amp; Rampoina
   {𝕩×(∧˝˘∧≢⥊∧˝)0≠𝕩}     # Dzaima
</code></pre>
<h2 id="on-the-joy-of-programming"><a class="header" href="#on-the-joy-of-programming">On the Joy of Programming</a></h2>
<p>It’s been a few months since I’ve written BQN or APL, so I feel like I’m looking at the language family with fresh eyes.</p>
<p>I was struck by the resemblance between solving this leetcode problem and creating art:</p>
<ol>
<li>I know I’m not the best at either, and many, <em>many</em> people can write more elegant BQN and more elegant poetry than I can (for example)</li>
<li>I can thoroughly enjoy both when detached from any performance metrics - the process is far more valuable to me than the end-product</li>
<li>both can be deeply social actions - sharing your painting with someone and discussing my solution with the BQN chat room are both social and exciting. Even if someone comes back with a more wonderful painting or more terse solution, I enjoy the social interaction just as much.</li>
</ol>
<p>I stumbled upon this thread on twitter describing how Kurt Vonnegut responded to a letter from a high school English student asking for life advice.
In short, his response was to do art and enjoy the process of becoming who you are.</p>
<center>
  <blockquote class="twitter-tweet"><p lang="en" dir="ltr">Tear it up into teeny-weeny pieces, and discard them into widely separated trash receptacles. You will find that you have already been gloriously rewarded for your poem. You have experienced becoming, learned a lot more about what’s inside you, and you have made your soul grow.</p>&mdash; Gabe Hudson (@gabehudson) <a href="https://twitter.com/gabehudson/status/1521139749322477569?ref_src=twsrc%5Etfw">May 2, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>
<p>Creating art seems to be central to the importance of life as far as I can tell.</p>
<a href="https://www.arraycast.com/episodes/episode26-stevan-apter" target="blank">
  The most recent episode of ArrayCast with Stevan Apter dipped into this as well when the panelists discussed the aesthetic of writing APL.
</a>
In some ways they were a little reserved about saying they enjoy APL at least in part due to the aesthetic of the language.
I don't think this is something to shy away from - if we can't appreciate the beauty of what we do, why are we doing it at all?
<p>I loved working through this rather simple problem.</p>
<p>I loved the process of visualizing the inputs, of thinking through possible solutions while going about my day.</p>
<p>I loved taking my solution to the BQN forum for more gifted and practiced BQN-ers to find far simpler and more elegant solutions than mine.</p>
<p>The whole process felt like writing a poem, and at the end I’m rewarded by sharing this poem with others, seeing what they come up with, and comparing their thoughts with mine.</p>
<p>There is a unique joy and beauty I find in BQN (and APL more broadly), and that’s what keeps me coming back.</p>
<p>As Kurt Vonnegut pointed out, what else could be a more worthwhile way to spend my time?</p>
<p>Please, give it a try, and fall in love with the community while you’re at it.</p>
<!---
## C++ Solution

I'll also include my C++ solution for kicks and giggles:

```cpp
void setZeroes(vector<vector<int>>& m) {
  const auto rs = m.size(), cs = m[0].size();
  vector<int> rows, cols;
  for (int i=0; i<rs; i++)
    for (int j=0; j<cs; j++)
      if (0 == m[i][j]) {
        rows.push_back(i);
        cols.push_back(j);
      }
  for (const auto r : rows)
    std::fill(m[r].begin(), m[r].end(), 0);
  for (const auto c : cols)
    for (auto& r : m)
      r[c] = 0;
}
```
-->
<!--
layout: post
title: LLVM Development on NixOS
permalink: /llvm-nixos
category: c++, nixos, llvm
cat: cs
-->
<p>I’ve found NixOS to provide a wonderful development environment after learning a bit of the Nix language - but building and hacking on LLVM on NixOS gave me some trouble.
Hopefully reading this post will save you the trouble!</p>
<h1 id="build-environment"><a class="header" href="#build-environment">Build Environment</a></h1>
<h3 id="if-youd-just-like-to-see-my-config-go-to-the-end-of-the-post-where-ive-pasted-the-whole-thing"><a class="header" href="#if-youd-just-like-to-see-my-config-go-to-the-end-of-the-post-where-ive-pasted-the-whole-thing"><em><strong>If you’d just like to see my config, go to the end of the post where I’ve pasted the whole thing</strong></em></a></h3>
<p>I’m no Nix language expert by any stretch of the imagination, nor am I an expert in dynamic linking or managing an operating system.</p>
<p>I started with the <code>nix-shell</code> example provided in <a href="https://nixos.wiki/wiki/LLVM">the nix documentation here</a> and made additions as I found the need.</p>
<p>I had to pass the library directories of both GCC and GLIBC using both the <code>-B</code> and <code>-L</code> flags, because some required object files (like <code>crt1.o</code>) must be found at link time, and clang/gcc don’t search <code>LD_LIBRARY_PATH</code> for these files.
<code>-B</code> will tell the compilers to look in the provided paths for these files.</p>
<pre><code class="language-nix">  libcFlags = [
    "-L ${stdenv.cc.libc}/lib"
    "-B ${stdenv.cc.libc}/lib"
  ];

  # The string version of just the gcc flags for NIX_LDFLAGS
  nixLd = lib.concatStringsSep " " [
    "-L ${gccForLibs}/lib"
    "-L ${gccForLibs}/lib/gcc/${targetPlatform.config}/${gccForLibs.version}"
  ];

  gccFlags = [
    "-B ${gccForLibs}/lib/gcc/${targetPlatform.config}/${gccForLibs.version}"
    "${nixLd}"
  ];
</code></pre>
<p>The official documentation uses <code>LLVM_ENABLE_PROJECTS</code> to enable runtimes, which is deprecated, so I first removed that in favor of a manual two-stage build for libc++ and libc++abi.</p>
<pre><code class="language-nix">  # For building clang itself, we're just using the compiler wrapper and we
  # don't need to inject any flags of our own.
  cmakeFlags = lib.concatStringsSep " " [
    "-DGCC_INSTALL_PREFIX=${gcc}"
    "-DC_INCLUDE_DIRS=${stdenv.cc.libc.dev}/include"
    "-DCMAKE_BUILD_TYPE=Release"
    "-DCMAKE_INSTALL_PREFIX=${installdir}"
    "-DLLVM_INSTALL_TOOLCHAIN_ONLY=ON"
    "-DLLVM_ENABLE_PROJECTS=clang"
    "-DLLVM_TARGETS_TO_BUILD=X86"
  ];
  cmakeCmd = lib.concatStringsSep " " [
    "export CC=${stdenv.cc}/bin/gcc; export CXX=${stdenv.cc}/bin/g++;"
    "${cmakeCurses}/bin/cmake -B ${builddir} -S llvm"
    "${cmakeFlags}"
  ];
</code></pre>
<p>To build clang itself, I activate the nix shell and build only clang:</p>
<pre><code class="language-console">$ cd llvm-project
$ nix-shell
$ eval "$cmakeCmd"
$ make -C build -j `nproc`
</code></pre>
<p>I didn’t use <code>LLVM_ENABLE_RUNTIMES</code> since I had trouble passing the CMake arguments to the runtime builds through the top-level build.
The purpose of <code>LLVM_ENABLE_RUNTIMES</code> is to build an LLVM project using the just-built clang/LLVM, however compile and link arguments are not passed to the runtime builds using the default <code>CMAKE_CXX_FLAGS</code> as I expected (or at least I was unable to get this to work).</p>
<p>Instead, I configured a seperate set of cmake arguments for the runtimes, and manually passed the just-built clang compiler as <code>CXX</code> to the runtime builds like so:</p>
<pre><code class="language-nix">  cmakeRuntimeFlags = lib.concatStringsSep " " [
    "-DCMAKE_CXX_FLAGS=\"${flags}\""
    "-DLIBCXX_TEST_COMPILER_FLAGS=\"${flags}\""
    "-DLIBCXX_TEST_LINKER_FLAGS=\"${flags}\""
    "-DLLVM_ENABLE_RUNTIMES='libcxx;libcxxabi'"
  ];
  cmakeRtCmd = lib.concatStringsSep " " [
    "export CC=${builddir}/bin/clang; export CXX=${builddir}/bin/clang++;"
    "${cmakeCurses}/bin/cmake -B ${builddir}-rt -S runtimes"
    "${cmakeRuntimeFlags}"
  ];
</code></pre>
<pre><code class="language-console">$ cd llvm-project
$ eval "$cmakeRtCmd"
$ make -C build-rt -j `nproc`
</code></pre>
<h1 id="testing-linking-running"><a class="header" href="#testing-linking-running">Testing, Linking, Running</a></h1>
<p>A huge issue with testing arose due to the way NixOS handles it’s dynamic loader.</p>
<p>When you use software in NixOS, it’s usually found somewhere in the <code>/run/current-system/sw/</code> prefix, while most software expects to be run from <code>/usr</code> or <code>/usr/local</code>, or it expects to be able to find key libraries under those prefixes (eg <code>libc.so</code>).</p>
<p>Instead, each software component has it’s own prefix under <code>/nix/store</code>, for example:</p>
<pre><code class="language-console">$ which perl
/run/current-system/sw/bin/perl
$ file $(which perl)
/run/current-system/sw/bin/perl: symbolic link to 
    /nix/store/kpzx6f97583zbjyyd7b17rbv057l4vn2-perl-5.34.0/bin/perl
</code></pre>
<p>Each compiler must then know the correct locations of the standard libraries and software components, such as the dynamic loader, the standard C library, etc.
To acomplish this, Nix-provided compilers ship with <em>wrappers</em> that inject the required flags.</p>
<p>If I inspect my GNU compilers, we see that I’m not using <code>g++</code> directly:</p>
<pre><code class="language-console">$ which g++
/nix/store/gkzmfpb04ddb7phzj8g9sl6saxzprssg-gcc-wrapper-10.3.0/bin/g++
$ file $(which g++)
/nix/store/gkzmfpb04ddb7phzj8g9sl6saxzprssg-gcc-wrapper-10.3.0/bin/g++:
  a /nix/store/v1d8l3zqnia3hccqd0701szhlx22g54z-bash-5.1-p8/bin/bash
  script, ASCII text executable
</code></pre>
<p>The actual compiler is found in a seperate prefix:</p>
<pre><code class="language-console">$ grep 'g++' $(which g++) | head -n1
[[ "/nix/store/mrqrvina0lfgrvdzfyri7sw9vxy6pyms-gcc-10.3.0/bin/g++" = *++ ]] &amp;&amp; isCxx=1 || isCxx=0
</code></pre>
<p>On my current system, the true compiler is found under <code>/nix/store/mrqrvina0lfgrvdzfyri7sw9vxy6pyms-gcc-10.3.0</code>.</p>
<p>This becomes an issue with testing LLVM because the tests run executables built with the just-built <code>clang++</code>, not with the wrapped compiler!
That’s why we have to inject so many flags in our <code>shell.nix</code>.</p>
<p>When I initially ran <code>make check-cxx</code> to run the tests for libc++, I found errors like this:</p>
<pre><code class="language-console">FileNotFoundError: [Errno 2]
    No such file or directory:
    '/home/asher/workspace/llvm-project/brt/libcxx/test/std/containers/sequences/deque/deque.modifiers/Output/erase_iter_iter.pass.cpp.dir/t.tmp.exe'
</code></pre>
<p>It <em>looks</em> like the tests rely on an executable that’s not there.
However, if I check that location:</p>
<pre><code class="language-console">$ file /home/asher/workspace/llvm-project/brt/libcxx/test/std/containers/sequences/deque/deque.modifiers/Output/erase_iter_iter.pass.cpp.dir/t.tmp.exe
/home/asher/workspace/llvm-project/brt/libcxx/test/std/containers/sequences/deque/deque.modifiers/Output/erase_iter_iter.pass.cpp.dir/t.tmp.exe:
ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked,
interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 2.6.32, with debug_info, not stripped
</code></pre>
<p>the executable <em>was</em> built.</p>
<p>If I try to run it directly, it still appears to not exist:</p>
<pre><code class="language-console">$ /home/asher/workspace/llvm-project/brt/libcxx/test/std/containers/sequences/deque/deque.modifiers/Output/erase_iter_iter.pass.cpp.dir/t.tmp.exe
bash: /home/asher/workspace/llvm-project/brt/libcxx/test/std/containers/sequences/deque/deque.modifiers/Output/erase_iter_iter.pass.cpp.dir/t.tmp.exe:
  No such file or directory
</code></pre>
<p><em><strong>What’s going on here?</strong></em></p>
<h2 id="dynamic-linker"><a class="header" href="#dynamic-linker">Dynamic Linker</a></h2>
<p>If we return to the output of running <code>file</code> on our mysterious executable, we see it thinks its dynamic linker is <code>/lib64/ld-linux-x86-64.so.2</code>:</p>
<pre><code>ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked,
interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 2.6.32, with debug_info, not stripped
</code></pre>
<p>However, if we look for that file, it doesn’t exist:</p>
<pre><code class="language-console">$ file /lib64/ld-linux-x86-64.so.2
/lib64/ld-linux-x86-64.so.2:
  cannot open `/lib64/ld-linux-x86-64.so.2'
  (No such file or directory)
</code></pre>
<p>The <code>patchelf</code> utility from the NixOS developers will tell us where the dynamic linker is for a given executable:</p>
<pre><code class="language-console">$ patchelf --print-interpreter /path/to/llvm-test.exe
/lib64/ld-linux-x86-64.so.2
</code></pre>
<p>Again, our executable wasn’t built by a Nix compiler wrapper, it was built by the clang we just compiled ourselves.
What dynamic linker do other programs on this system use?</p>
<pre><code class="language-console">$ patchelf --print-interpreter $(which bash)
/nix/store/vjq3q7dq8vmc13c3py97v27qwizvq7fd-glibc-2.33-59/lib/ld-linux-x86-64.so.2
</code></pre>
<p>That’s right, everything on NixOS is built in its own prefix.
If we had used a compiler wrapper, the flags to tell the executable which linker to use would have been injected.</p>
<p>I found that the linker flag <code>--dynamic-linker</code> will set the dynamic linker path for a given executable, and it’s used here in GCC’s compiler wrapper:</p>
<pre><code class="language-console">$ grep 'dynamic-link' $(which g++)
        extraBefore+=("-Wl,-dynamic-linker=$NIX_DYNAMIC_LINKER_x86_64_unknown_linux_gnu")
</code></pre>
<p>I can’t quite figure out how <code>NIX_DYNAMIC_LINKER_x86_64_unknown_linux_gnu</code> is set other than that it’s set by the script in <code>$gcc_wrapper_prefix/nix-support/add-flags.sh</code>, but I did find the file <code>dynamic-linker</code> under that same prefix:</p>
<pre><code class="language-console">$ file $(which g++)
/run/current-system/sw/bin/g++: symbolic link to
  /nix/store/gkzmfpb04ddb7phzj8g9sl6saxzprssg-gcc-wrapper-10.3.0/bin/g++
$ ls /nix/store/gkzmfpb04ddb7phzj8g9sl6saxzprssg-gcc-wrapper-10.3.0
bin  nix-support
$ ls /nix/store/gkzmfpb04ddb7phzj8g9sl6saxzprssg-gcc-wrapper-10.3.0/nix-support
add-flags.sh      cc-ldflags      libc-crt1-cflags  libcxx-ldflags  orig-libc-dev            utils.bash
add-hardening.sh  dynamic-linker  libc-ldflags      orig-cc         propagated-build-inputs
cc-cflags         libc-cflags     libcxx-cxxflags   orig-libc       setup-hook
</code></pre>
<p>So the file <code>$gcc_wrapper_prefix/nix-support/dynamic-linker</code> contains the path to the dynamic linker the compiler is using:</p>
<pre><code class="language-console">$ cat /nix/store/gkzmfpb04ddb7phzj8g9sl6saxzprssg-gcc-wrapper-10.3.0/nix-support/dynamic-linker
/nix/store/vjq3q7dq8vmc13c3py97v27qwizvq7fd-glibc-2.33-59/lib/ld-linux-x86-64.so.2
</code></pre>
<p>I’ll then use this in my <code>shell.nix</code> to get the path to the dynamic linker, and then pass that to clang for building the LLVM runtimes so the correct dynamic linker is used for executables built by clang:</p>
<pre><code class="language-nix">  dynLinker = lib.fileContents "${stdenv.cc}/nix-support/dynamic-linker";
  flags = lib.concatStringsSep " " ([
    "-Wno-unused-command-line-argument"
    "-Wl,--dynamic-linker=${dynLinker}"
                          ↑↑↑↑↑↑↑↑↑↑↑↑↑
  ] ++ gccFlags ++ libcFlags);
</code></pre>
<p>I’ve also added <code>-Wno-unused-command-line-argument</code> to the compile flags so we’re not spammed with warnings every time a link directory or file directory I pass to the compiler invokation is ignored.</p>
<p>We can now finally run our tests.
Sometimes required binaries are still not found by lit, so I use the <code>cxx-test-depends</code> target to build test dependencies and then I run lit manually:</p>
<pre><code class="language-console">$ make cxx-test-depends -C build-rt
$ ./build/bin/llvm-lit ./libcxx
</code></pre>
<h1 id="full-config"><a class="header" href="#full-config">Full Config</a></h1>
<p>I was able to find lots of information about nix from playing around in the nix repl and using tab completion, like this:</p>
<pre><code class="language-console">$ nix repl
Welcome to Nix 2.4. Type :? for help.

nix-repl&gt; pkgs = import &lt;nixpkgs&gt; {}

nix-repl&gt; pkgs.lib.concatStringsSep " " ["one" "two" "three"]
"one two three"

nix-repl&gt; pkgs.stdenv.cc.&lt;TAB&gt;&lt;TAB&gt;
pkgs.stdenv.cc.__ignoreNulls                pkgs.stdenv.cc.libc_dev
pkgs.stdenv.cc.all                          pkgs.stdenv.cc.libc_lib
pkgs.stdenv.cc.args                         pkgs.stdenv.cc.man
pkgs.stdenv.cc.bintools                     pkgs.stdenv.cc.meta
...

nix-repl&gt; "${pkgs.stdenv.cc.cc}"
"/nix/store/mrqrvina0lfgrvdzfyri7sw9vxy6pyms-gcc-10.3.0"

nix-repl&gt; pkgs.lib.fileContents "${pkgs.stdenv.cc}/nix-support/dynamic-linker"
"/nix/store/vjq3q7dq8vmc13c3py97v27qwizvq7fd-glibc-2.33-59/lib/ld-linux-x86-64.so.2"
</code></pre>
<p>Here’s the full config:</p>
<pre><code class="language-nix">with import &lt;nixpkgs&gt; {};

let
  builddir = "build";
  installdir = "install";
  gccForLibs = stdenv.cc.cc;
  dynLinker = lib.fileContents "${stdenv.cc}/nix-support/dynamic-linker";
  libcFlags = [
    "-L ${stdenv.cc.libc}/lib"
    "-B ${stdenv.cc.libc}/lib"
    ];

  # The string version of just the gcc flags for NIX_LDFLAGS
  nixLd = lib.concatStringsSep " " [
    "-L ${gccForLibs}/lib"
    "-L ${gccForLibs}/lib/gcc/${targetPlatform.config}/${gccForLibs.version}"
  ];

  gccFlags = [
    "-B ${gccForLibs}/lib/gcc/${targetPlatform.config}/${gccForLibs.version}"
    "${nixLd}"
    ];

  flags = lib.concatStringsSep " " ([
      "-Wno-unused-command-line-argument"
      "-Wl,--dynamic-linker=${dynLinker}"
    ] ++ gccFlags ++ libcFlags);

  # For building clang itself, we're just using the compiler wrapper and we
  # don't need to inject any flags of our own.
  cmakeFlags = lib.concatStringsSep " " [
    "-DGCC_INSTALL_PREFIX=${gcc}"
    "-DC_INCLUDE_DIRS=${stdenv.cc.libc.dev}/include"
    "-DCMAKE_BUILD_TYPE=Release"
    "-DCMAKE_INSTALL_PREFIX=${installdir}"
    "-DLLVM_INSTALL_TOOLCHAIN_ONLY=ON"
    "-DLLVM_ENABLE_PROJECTS=clang"
    "-DLLVM_TARGETS_TO_BUILD=X86"
  ];

  # For configuring a build of LLVM runtimes however, we do need to inject the
  # extra flags.
  cmakeRuntimeFlags = lib.concatStringsSep " " [
    "-DCMAKE_CXX_FLAGS=\"${flags}\""
    "-DLIBCXX_TEST_COMPILER_FLAGS=\"${flags}\""
    "-DLIBCXX_TEST_LINKER_FLAGS=\"${flags}\""
    "-DLLVM_ENABLE_RUNTIMES='libcxx;libcxxabi'"
  ];

  cmakeCmd = lib.concatStringsSep " " [
    "export CC=${stdenv.cc}/bin/gcc; export CXX=${stdenv.cc}/bin/g++;"
    "${cmakeCurses}/bin/cmake -B ${builddir} -S llvm"
    "${cmakeFlags}"
  ];

  cmakeRtCmd = lib.concatStringsSep " " [
    "export CC=${builddir}/bin/clang; export CXX=${builddir}/bin/clang++;"
    "${cmakeCurses}/bin/cmake -B ${builddir}-rt -S runtimes"
    "${cmakeRuntimeFlags}"
  ];

in stdenv.mkDerivation {

  name = "llvm-dev-env";

  buildInputs = [
    bashInteractive
    cmakeCurses
    llvmPackages_latest.llvm
  ];

  # where to find libgcc
  NIX_LDFLAGS = "${nixLd}";

  # teach clang about C startup file locations
  CFLAGS = "${flags}";
  CXXFLAGS = "${flags}";

  cmakeRuntimeFlags="${cmakeRuntimeFlags}";
  cmakeFlags="${cmakeFlags}";

  cmake="${cmakeCmd}";
  cmakeRt="${cmakeRtCmd}";
}
</code></pre>
<!--
layout: post
title: "CUDA 101: Matrix-Vector Product"
permalink: /cuda-matvec
category: c, c++, cuda, GPU, HPC
cat: cs
-->
<p>{% include latex.html %}
{% include mermaid.html %}</p>
<p>The foundation of GPU programming is linear algebra.
This post takes you from a basic linear algebra example problem to a GPU-accelerated example that calculates a matrix-vector product.</p>
<h1 id="key-takeaways"><a class="header" href="#key-takeaways">Key Takeaways</a></h1>
<ol>
<li>Correctness precedes parallelism and performance</li>
<li>Identify and understand the underlying algorithms at play</li>
<li>Speed is not the same as efficiency</li>
<li>Use sane defaults, only optimize after profiling and testing</li>
<li>Use the most specialized tool for the job</li>
</ol>
<p><em><strong>NOTE: This post is geared towards those without significant experience in linear algebra, high performance computing, or GPU programming.</strong></em></p>
<h1 id="outline"><a class="header" href="#outline">Outline</a></h1>
<ol>
<li>Mathematical Understanding of Algorithm</li>
<li>Algorithm Analysis</li>
<li>C on the Host</li>
<li>CUDA C
<ol>
<li>Core Concepts
<ol>
<li>Host and Device</li>
<li>Using the CUDA Runtime</li>
<li>Shared Memory</li>
<li>CUDA Mat-Vec Multiply</li>
</ol>
</li>
</ol>
</li>
<li>CUDA C++
<ol>
<li>Naive Approach</li>
<li>More Sophisticated Approach</li>
<li>The Best Tool for the Job</li>
</ol>
</li>
<li>Conclusion</li>
<li>BQN Example</li>
<li>Links, References, Additional Reading</li>
</ol>
<h1 id="mathematical-understanding-of-algorithm"><a class="header" href="#mathematical-understanding-of-algorithm">Mathematical Understanding of Algorithm</a></h1>
<p>We’ll be performing a matrix-vector dot product several ways in this post.</p>
<p>The operation is depicted below:</p>
<center>
<img
  src="/images/hpc-101-matvec/matvec.png"
  alt="Matvec dot product, credit this post: https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.2-Multiplying-Matrices-and-Vectors/"
  >
</center>
<p>Let <code>p</code> be the result of the dot product of matrix <code>Mat</code> and vector <code>v</code>.
The dot product is calculated like so:</p>
<center>
$$
\\
  p \gets Mat \cdot v
<p>\</p>
<p>=</p>
<p>v_0 \cdot
\left[ {\begin{array}{c}
Mat_{0, 0} \
Mat_{1, 0} \
Mat_{2, 0} \
\end{array} } \right]
+
v_1 \cdot
\left[ {\begin{array}{c}
Mat_{0, 1} \
Mat_{1, 1} \
Mat_{2, 1} \
\end{array} } \right]
+
v_2 \cdot
\left[ {\begin{array}{c}
Mat_{0, 2} \
Mat_{1, 2} \
Mat_{2, 2} \
\end{array} } \right]</p>
<p>\</p>
<p>=</p>
<p>\left[ {\begin{array}{cc}
(Mat_{0,0} \cdot v_0) + (Mat_{0,1} \cdot v_1) + (Mat_{0,2} \cdot v_2) \
(Mat_{1,0} \cdot v_0) + (Mat_{1,1} \cdot v_1) + (Mat_{1,2} \cdot v_2) \
(Mat_{2,0} \cdot v_0) + (Mat_{2,1} \cdot v_1) + (Mat_{2,2} \cdot v_2) \
\end{array} } \right]
\
$$</p>
<!---
  =   \left[ {\begin{array}{cc}
    6 \\ 24 \\ 42
  \end{array} } \right]
  --->
</center>
<p>Notice how values of <code>v</code> are broadcast to match the shape of <code>Mat</code>:</p>
<center>
$$
\\
  \left[ {\begin{array}{c}
    v_{0} & v_{1} & \cdots & v_{n}\\
    v_{0} & v_{1} & \cdots & v_{n}\\
    \vdots & \vdots & \ddots & \vdots\\
    v_{0} & v_{1} & \cdots & v_{n}\\
  \end{array} } \right]
\\
$$
</center>
<p>We can broadcast values of <code>v</code> into columns of a matrix with the same shape as the matrix <code>Mat</code>, and then pair the <code>Mat</code> and <code>v</code> element-wise, creating a matrix of tuples (or a 3d matrix if you prefer):</p>
<center>
$$
\\
  tuplespace \gets
  \left[ {\begin{array}{cc}
    (Mat_{0,0}, v_0) & (Mat_{0,1}, v_1) & (Mat_{0,2}, v_2) \\
    (Mat_{1,0}, v_0) & (Mat_{1,1}, v_1) & (Mat_{1,2}, v_2) \\
    (Mat_{2,0}, v_0) & (Mat_{2,1}, v_1) & (Mat_{2,2}, v_2) \\
  \end{array} } \right]
\\
$$
</center>
<p>This is sometimes called a <em>tuple space</em>, or the <em>domain</em> of our algorithm.
The book <a href="https://www.worldcat.org/title/how-to-write-parallel-programs-a-first-course/oclc/912171709&referer=brief_results" target="blank"><em>How to Write Parallel Programs: A First Course</em></a> covers tuple spaces in great detail.</p>
<p>Now that we have constructed our tuple space, we might group our computations into self-contained units of work along each row.</p>
<p>Let <em>tuplespace</em> be the 2 dimensional matrix tuple space given above.
We then may form a vector with units of work yielding indices of the output vector:</p>
<center>
$$
\\
  \left[ {\begin{array}{cccc}
    w(0) \gets \sum_{i \gets 0}^{N} tuplespace_{0, i, 0} \cdot tuplespace_{0, i, 1} \\
    w(1) \gets \sum_{i \gets 0}^{N} tuplespace_{1, i, 0} \cdot tuplespace_{1, i, 1} \\
    \vdots \\
    w(M) \gets \sum_{i \gets 0}^{N} tuplespace_{M, i, 0} \cdot tuplespace_{M, i, 1} \\
  \end{array} } \right]
\\
$$
</center>
<p>Equivalently:</p>
<center>
$$
\\
  \left[ {\begin{array}{cccc}
    w(0) \gets \sum_{i \gets 0}^{N} Mat_{0,i} \cdot v_{i} \\
    w(1) \gets \sum_{i \gets 0}^{N} Mat_{1,i} \cdot v_{i} \\
    \vdots \\
    w(M) \gets \sum_{i \gets 0}^{N} Mat_{M,i} \cdot v_{i} \\
  \end{array} } \right]
\\
$$
</center>
<p>Our units of work may independently operate on subsets (rows) of our tuple space.</p>
<h1 id="algorithm-analysis"><a class="header" href="#algorithm-analysis">Algorithm Analysis</a></h1>
<p>The first question we must ask ourselves when parallelizing code is this: <em>are any iterations of the algorithm dependent on values calculated in other iterations? Is iteration <code>N</code> dependent on calculations in iteration <code>N-1</code>?</em>
In other words, <em>are the loop bodies entirely</em> <em><strong>independent</strong></em> <em>of each other?</em></p>
<p>If so, our algorithm is <em>loop independent</em> and <em>trivially parallelizable</em>.
<a href="https://www.cs.utexas.edu/~lin/cs380c/handout27.pdf" target="blank">This slidedeck from a UT Austin lecture</a> are helpful additional reading on this topic.</p>
<p>The fundamental algorithm at play here is a <em>reduction</em> or a <em>fold</em>.
If you see these terms elsewhere in literature, documentation, or algorithms in libraries or programming languages, they almost certainly mean the same thing.
Some collection of values are <em>reduced</em> or <em>folded</em> into a single value.</p>
<p>You might be thinking to yourself, <em>we are starting with a collection of values (a matrix) and yet we end up with a collection of values (a vector). How is this a reduction/fold?</em></p>
<p>This is a good question: the reduction is not performed over the entire matrix, but only the <em>rows</em> of the matrix.
Each row of the matrix is <em>reduced</em> into a single value.</p>
<!---

For the following definitions:

<center>
$$
  \\
  M \gets   \left[ {\begin{array}{cc}
    0 & 1 & 2 \\
    3 & 4 & 5 \\
    6 & 7 & 8 \\
  \end{array} } \right] ,
  v \gets \left[ {\begin{array}{cc} 2 \\ 2 \\ 2  \end{array} } \right]
  \\
$$
</center>

-->
<p>The algorithm each unit of work performs is called <em>transform-reduce</em> (or sometimes <em>map-reduce</em>).</p>
<p>Although <em>transform-reduce</em> might seem like two algorithms (it kinda is!), it is such a universal operation that it is often considered it’s own algorithm (or at least it’s packaged as its own algorithm in libraries).
For example, <a href="https://thrust.github.io/doc/group__transformed__reductions_ga0d4232a9685675f488c3cc847111e48d.html" target="blank">the Thrust abstraction library that ships with NVIDIA’s CUDA Toolkit has the <em>transform-reduce</em> algorithm built-in.</a></p>
<p>In this case, we would like to <em>transform</em> our input tuples by multiplying two elements together, and then <em>reduce</em> our input using the sum operator.</p>
<p>In Python, a given unit of work might look like this:</p>
<pre><code class="language-python">from functools import reduce
tuplespace_row0 = [
    (0, 2),
    (1, 2),
    (2, 2),
    ]

def work(tupl):
    return reduce(
            lambda a, b: a + b,        # use + to reduce
            map(lambda x: x[0] * x[1], # use * to transform
                tupl                   # input tuple
                )
            )

# Input to map is mat_row
# Input to reduce is [0, 2, 4]
# Final value is 6
print(work(tuplespace_row0)) # yields 6
</code></pre>
<p>The following formula is a more formal definition of a single unit of work in our example:</p>
<center>
$$
\\
  r \gets current rank \\
  W_{r} \gets \sum_{i \gets 0}^{N} M_{r,i} \cdot v_{i} \\
\\
$$
</center>
<p>In the above case, the summation is the <em>reduce</em> operation, and the multiplication of the matrix elements and vector elements is the <em>transform</em> operation, transforming each tuple into a scalar before the reduction.</p>
<p>The key insight about this reduction is that no unit of work depends on another unit of work.
The domains of each unit of work are non-overlapping.
In other words, this algorithm is <em>loop independent</em> and can be parallelized along the rows of our tuplespace, again given by:</p>
<center>
$$
\\
  \left[ {\begin{array}{ccc}
    (Mat_{0,0}, v_0) & (Mat_{0,1}, v_1) & (Mat_{0,2}, v_2) \\
    \hline \\
    (Mat_{1,0}, v_0) & (Mat_{1,1}, v_1) & (Mat_{1,2}, v_2) \\
    \hline \\
    (Mat_{2,0}, v_0) & (Mat_{2,1}, v_1) & (Mat_{2,2}, v_2) \\
  \end{array} } \right]
\\
$$
</center>
<p>It was by identifying and understanding the underlying algorithms (<em>broadcast</em> and <em>transform-reduce</em>) of our higher-level algorithm that we are able to determine if and how it is parallelizable and loop independent.</p>
<blockquote>
<p>Identify and understand the underlying algorithms</p>
</blockquote>
<p><em>NOTE: Even if your operation seems to be loop dependent, there are sometimes clever tricks you can use to parallelize your code. Perhaps you just haven’t been exposed to the correct algorithm yet!</em></p>
<p>We now hopefully understand that a matrix-vector product is formally <em>a broadcasted multiply followed by a series of sum-reductions</em> and that we can parallelize our algorithm by breaking it up into self-contained units of work.
We can now move on to implementing and parallelizing the algorithm.</p>
<h1 id="c-on-the-host"><a class="header" href="#c-on-the-host">C on the Host</a></h1>
<p><a href="https://godbolt.org/z/T3qzr8fve" target="blank">The code for such a calculation might look like this in C</a>:</p>
<pre><code class="language-c">void matvecmul(int* mat, int* vec, int* out, int m, int n) {
    for (int i=0; i &lt; m; i++)
        for (int j=0; j &lt; n; j++)
            out[i] += vec[j] * mat[j+(i*n)];
}
</code></pre>
<p>Here’s some example data fed into our matrix vector product:</p>
<pre><code class="language-c">int main() {
    int M = 3;
    int N = 4;

    int mat[M*N];
    for (int i=0; i &lt; M*N; i++) mat[i] = i;

    int vec[N];
    for (int i=0; i &lt; N; i++) vec[i] = i;

    int out[M];

    memset(out, 0, sizeof(int[M]));
    matvecmul(mat, vec, out, M, N);

    return 0;
}
</code></pre>
<p>The output of this program (with some printing code added in):</p>
<pre><code class="language-console">Matrix:
  0   1   2   3 
  4   5   6   7 
  8   9  10  11 
Vector:
  0   1   2   3 
Output:
 14  38  62 
</code></pre>
<p>Feel free to verify these results and play around with other values using <a href="https://keisan.casio.com/exec/system/15052033860538" target="blank">online software like this CASIO calculator website</a>, or a scripting language.
<a href="https://mlochbaum.github.io/BQN/try.html#code=bSDihpAgMwpuIOKGkCA0Ck11bCDihpAgK8ud4oiYw5fijokx4oC/4oieCgptYXQg4oaQIG3igL9u4qWK4oaVMjAwCnZlYyDihpAg4oaVbgoKbWF0IE11bCB2ZWM=" target="blank">
Here’s an example of the above problem using BQN, one of my favorite languages to use when understanding an algorithm.
</a></p>
<p>Demonstrating that we have a <em>correct</em> algorithm with tests is a precondition for optimizing and parallelizing an algorithm:</p>
<blockquote>
<p>Testing for correctness precedes parallelism and performance</p>
</blockquote>
<p>We know that a given index in our output vector can be computed independently of any other indices in the output vector from the respective row in our tuple space.
We can then pull out a function that performs a <em>single unit of work</em> as identified above.</p>
<pre><code class="language-c">int unit_of_work(int* mat, int* vec, int row, int n) {
    double sum = 0;
    mat += row * n;
    for (int i=0; i &lt; n; i++)
        sum += mat[i] * vec[i];
    return sum;
}
</code></pre>
<p>Compare this now with the single unit of work we described above:</p>
<center>
$$
\\
  r \gets current rank \\
  W_{r} \gets \sum_{i \gets 0}^{N} M_{r,i} \cdot v_{i} \\
\\
$$
</center>
<p>Our new <code>matvecmul</code> function can now just iterate over all the rows and dispatch the actual work to the <code>unit_of_work</code> function.
We can even use OpenMP to parallelize our loop:</p>
<pre><code class="language-c">void matvecmul_on_tuplespace(int* mat, int* vec, int* out, int m, int n) {
    // dispatch calculations to unit_of_work for each row of mat
    #pragma omp parallel for
    for (int row=0; row &lt; m; row++)
        out[row] = unit_of_work(mat, vec, row, n);
}
</code></pre>
<p>You might have noticed that our new implementation has more code than our original implementation, and might be slightly more complex.
This is okay, and it gets at an important point:</p>
<blockquote>
<p>Speed is not the same as efficiency</p>
</blockquote>
<a href="https://adspthepodcast.com/2021/11/12/Episode-51.html" target="blank">
This excellent podcast episode from the lead HPC architect at NVIDIA explains this point in detail.
</a>
<p>If our code performs <em>more work overall</em> it is less <em>efficient</em>.
If that additional work means we can perform calculations on multiple threads or additional devices resulting in lower runtime, it is <em>faster</em> and we’ve increased its <em>speed</em>.
The key difference between speed and efficiency is this: speed is a factor of <em>time</em> and efficiency is a factor of <em>work</em>.
Sometimes optimizing code means improving speed, other times efficiency.
Most of the time, to run code on a GPU, you do have to perform more work to set up the calculation, so strictly speaking our code will be faster and less efficient.</p>
<h1 id="cuda-c"><a class="header" href="#cuda-c">CUDA C</a></h1>
<p>CUDA C is the basis of the CUDA runtime, and forms the foundation for all other CUDA-related abstractions.
We’ll take a look at some basic concepts before jumping into the code.
<a href="https://www.nvidia.com/content/GTC-2010/pdfs/2131_GTC2010.pdf" target="blank">
This CUDA C introductory slide deck is helpful in understanding the basics.
</a></p>
<h2 id="core-concepts"><a class="header" href="#core-concepts">Core Concepts</a></h2>
<h3 id="host-and-device"><a class="header" href="#host-and-device">Host and Device</a></h3>
<p>When working with a GPU, it’s important to keep in mind the difference between the <em>host</em> and the <em>device</em>.</p>
<center>
<img
  src="https://cis.temple.edu/~giorgio/cis307/readings/CUDA_processing_flow.png"
  alt="GPU-CPU interaction"
  />
</center>
<p>Just like your CPU, your GPU has access to it’s own <em>memory</em>.
Programming a GPU entails managing your CPU’s memory along with your GPU’s memory.
If you would like your GPU to have access to some memory you’re using on the CPU, you’ll have to allocate memory on the GPU and copy it over.</p>
<a href="https://godbolt.org/z/9eeEedhd5" target="blank">
If you don't tell the GPU to perform any work, then your CUDA C code is really just C code:
</a>
```c
#include <cstdio>
int main() {
  puts("Hello!");
  return 0;
}
```
<p>You can then invoke NVIDIA’s compiler, NVCC, to compile the program:</p>
<pre><code class="language-console">$ cat hello.cu
#include &lt;cstdio&gt;
int main() {
  puts("Hello!");
  return 0;
}
$ nvcc hello.cu -o hello &amp;&amp; ./hello
Hello!
</code></pre>
<p>If you invoke <code>nvcc</code> with the <code>-v</code> flag for extra verbosity, you can see that <code>nvcc</code> actually uses a <em>host</em> compiler to build the parts of your program that don’t involve running code or manipulating memory on the GPU.
<code>nvcc</code> uses multiple passes, where it compiles the CUDA code and generates host-only source for the host compiler to compile.
<a href="https://godbolt.org/z/axTn1ex5x" target="blank">
See this Compiler Explorer link and look at the compilation output window in the bottom right pane to see all the output.
</a>
Notice that GCC is invoked, along with the program <code>ptxas</code>.
PTX is an assembly target, so your CUDA programs will emit ptx code which can be run on your GPU’s special purpose processing units.
<a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#nvcc-examples" target="blank">
The command line flags for code generation can be complicated.
Refer to the official CUDA programming guide when needed.
</a>
Just as you can use <code>asm volitile("" : : : "");</code> in C and C++ to write inline assembly, you can also write inline ptx assembly in your programs.
Also like C and C++, it is almost certainly more effective for you to write your code in a higher level language like CUDA C++, and write PTX after profiling and testing, when you are sure you need it.</p>
<p>If you’re careful, you might also have noticed that GCC was passed the command line argument <code>-x c++</code>, even though we’re working in plain CUDA C.
This is because cuda code is <em>by default built on the host as C++</em>.
If you use the oldest CUDA compiler available on Compiler Explorer, you’ll see that it still defaults to building the host code under C++14.</p>
<p>The full NVCC compilation pipeline is depicted below:</p>
<center>
<img
src="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/graphics/cuda-compilation-from-cu-to-executable.png"
alt="The full NVCC compilation pipeline"
/>
</center>
<h3 id="using-the-cuda-runtime"><a class="header" href="#using-the-cuda-runtime">Using the CUDA Runtime</a></h3>
<a href="https://godbolt.org/z/81v3jfehq" target="blank">
In this example, we introduce three aspects of the CUDA programming model:
</a>
<ul>
<li>The special keyword <code>__global__</code></li>
<li>Device memory management</li>
<li>Kernel launches</li>
</ul>
<pre><code class="language-cuda">// square.cu
#include &lt;cstdio&gt;
#include &lt;cuda.h&gt;
#include &lt;cuda_runtime.h&gt;

__global__ void square(int *ar, int n) {
  int tid = threadIdx.x;
  if (tid &lt; n)
    ar[tid] = ar[tid] * ar[tid];
}

int main() {
  #define N 10

  // Allocate static memory on host
  int ar[N];
  for (int i=0; i &lt; N; i++) ar[i] = i;

  // Allocate memory on device, copy from host to device
  int* d_ar;
  cudaMalloc(&amp;d_ar, sizeof(int[N]));
  cudaMemcpy(d_ar, ar, sizeof(int[N]), cudaMemcpyHostToDevice);

  // Launch kernel to run on the device
  square&lt;&lt;&lt;1, 15&gt;&gt;&gt;(d_ar, N);

  // Copy memory back from device
  cudaMemcpy(ar, d_ar, sizeof(int[N]), cudaMemcpyDeviceToHost);

  // Display values after kernel
  for (int i=0; i &lt; N; i++)
    printf("%d ", ar[i]);
  puts("");

  // Deallocate memory
  cudaFree(d_ar);
  return 0;
}
</code></pre>
<pre><code class="language-console">$ nvcc square.cu -o square
$ ./square
0 1 4 9 16 25 36 49 64 81
</code></pre>
<p><code>__global__</code> indicates that the code <em>runs on the device</em> and is <em>called from the host</em>.
Keep in mind that we have two <em>memory spaces</em> and two <em>execution spaces</em>.</p>
<p>The following table enumerates common operations in C along with their CUDA counterpart:</p>
<center>
{% include hpc-101-matvec/cuda-c-alloc-table.html %}
</center>
<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#execution-configuration" target="blank">
The angle brackets surrounding our _kernel launch parameters_ determine how the kernel will be executed by the GPU.
The possible kernel launch parameters are enumerated at this link.
</a>
<p>The kernel launch parameters determine how many streaming multiprocessors (SMs) will execute code on the GPU.
The first two parameters are objects of type <code>dim3</code>, and they can be up to three-dimensional vectors.
The first kernel launch parameter is the <em>grid size</em>, and the second is the <em>block size</em>.</p>
<p>Grids consist of blocks.</p>
<p>Blocks consist of threads.</p>
<p>Therefore, the total number of threads launched by your kernel will be:</p>
<center>
$$
totalthreads \gets gridsize.x \times gridsize.y \times gridsize.z \\
                   \times blocksize.x \times blocksize.y \times blocksize.z \\
$$
</center>
<p>CUDA kernels may be launched with a 1-3 dimensional grid, and a 1-3 dimensional block.
The image below might have been launched with these kernel launch parameters:</p>
<pre><code class="language-cuda">  dim3 grid_size(3, 3, 1);
  dim3 block_size(3, 3, 1);
  myfunc&lt;&lt;&lt;grid_size, block_size&gt;&gt;&gt;();
</code></pre>
<center>
<img
  src="http://www.microway.com/wp-content/uploads/CUDA-GridBlockThread-Structure.png"
  alt="CUDA Grid and Block Depiction"
  />
</center>
<p>You might also notice that we guard our operation with this <code>if</code> statement.</p>
<pre><code class="language-cuda">  if (tid &lt; n)
    ar[tid] = ar[tid] * ar[tid];
</code></pre>
<p>For performance reasons, it’s usually best to launch your kernels with a multiple of the number of threads in a given block on your GPU, so you may launch with more GPU threads than you need.</p>
<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation" target="blank">
For additional reading on the hardware implementation of the CUDA programming model, please refer to chapter 4 of the NVIDIA CUDA Programming Guide.
</a>
<h3 id="shared-memory"><a class="header" href="#shared-memory">Shared Memory</a></h3>
<p>Although each thread launches with its own stack memory, threads can share memory just like OS threads.
The third kernel launch parameter determines how many bytes will be allocated <em>for each block</em> that is launched.</p>
<a href="https://godbolt.org/z/nrbdK9nKj" target="blank">
In the following example, we make use of CUDA shared memory, as indicated by the `__shared__` keyword annotating the array in our kernel, as well as our use of the third kernel launch parameter:
</a>
```cuda
#include <cstdio>
#include <cuda.h>
#include <cuda_runtime.h>
<p><strong>global</strong> void mulsum(int* a, int* b, int* out, int n) {
int tid = threadIdx.x;
extern <strong>shared</strong> int tmp[];
/* ▲</p>
<ul>
<li>│  ┌───────────────────────────────┐</li>
<li>│  │External and shared, allocated │</li>
<li>└──┤by the cuda runtime when kernel│</li>
<li>│         is launched           │</li>
<li>└───────────────────────────────┘
*/
if (tid &gt;= n)
return;</li>
</ul>
<p>tmp[tid] = a[tid] * b[tid];</p>
<p>__syncthreads();</p>
<p>if (tid == 0) {
int sum = 0;
for (int i=0; i &lt; n; i++)
sum += tmp[i];
*out = sum;
}
}</p>
<p>int main() {
#define N 10</p>
<p>int a[N];
for (int i=0; i &lt; N; i++) a[i] = i;</p>
<p>int b[N];
for (int i=0; i &lt; N; i++) b[i] = i;</p>
<p>int* d_a;
cudaMalloc(&amp;d_a, sizeof(int[N]));
cudaMemcpy(d_a, a, sizeof(int[N]), cudaMemcpyHostToDevice);</p>
<p>int* d_b;
cudaMalloc(&amp;d_b, sizeof(int[N]));
cudaMemcpy(d_b, b, sizeof(int[N]), cudaMemcpyHostToDevice);</p>
<p>int* d_out;
cudaMalloc(&amp;d_out, sizeof(int));</p>
<p>mulsum&lt;&lt;&lt;1, N, sizeof(int[N])&gt;&gt;&gt;(d_a, d_b, d_out, N);
/*             ▲</p>
<ul>
<li>
<pre><code>        │
</code></pre>
</li>
<li>┌────────┴────────────────────────────┐</li>
<li>│Size of shared memory to be allocated│</li>
<li>│         for kernel launch           │</li>
<li>└─────────────────────────────────────┘
*/</li>
</ul>
<p>int out;
cudaMemcpy(&amp;out, d_out, sizeof(int), cudaMemcpyDeviceToHost);
printf(“%d\n”, out);</p>
<p>cudaFree(d_a);
cudaFree(d_b);
cudaFree(d_out);
return 0;
}</p>
<pre><code>
```console
$ nvcc mul-sum-reduce.cu &amp;&amp; ./a.out
285
</code></pre>
<p>Notice how our shared memory is declared:</p>
<pre><code class="language-cuda">  extern __shared__ int tmp[];
</code></pre>
<p>It is <code>external</code> because we are not allocating the memory in our kernel; it’s allocated by the cuda runtime when we pass the third parameter to the kernel launch parameters:</p>
<pre><code class="language-cuda">mulsum&lt;&lt;&lt;1, N, sizeof(int)*N&gt;&gt;&gt;(d_a, d_b, d_out, N);
               ▲
               │
 ┌─────────────┴───────────────────────┐
 │Size of shared memory to be allocated│
 │         for kernel launch           │
 └─────────────────────────────────────┘
</code></pre>
<p>There can only be one segment of shared memory in a kernel launch, so the shared memory segment will be interpreted as whatever type we declare our shared memory with.
In this case, it’s an array of ints.
Although there is strictly one <em>segment</em> of shared memory in a kernel launch, you can still declare multiple variables as <code>__shared__</code>, so long as they all fit in the allocated shared memroy.</p>
<p>We also introduced another CUDA extension to the host language: <code>__syncthreads()</code>.
<code>__syncthreads()</code> is a <em>fence</em> or <em>barrier</em>, a point which no thread <em>in that block</em> can cross until all threads have reached it.
<code>__syncthreads()</code>
There are many other CUDA primitives for atomic, and synchronization operations, such as <code>atomicAdd</code>.</p>
<h3 id="cuda-mat-vec-multiply"><a class="header" href="#cuda-mat-vec-multiply">CUDA Mat-Vec Multiply</a></h3>
<p>We again return to our <code>matvecmul</code> example, this time armed with some knowledge about the CUDA runtime and some software and hardware abstractions.</p>
<pre><code class="language-cuda">#include &lt;cstdio&gt;
#include &lt;cuda.h&gt;
#include &lt;cuda_runtime.h&gt;

__global__ void matvecmul(int* mat, int* vec, int* outv,
                          int m, int n) {

  int rowidx = blockIdx.x;
  int colidx = threadIdx.x;

  extern __shared__ int tmp[];

  if (colidx &lt; n &amp;&amp; rowidx &lt; m) {
    tmp[colidx] = mat[colidx + (rowidx * n)] * vec[colidx];

    __syncthreads();

    if (colidx == 0) {
      int sum = 0;
      for (int i=0; i &lt; n; i++)
        sum += tmp[i];
      outv[rowidx] = sum;
    }
  }
}

int main() {
  #define M 10
  #define N 15

  int a[M*N];
  for (int i=0; i &lt; M*N; i++) a[i] = i;

  int b[N];
  for (int i=0; i &lt; N; i++) b[i] = i;

  int* d_a;
  cudaMalloc(&amp;d_a, sizeof(int[M*N]));
  cudaMemcpy(d_a, a, sizeof(int[M*N]), cudaMemcpyHostToDevice);

  int* d_b;
  cudaMalloc(&amp;d_b, sizeof(int[N]));
  cudaMemcpy(d_b, b, sizeof(int[N]), cudaMemcpyHostToDevice);

  int* d_c;
  cudaMalloc(&amp;d_c, sizeof(int[M]));

  matvecmul&lt;&lt;&lt;M, N, sizeof(int[N])&gt;&gt;&gt;(d_a, d_b, d_c, M, N);

  int c[M];
  cudaMemcpy(c, d_c, sizeof(int[M]), cudaMemcpyDeviceToHost);

  cudaFree(d_a);
  cudaFree(d_b);
  cudaFree(d_c);
  return 0;
}
</code></pre>
<p>After adding some printing code to our example above, we get the following:</p>
<pre><code class="language-console">$ nvcc matvecmul.cu &amp;&amp; ./a.out
Matrix:
   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14
  15   16   17   18   19   20   21   22   23   24   25   26   27   28   29
  30   31   32   33   34   35   36   37   38   39   40   41   42   43   44
  45   46   47   48   49   50   51   52   53   54   55   56   57   58   59
  60   61   62   63   64   65   66   67   68   69   70   71   72   73   74
  75   76   77   78   79   80   81   82   83   84   85   86   87   88   89
  90   91   92   93   94   95   96   97   98   99  100  101  102  103  104
 105  106  107  108  109  110  111  112  113  114  115  116  117  118  119
 120  121  122  123  124  125  126  127  128  129  130  131  132  133  134
 135  136  137  138  139  140  141  142  143  144  145  146  147  148  149

Vector:
   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14

Output:
1015 2590 4165 5740 7315 8890 10465 12040 13615 15190
</code></pre>
<a href="https://mlochbaum.github.io/BQN/try.html#code=TXVsIOKGkCAry53iiJjDl+KOiTHigL/iiJ4KCm3ihpAxMOKAvzE14qWK4oaVMjAwCnbihpDihpUxNQoKbSBNdWwgdg==" target="blank">
This BQN example verifies the output from our CUDA program:
</a>
```
   Mul ← +˝∘×⎉1‿∞
   m←10‿15⥊↕200
   v←↕15
   m Mul v
⟨ 1015 2590 4165 5740 7315 8890 10465 12040 13615 15190 ⟩
```
<p>In our CUDA C example, we launch a block for each row of our matrix.
This way, we can share memory between each thread operating on a given row of the matrix.
A single thread per row can then perform the sum reduction and assign the value to the index in the output vector.</p>
<h1 id="cuda-c-1"><a class="header" href="#cuda-c-1">CUDA C++</a></h1>
<h2 id="naive-approach"><a class="header" href="#naive-approach">Naive Approach</a></h2>
<p>In our previous CUDA C example, we weren’t really using CUDA C <em>perse</em>, but CUDA C++.
If you read the introductory chapter in <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#cuda-general-purpose-parallel-computing-architecture" target="blank">the official NVIDIA CUDA Programming guide</a>, you’ll see that CUDA C is really just CUDA mixed with the common language subset between C and C++ on the host.
We were using CUDA C++ the whole time, we just restricted ourselves to the C subset of C++ for simplicity.</p>
<pre><code class="language-c++">#include &lt;cstdio&gt;
#include &lt;thrust/device_vector.h&gt;
#include &lt;thrust/host_vector.h&gt;
#include &lt;thrust/sequence.h&gt;
#include &lt;cuda.h&gt;
#include &lt;cuda_runtime.h&gt;

/* ┌───────────────────────────┐
   │Using thrust's pointer type│
   │     instead of int*       │
   └───────────────────────────┘ */
__global__ void matvecmul(
    thrust::device_ptr&lt;int&gt; mat,
    thrust::device_ptr&lt;int&gt; vec,
    thrust::device_ptr&lt;int&gt; outv,
    int m, int n) {

  int rowidx = blockIdx.x;
  int colidx = threadIdx.x;

  extern __shared__ int tmp[];

  if (colidx &lt; n &amp;&amp; rowidx &lt; m)
  {
    tmp[colidx] = mat[colidx + (rowidx * n)] * vec[colidx];

    __syncthreads();

    if (colidx == 0) {
      int sum = 0;
      for (int i=0; i &lt; n; i++)
        sum += tmp[i];
      outv[rowidx] = sum;
    }
  }
}

int main() {
  #define M 10
  #define N 15

  /* ┌────────────────────────────────┐
     │ Using thrust's host and device │
     │    vectors over raw arrays.    │
     │No longer need to use cudaMemcpy│
     │        or cudaMalloc!          │
     └────────────────────────────────┘ */
  thrust::device_vector&lt;int&gt; a(M*N);
  thrust::sequence(a.begin(), a.end(), 0);

  thrust::device_vector&lt;int&gt; b(N);
  thrust::sequence(b.begin(), b.end(), 0);

  thrust::device_vector&lt;int&gt; c(M, 0);

  matvecmul&lt;&lt;&lt;M, N, sizeof(int[N])&gt;&gt;&gt;(a.data(), b.data(), c.data(), M, N);

  /* ┌────────────────────────────┐
     │The assignment operator will│
     │   perform the cudaMemcpy   │
     └────────────────────────────┘ */
  thrust::host_vector&lt;int&gt; out = c;

  puts("Output:");
  for (int i=0; i &lt; M; i++)
    printf("%d ", out[i]);
  puts("");
  return 0;
}
</code></pre>
<pre><code class="language-console">$ nvcc thrust-ex.cu &amp;&amp; ./a.out
Output:
1015 2590 4165 5740 7315 8890 10465 12040 13615 15190
</code></pre>
<p>As you can see, the code looks quite similar, except for the lack of memory management.
This is hiding a few extra details as well.
In our original CUDA example, we first allocate and assign to memory on the host before copying it to the device.
In this example, we allocate memory <em>on the device first</em>, and perform assignments <em>on the device</em>.</p>
<p>In this line, we allocate <em>device</em> memory for our matrix, and assign values to it <em>on the device</em>.</p>
<pre><code class="language-c++">  thrust::device_vector&lt;int&gt; a(M*N);
  thrust::sequence(a.begin(), a.end(), 0);
</code></pre>
<p><code>thrust::sequence</code> is almost identical to <code>std::iota</code> or <code>for (int i=0; i &lt; N; i++) vec[i] = i;</code>, except that it may execute on the device.
In this new example, we launch three kernels instead of one: one for each call to <code>thrust::sequence</code>, and one for our manual kernel launch.
<a href="https://godbolt.org/z/nKvajeE5P" target="blank">
You can look at the details of the ptx assembly in Compiler Explorer here.
</a></p>
<h2 id="more-sophisticated-approach"><a class="header" href="#more-sophisticated-approach">More Sophisticated Approach</a></h2>
<p>Remember all that fuss about <em>fundamental algorithms</em> in the earlier sections?
How our <em>fundamental algorithm</em> here is a transform-reduce?</p>
<p>Well, in our first-pass CUDA implementation, we don’t really use this to our advantage.
Our kernel contains the following lines:</p>
<pre><code class="language-cpp">    if (colidx == 0) {
      int sum = 0;
      for (int i=0; i &lt; n; i++)
        sum += tmp[i];
      outv[rowidx] = sum;
    }
</code></pre>
<a href="https://github.com/NVIDIA/thrust/blob/d461afaefdb0b22d830f8d5e9a7b42aebff7004f/thrust/system/cuda/detail/reduce.h#L489" target="blank">
Thrust's `transform_reduce` uses a rather complicated multi-pass, tiled approach to reducing a collection of values to a single value, but we only use a single thread in a block to actually reduce a given index in our output vector.
</a>
<p>While we used a raw loop at least once per block, an optimized reduction will perform something like the following:</p>
<center>
<img
  src="https://i.stack.imgur.com/HxccQ.png"
  alt="depiction of a multi-pass sum reduction"
  />
</center>
<p>Extremely performant reductions are actually quite hard to get right - it’s easy to get <em>some</em> parallelism in a reduction, but it takes significant effort to truly maximize the speed you can get from a GPU.
<a href="https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf" target="blank">
This slidedeck from the NVIDIA developer blog details various approaches to optimizing a reduction operation on a GPU.
</a>
Thrust’s reduce implementation will even select different strategies and launch parameters based on the sizes of the data it operates on.</p>
<p>The point of this Thrust discussion is not to dissuade you from writing raw CUDA kernels - it’s to dissuage you from doing it too early.
In the majority of cases, it’s likely that using a library around raw CUDA kernels will result in faster code and less development time.
Once you have already written your code using <em>known algorithms</em>, once you have tested your code to demonstrate its correctness, once you have profiled your code to demonstrate where the performance bottlenecks are on the target architectures you care about, then it makes sense to write raw CUDA kernels.</p>
<blockquote>
<p>Use sane defaults, only optimize after profiling and testing</p>
</blockquote>
<p>So let’s try again, using Thrust’s parallel algorithms to compute the reductions for each row of the matrix-vector multiplication <a href="https://godbolt.org/z/G7KEfqWcE" target="blank">(godbolt link here)</a>:</p>
<pre><code class="language-cpp">#include &lt;cstdio&gt;
#include &lt;thrust/iterator/zip_iterator.h&gt;
#include &lt;thrust/tuple.h&gt;
#include &lt;thrust/device_vector.h&gt;
#include &lt;thrust/host_vector.h&gt;
#include &lt;thrust/sequence.h&gt;

__global__
void broadcast_to_matrix(thrust::device_ptr&lt;int&gt; mat,
    thrust::device_ptr&lt;int&gt; vec,
    int m, int n) {
  const auto col = blockIdx.x;
  const auto row = threadIdx.x;

  if (row &lt; m and col &lt; n)
    mat[col+(row*n)] = vec[col];
}

int main() {
  #define M 10
  #define N 15

  thrust::device_vector&lt;int&gt; a(M*N);
  thrust::sequence(a.begin(), a.end(), 0);

  thrust::device_vector&lt;int&gt; b(N);
  thrust::sequence(b.begin(), b.end(), 0);

  thrust::device_vector&lt;int&gt; broadcasted_b(M*N);
  broadcast_to_matrix&lt;&lt;&lt;N, M&gt;&gt;&gt;(broadcasted_b.data(), b.data(), M, N);

  thrust::host_vector&lt;int&gt; c(M);

  thrust::zip_iterator iter(thrust::make_tuple(a.begin(), broadcasted_b.begin()));

  for (int i=0; i &lt; M; i++)
    c[i] = thrust::transform_reduce(iter+(i*N), iter+(i*N)+N,
        [] __device__ (auto tup) -&gt; int {
          return thrust::get&lt;0&gt;(tup) * thrust::get&lt;1&gt;(tup);
        },
        0,
        thrust::plus&lt;int&gt;()
        );

  puts("Output:");
  for (int i=0; i &lt; M; i++)
    printf("%d ", c[i]);
  puts("");
  return 0;
}
</code></pre>
<p>Since we’re using some more fancy C++ features, we have to pass some extra flags to the compiler:</p>
<pre><code class="language-console">$ nvcc -std=c++17 --extended-lambda fancy.cu &amp;&amp; ./a.out
Output:
1015 2590 4165 5740 7315 8890 10465 12040 13615 15190
</code></pre>
<p>The first kernel is launched manually, since we’re just broadcasting values from the vector to another vector to match the shape of our matrix.
We perform this step so we can create a zip iterator from the matrix and the broadcasted vector:</p>
<pre><code class="language-cpp">  thrust::zip_iterator iter(thrust::make_tuple(a.begin(), broadcasted_b.begin()));
</code></pre>
<p>This means we can feed the single iterator into our <code>transform_reduce</code> operation.
Elements obtained by the zip iterator are passed into our lambda function, and we simply multiply the two values together, before using the <code>plus</code> functor to reduce the vector of intermediate values for a given row into a scalar:</p>
<pre><code class="language-cpp">  for (int i=0; i &lt; M; i++)
    c[i] = thrust::transform_reduce(iter+(i*N), iter+(i*N)+N,
        [] __device__ (auto tup) -&gt; int {
          return thrust::get&lt;0&gt;(tup) * thrust::get&lt;1&gt;(tup);
        },
        0,
        thrust::plus&lt;int&gt;()
        );
</code></pre>
<p>If we want to use threading on the host as well, we can even use an OpenMP directive:</p>
<pre><code class="language-cpp">#pragma openmp parallel for
  for (int i=0; i &lt; M; i++)
    c[i] = thrust::transform_reduce(iter+(i*N), iter+(i*N)+N,
        [] __device__ (auto tup) -&gt; int {
          return thrust::get&lt;0&gt;(tup) * thrust::get&lt;1&gt;(tup);
        },
        0,
        thrust::plus&lt;int&gt;()
        );
</code></pre>
<p>We’ll have to tell <code>nvcc</code> to pass the <code>-fopenmp</code> flag to the host compiler:</p>
<pre><code class="language-console">$ nvcc -std=c++17 --extended-lambda -Xcompiler -fopenmp fancy.cu
</code></pre>
<h2 id="the-best-tool-for-the-job"><a class="header" href="#the-best-tool-for-the-job">The Best Tool for the Job</a></h2>
<p>We have by now hopefully learned that we should use the most specialized tool for the job, and we should write kernels by hand only when we’re sure we can do better than your libraries of choice.
We can take this principle one step further with a little extra knowledge of our problem.</p>
<p>A matrix-vector product is a very common linear algebra operation, and a member of the Basic Linear Algebra Subroutines interface, which CUDA provides a library for (CUBLAS).
Because this is such a common operation, NVIDIA provides an extremely fast implementation - far more optimized than anything we would write by hand.</p>
<p>This knowledge of our problem leads us to using the most appropriate library, and likely to the fastest solution.</p>
<pre><code class="language-cpp">#include &lt;cstdio&gt;
#include &lt;thrust/device_vector.h&gt;
#include &lt;thrust/host_vector.h&gt;
#include &lt;thrust/sequence.h&gt;
#include &lt;cuda.h&gt;
#include &lt;cuda_runtime.h&gt;
#include &lt;cublas_v2.h&gt;

int main() {
  #define M 10
  #define N 15

  cublasHandle_t ch;
  cublasCreate(&amp;ch);

  thrust::device_vector&lt;double&gt; a(M*N);
  thrust::sequence(a.begin(), a.end(), 0);

  const double alpha = 1.0;
  const double beta = 0.0;

  thrust::device_vector&lt;double&gt; b(N);
  thrust::sequence(b.begin(), b.end(), 0);

  thrust::device_vector&lt;double&gt; c(M);

  #define PTR(x) thrust::raw_pointer_cast(x.data())
  cublasDgemv(
      ch,
      CUBLAS_OP_T,
      N, M,
      &amp;alpha,
      PTR(a), N,
      PTR(b), 1,
      &amp;beta,
      PTR(c), 1
      );
  #undef PTR

  thrust::host_vector&lt;double&gt; hc = c;

  puts("Output:");
  for (int i=0; i &lt; M; i++)
    printf("%.1f ", hc[i]);
  puts("");

  cublasDestroy(ch);

  return 0;
}
</code></pre>
<pre><code class="language-console">$ nvcc cublas.cu -lcublas &amp;&amp; ./a.out
Output:
1015.0 2590.0 4165.0 5740.0 7315.0 8890.0 10465.0 12040.0 13615.0 15190.0
</code></pre>
<h1 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h1>
<h1 id="bqn-example"><a class="header" href="#bqn-example">BQN Example</a></h1>
<p>Personally, I use BQN to prototype solutions to problems and to better understand the fundamental algorithms at play; you don’t have to know an APL in order to understand this, but it might be helpful.
Feel free to skip this section; it is not critical to understanding the concepts.</p>
<p><a href="https://mlochbaum.github.io/BQN/try.html#code=4oCiU2hvdyBtYXQg4oaQIDPigL8z4qWK4oaVMTAK4oCiU2hvdyB2ZWMg4oaQIDPipYoyCivLneKOiTEgbWF0w5d2ZWMK" target="blank">Here’s a permalink to the BQN snippet.</a></p>
<pre><code>   # Same matrix as in our C example
   mat ← 3‿3⥊↕10
┌─       
╵ 0 1 2  
  3 4 5  
  6 7 8  
        ┘
   # Same vector as in our C example
   vec ← 3⥊2
⟨ 2 2 2 ⟩

   +˝⎉1 mat×vec
⟨ 6 24 42 ⟩
</code></pre>
<p>The core algorithm is seen in the final expression:</p>
<pre><code>+˝⎉1 mat×vec
▲    ▲
│    │     ┌───────────────────────────┐
│    └─────┤Multiply rows of mat by vec│
│          │        element-wise       │
│          └───────────────────────────┘
│     ┌─────────────────────────┐
│     │Sum-reduce rows of matrix│
└─────┤ resulting from mat×vec  │
      └─────────────────────────┘
</code></pre>
<p>Alternatively:</p>
<center>
<img height=300 src="csblog//images/hpc-101-matvec/bqn-matvecmul-explain.png" alt="Try BQN explanation of matvecmul"/>
</center>
<h1 id="links-references-additional-reading"><a class="header" href="#links-references-additional-reading">Links, References, Additional Reading</a></h1>
<ul>
<li><a href="https://mlochbaum.github.io/BQN/try.html#code=4oCiU2hvdyBtYXQg4oaQIDPigL8z4qWK4oaVMTAK4oCiU2hvdyB2ZWMg4oaQIDPipYoyCivLneKOiTEgbWF0w5d2ZWMK" target="blank">BQN matvecmul example</a></li>
<li><a href="https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.2-Multiplying-Matrices-and-Vectors/" target="blank">Matrix-Vector Product image</a></li>
<li><a href="https://www.cs.utexas.edu/~lin/cs380c/handout27.pdf" target="blank">UT Austin slides on loop-carried dependencies and parallelism</a></li>
<li><a href="https://www.worldcat.org/title/how-to-write-parallel-programs-a-first-course/oclc/912171709&referer=brief_results" target="blank"><em>How to Write Parallel Programs: A First Course</em></a></li>
<li><a href="https://thrust.github.io/doc/group__transformed__reductions_ga0d4232a9685675f488c3cc847111e48d.html" target="blank">Thrust parallel algorithms library</a></li>
<li><a href="https://adspthepodcast.com/2021/11/12/Episode-51.html" target="blank"> ADSP podcast episode from the lead HPC architect at NVIDIA discussing speed vs efficiency</a></li>
<li><a href="https://youtu.be/KK3JXvSiJG4" target="blank"> Bryce Adelstein Lelbach’s talk on C++ standard parallelism </a></li>
<li><a href="https://github.com/kokkos/mdspan/blob/single-header/mdspan.hpp" target="blank"> Kokkos <code>mdspan</code> single header </a></li>
<li><a href="https://www.nvidia.com/content/GTC-2010/pdfs/2131_GTC2010.pdf" target="blank">CUDA C Introduction Slides</a></li>
<li><a href="https://github.com/uysalere/cuda-matrix-vector-multiplication" target="blank"> More sophisticated CUDA matrix-vector product implementations </a></li>
<li><a href="https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf" target="blank"> Slides on CUDA reduction operation </a></li>
</ul>
<!--
layout: post
title: Debugging Performance in Compilers
permalink: /comp-debug-perf
category: c++, llvm, compilers
wip: false
cat: cs
-->
<p>Overview of how I debug performance regressions when developing a compiler.
I don’t claim this is the best way to do it, email me or tweet at me if you’ve got better ideas😉</p>
<h2 id="starting-point"><a class="header" href="#starting-point">Starting Point</a></h2>
<p>Compilers are very complicated and the results can be surprising.
Sometimes performance issues only show up in large scale real-world applications.
How do you go about debugging such an issue?</p>
<p>As you might expect, narrowing down the issue to be minimal and reproducible is the first task.
Ideally, we narrow the performance regression down to a single translation unit, though sometimes this isn’t enough.
For this post, we’ll assume that the bulk of the performance regression you see in your application is coming from one translation unit, and that you know which patch is causing the regression (if you don’t know which patch is causing the regression… well you can bisect the recent patches too😁).</p>
<h2 id="bisecting-the-object-files"><a class="header" href="#bisecting-the-object-files">Bisecting the Object Files</a></h2>
<p>Assume we have two compilers: compiler A which doesn’t have the “bad” changes (the “good” compiler), and compiler B which does (the “bad” compiler).
We’ll start by building the application with both compilers, building half of the object files with compiler A and half with compiler B.
Say we have 100 object files that are linked into the application; we’d build the first 50 with compiler A and the second 50 with compiler B.</p>
<p>If the perf regression isn’t observed after you re-link all the object files into the application, then we know the bulk of the issue is in the object files that were just built with compiler A.
We can then rebuild all the object files in the second 50 with compiler A and build object files 26-50 or 1-25 with compiler B.
In this way, we bisect all the translation units until we find the single TU with the largest impact on performance.</p>
<p>This can be really tedious and manual, but it’s not too hard to script😉.</p>
<h2 id="bisecting-the-source-file"><a class="header" href="#bisecting-the-source-file">Bisecting the Source File</a></h2>
<p>Now that we’ve narrowed our regression down to a single TU, our work gets a little more complicated.
We can use the same bisection process as before, but this time we’ll have to do it on a single file.
To acomplish this, we’ll have to figure out which parts of the source file depend on each other so we can break it into two new source files, one to be built with compiler A and one to be built with compiler B (all other TUs being built with the “good” compiler).</p>
<p>Depending on the situation you may create two source files, each with half of the content of the original, or maybe you’ll use the same source file but use macro guards so each compiler only builds half of the source, eg:</p>
<pre><code class="language-c++">/* includes, declarations, and global defs up here */

#ifdef COMPILERA
// stuff built with the good compiler...
#else /* COMPILERB */
// stuff built with the bad compiler...
#endif
</code></pre>
<p>You may then add <code>-DCOMPILERA</code> to the invokation of compiler A so each compiler only builds half of the TU in question.
Again, if we don’t see the perf regression, we swap the macro guards and try again.
We then have compiler B build a quarter of the original TU and have compiler A build the other 3/4ths, and see if we observe the regression, etc etc.
Ideally, at the end of this process we know exactly which function(s) are causing the regression.</p>
<h2 id="what-next"><a class="header" href="#what-next">What Next?</a></h2>
<p>After we’ve narrowed the regression down to a function or two (🤞) things can get tricky, and very much depends on the nature of the changes that caused the regression.</p>
<p>At this point I think it’s best to ask some questions:</p>
<ul>
<li>Was the patch in question related to a specific pass?
<ul>
<li>Can the effects of that pass be seen in the function(s) we found to be causing the regression?</li>
<li>Is the regression observed when the pass is disabled?</li>
</ul>
</li>
<li>Do you notice any obvious differences between the IR the compilers generate for the identified functions?
<ul>
<li>Can you use those differences to work backwards to the code that generated that IR?</li>
</ul>
</li>
<li>If you enable lots of debugging output (like dumping all the <code>opt</code> pass remarks) and build with compilers A and B and then diff the output, are there any glaring differences? Maybe an earlier change allowed another pass (uninvolved in the patch) to perform some transformations it otherwise would not, or maybe vice-versa.</li>
</ul>
<h2 id="why-might-this-not-work"><a class="header" href="#why-might-this-not-work">Why Might This Not Work?</a></h2>
<p>Sometimes the effects only occur in a short function that is always inlined, in which case you might not find a specific TU or set of functions at the root of the regression; for this reason, you might want to crank the inlining pass down as low as it goes to help you narrow down the issue.
It’s often best to use the fewest optimizations possible when debugging this sort of thing (so long as you still observe the behavior).</p>
<!--
-->
<!--
layout: post
title: std::expected And Why It's Awesome
permalink: /std-expected
cat: cs
-->
<p><code>std::expected</code> and some spectacular extensions are hopefully coming to C++23.</p>
<h2 id="existing-practice"><a class="header" href="#existing-practice">Existing Practice</a></h2>
<p>I’m used to seeing code that looks like this, not just in C but in C++ codebases too:</p>
<pre><code class="language-c++">int main() {
  int ierr;
  double * mat = (double*)malloc(sizeof(double[100]));

  ierr = dowork(mat, 10, 10);
  check_error(ierr);

  ierr = domorework(mat, 10, 10);
  check_error(ierr);

  free(mat);
}
</code></pre>
<p>Integers represent error conditions, and these usually map to an error message like so:</p>
<pre><code class="language-c++">const char* error_messages[] = {
    "success",
    "got some failure",
    "another kind of failure",
};
enum error_types {
    success,
    some_failure,
    some_other_failure,
};

void check_error(int ierr) {
    if (ierr) {
        printf("got error '%s'\n", error_messages[ierr]);
        exit(ierr);
    }
}
</code></pre>
<p>This way when you encounter an error condition in some function, you just return the corresponding error code like so:</p>
<pre><code class="language-c++">int dowork(double* matrix, int M, int N) {
  // do some work here

  if (/*some error condition*/)
      return some_failure;

  // success state
  return success;
}
</code></pre>
<p>And the error handler reports the failures:</p>
<pre><code class="language-console">  Program returned: 1
got error 'got some failure'
</code></pre>
<p>On one hand, there’s a sense of security to writing code like this.
You always check your error conditions, your code never throws exceptions, and the range of possible failures is very clear.
I don’t mind this pattern in C, but in C++ we have some goodies that are far nicer to use in my opinion (especially in C++23).</p>
<h2 id="why-stdexpected-is-awesome"><a class="header" href="#why-stdexpected-is-awesome">Why <code>std::expected</code> is Awesome</a></h2>
<p>I’ll be using <a href="https://github.com/kokkos/mdspan"><code>mdspan</code> from the Kokkos implementation</a> and <a href="https://github.com/TartanLlama/expected"><code>expected</code> from Sy Brand’s implementation</a> for this section.</p>
<p>In the last year, 3 papers have come through the ISO C++ mailing lists for <code>std::expected</code>, and <a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2021/p2505r0.html">the most recent paper from Jeff Garland</a> proposes some of Sy Brand’s wonderful extensions to <code>std::expected</code> which allow you to chain together monadic operations on expected values:</p>
<ol>
<li><code>and_then</code></li>
<li><code>or_else</code></li>
<li><code>transform</code></li>
</ol>
<p>I think these extensions are <em>extremely</em> elegant, and I think some folks that are more used to the C-style error handling could be won over.
Using <code>std::expected</code> means your errors have <em>value semantics</em>, which is something I like about the C-stlye error handling.
Chaining together these operations makes the programmer’s intent so much clearer.</p>
<p>Let’s look at another example using matrices, but this time using <code>expected</code> and <code>mdspan</code>.</p>
<h3 id="expected-example"><a class="header" href="#expected-example"><code>expected</code> Example</a></h3>
<p>I’ll get some <code>using</code> statements out of the way:</p>
<pre><code class="language-c++">namespace stdex = std::experimental;
using mat_t = stdex::mdspan&lt;double, 
    stdex::extents&lt;
      stdex::dynamic_extent,
      stdex::dynamic_extent
    &gt;
  &gt;;
using expect_mat = tl::expected&lt;mat_t, std::string&gt;;
</code></pre>
<p>I can’t help but marvel at how nice and readable this looks:
The intent of the programmer is very clear in my opinion, even without seeing the rest of the code.</p>
<pre><code class="language-cpp">int main() {
  auto raw = std::make_unique&lt;double[]&gt;(25);
  auto mat = mat_t(raw.get(), 5, 5);
  setup(mat)                  // zero initialize
    .and_then(set_diag)       // set the diagonal of the matrix
    .and_then(print)          // print out some values
    .or_else(report_errors);  // if unexpected, print the error and quit
}

/*
 * Program returned: 0
 * 1.0 0.0 0.0 0.0 0.0 
 * 0.0 1.0 0.0 0.0 0.0 
 * 0.0 0.0 1.0 0.0 0.0 
 * 0.0 0.0 0.0 1.0 0.0 
 * 0.0 0.0 0.0 0.0 1.0 
 */
</code></pre>
<p>This leads to some nice and/or interesting patterns.
Say we want to check that the matrix passed to <code>set_diag</code> is square;
we could perform our check and return an error message if the check fails, much like we could throw an exception:</p>
<pre><code class="language-cpp">auto set_diag(mat_t mat) {
  if (mat.extent(0) != mat.extent(1))
    return make_unexpected("expected square matrix!");

  for (int i=0; i &lt; mat.extent(0); i++)
    mat(i, i) = 1.0;

  return expect_mat(mat);
} 
</code></pre>
<p>I also like using an immediatly invoked lambda for this, but I’m not sure how readable/maintainable this is long-term:</p>
<pre><code class="language-cpp">auto set_diag(mat_t mat) {
  return mat.extent(0) == mat.extent(1)
    ? [=] {
      for (int i=0; i &lt; mat.extent(0); i++)
        mat(i, i) = 1.0;
      return expect_mat(mat);
    }()
    : make_unexpected("expected square matrix!");
} 
</code></pre>
<p>Either way, the error is handled as expected (no pun intended):</p>
<pre><code class="language-cpp">auto report_errors(std::string_view err) {
  fmt::print("got error: '{}'\n", err);
  std::exit(EXIT_FAILURE);
}
int main() {
  auto raw = std::make_unique&lt;double[]&gt;(25);
  // It's not square!
  auto mat = mat_t(raw.get(), 25, 1);
  setup(mat)
    .and_then(set_diag)
    .and_then(print)
    .or_else(report_errors);
}
/*
 * Program returned: 1
 * got error: 'expected square matrix!'
 */
</code></pre>
<h3 id="3-ways-to-use-the-monadic-functions-on-an-expected-value"><a class="header" href="#3-ways-to-use-the-monadic-functions-on-an-expected-value">3 Ways to use the Monadic Functions on an Expected Value</a></h3>
<h4 id="1-functor"><a class="header" href="#1-functor">1. Functor</a></h4>
<p>We can use functors in the expected chain like so:</p>
<pre><code class="language-cpp">struct SetRow {
  std::size_t row;
  double value;
  expect_mat operator()(mat_t mat) {
    for (int i=0; i&lt;mat.extent(1); i++)
      mat(row, i) = value;
    return mat;
  }
};
int main() {
  auto raw = std::make_unique&lt;double[]&gt;(25);
  auto mat = mat_t(raw.get(), 5, 5);
  setup(mat)
    .and_then(set_diag)
    .and_then(SetRow{1, 3.5})
    .and_then(print)
    .or_else(report_errors);
}
/*
 * Program returned: 0
 * 1.0 0.0 0.0 0.0 0.0 
 * 3.5 3.5 3.5 3.5 3.5 
 * 0.0 0.0 1.0 0.0 0.0 
 * 0.0 0.0 0.0 1.0 0.0 
 * 0.0 0.0 0.0 0.0 1.0 
 */
</code></pre>
<h4 id="2-binding-args-to-a-function-that-takes-multiple-arguments"><a class="header" href="#2-binding-args-to-a-function-that-takes-multiple-arguments">2. Binding Args to a Function that Takes Multiple Arguments</a></h4>
<p>Using <code>std::bind</code> on a function taking more arguments would also acomplish this:</p>
<pre><code class="language-cpp">auto set_row(mat_t mat, std::size_t row, double value) {
    for (int i=0; i&lt;mat.extent(1); i++)
        mat(row, i) = value;
    return expect_mat(mat);
}
int main() {
  auto raw = std::make_unique&lt;double[]&gt;(25);
  auto mat = mat_t(raw.get(), 5, 5);
  setup(mat)
    .and_then(set_diag)
    .and_then(std::bind(set_row, /*mat=*/_1, /*row=*/1, /*value=*/3.5))
    .and_then(print)
    .or_else(report_errors);
}
/*
 * Program returned: 0
 * 1.0 0.0 0.0 0.0 0.0 
 * 3.5 3.5 3.5 3.5 3.5 
 * 0.0 0.0 1.0 0.0 0.0 
 * 0.0 0.0 0.0 1.0 0.0 
 * 0.0 0.0 0.0 0.0 1.0 
 */
</code></pre>
<h4 id="3-lambdas"><a class="header" href="#3-lambdas">3. Lambdas</a></h4>
<p>And of course, lambdas:</p>
<pre><code class="language-cpp">int main() {
  auto raw = std::make_unique&lt;double[]&gt;(25);
  auto mat = mat_t(raw.get(), 5, 5);
  setup(mat)
    .and_then(set_diag)
    .and_then([] (auto mat) {
      for (int i=0; i &lt; mat.extent(1); i++)
        mat(3, i) = 2.0;
      return expect_mat(mat);
    })
    .and_then(print)
    .or_else(report_errors);
}
/*
 * Program returned: 0
 * 1.0 0.0 0.0 0.0 0.0 
 * 0.0 1.0 0.0 0.0 0.0 
 * 0.0 0.0 1.0 0.0 0.0 
 * 2.0 2.0 2.0 2.0 2.0 
 * 0.0 0.0 0.0 0.0 1.0
 */
</code></pre>
<h2 id="conclusion-1"><a class="header" href="#conclusion-1">Conclusion</a></h2>
<p>Hopefully you’ve been won over by the elegance of <code>expected</code> and <code>mdspan</code>.
Godbolt links can be found for these examples in the links below:</p>
<ol>
<li><a href="https://youtu.be/uj9ozuzZy6g">YouTube version of this content</a></li>
<li><a href="https://godbolt.org/z/jPqdYPEv9">C-style example</a></li>
<li><a href="https://godbolt.org/z/hWYj34EcW">Full <code>expected</code>+<code>mdspan</code> example</a></li>
<li><a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2021/p2505r0.html">Jeff Garland’s 12/2022 <code>expected</code> paper</a></li>
<li><a href="https://github.com/TartanLlama">Sy Brand’s <code>tl::expected</code></a></li>
<li><a href="https://github.com/kokkos/mdspan">Kokkos <code>mdspan</code> impl</a></li>
</ol>
<!--
layout: post
title: GTest Type and Value Parameterized Tests
permalink: /gtest-type-val-param
cat: cs
-->
<p>GTest exposes clean interfaces for parameterizing your tests by value and by
type - but what if you want both?</p>
<h2 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h2>
<p>The need for this arose when converting some of Flang’s unit tests to use GTest
to conform with the rest of LLVM’s testing framework (Flang is LLVM’s fortran 2018 compiler).
<a href="https://reviews.llvm.org/D97403">Linked here is the revision where we needed to parameterize both by value and type.</a></p>
<p>Let’s start with an interface to test:</p>
<pre><code class="language-cpp">// addone.hpp
#pragma once

template&lt;typename T&gt;
auto addOne(T t) {
  return t + 1;
}
</code></pre>
<p>Our first test might look like this:</p>
<pre><code class="language-cpp">#include &lt;gtest/gtest.h&gt;
#include "addone.hpp"

TEST(AddOneTests, doAddTo5) {
  ASSERT_EQ(addOne(5), 6) &lt;&lt; "addOne(5) != 6!";
}
</code></pre>
<p>What if we want to test multiple values without repeating code?
Instead of something like this:</p>
<pre><code class="language-cpp">TEST(AddOneTests, doAddTo5) {
  ASSERT_EQ(addOne(5), 6) &lt;&lt; "addOne(5) != 6!";
  ASSERT_EQ(addOne(6), 7) &lt;&lt; "addOne(6) != 7!";
  ...
}
</code></pre>
<h2 id="value-parameterized-tests"><a class="header" href="#value-parameterized-tests">Value Parameterized Tests</a></h2>
<p>We can parameterize our test by value with a fixture:</p>
<pre><code class="language-cpp">struct AddOneTestsFixture
    : public testing::TestWithParam&lt;std::tuple&lt;int, int&gt;&gt; {};

TEST_P(AddOneTestsFixture, doAdd) {
  int input = std::get&lt;0&gt;(GetParam());
  int expect = std::get&lt;1&gt;(GetParam());
  ASSERT_EQ(addOne(input), expect)
      &lt;&lt; "addOne(" &lt;&lt; input &lt;&lt; ") != " &lt;&lt; expect &lt;&lt; "!";
}

INSTANTIATE_TEST_SUITE_P(
    AddOneTests,
    AddOneTestsFixture,
    testing::Values(
      std::make_tuple(1, 2),
      std::make_tuple(3, 4),
      std::make_tuple(9, 10)));
</code></pre>
<p>This way, our tests run over all values we pass in at the end:</p>
<pre><code class="language-console">$ ./tests
Running main() from /tmp/googletest-20201214-81667-fx54ix/googletest-release-1.10.0/googletest/src/gtest_main.cc
[==========] Running 3 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 3 tests from AddOneTests/AddOneTestsFixture
[ RUN      ] AddOneTests/AddOneTestsFixture.doAdd/0
[       OK ] AddOneTests/AddOneTestsFixture.doAdd/0 (0 ms)
[ RUN      ] AddOneTests/AddOneTestsFixture.doAdd/1
[       OK ] AddOneTests/AddOneTestsFixture.doAdd/1 (0 ms)
[ RUN      ] AddOneTests/AddOneTestsFixture.doAdd/2
[       OK ] AddOneTests/AddOneTestsFixture.doAdd/2 (0 ms)
[----------] 3 tests from AddOneTests/AddOneTestsFixture (0 ms total)

[----------] Global test environment tear-down
[==========] 3 tests from 1 test suite ran. (0 ms total)
[  PASSED  ] 3 tests.
</code></pre>
<h2 id="type-parameterized-tests"><a class="header" href="#type-parameterized-tests">Type Parameterized Tests</a></h2>
<p>Our interface <code>addOne</code> takes a template parameter - what if we want to test this
on multiple types?</p>
<p>First, we’ll want our fixture to take template parameters and we’ll have to
declare the fixture as templated in GTest:</p>
<pre><code class="language-cpp">template&lt;typename T&gt;
struct AddOneTestsFixture : public ::testing::Test {};
TYPED_TEST_SUITE_P(AddOneTestsFixture);
</code></pre>
<p>And keep the first iteration of our test, but this time using the <code>TypeParam</code>
type exposed by the GTest <code>TYPED_TEST_SUITE</code> api:</p>
<pre><code class="language-cpp">TYPED_TEST_P(AddOneTestsFixture, doAddOne) {
  ASSERT_EQ(addOne&lt;TypeParam&gt;(5), 6) &lt;&lt; "addOne(5) != 6!";
}
</code></pre>
<p>We’ll also have to register each test with our typed test suite:</p>
<pre><code class="language-cpp">REGISTER_TYPED_TEST_SUITE_P(AddOneTestsFixture, doAddOne);
</code></pre>
<p>If we had more tests, you would register them in the same statement as above:</p>
<pre><code class="language-cpp">REGISTER_TYPED_TEST_SUITE_P(AddOneTestsFixture, doAddOne, doAddTwo, ...);
</code></pre>
<p>We are then able to instantiate our templated test suite with all the types we
intend to use with our test suite:</p>
<pre><code class="language-cpp">using Types = testing::Types&lt;int, long long, std::size_t&gt;;
INSTANTIATE_TYPED_TEST_SUITE_P(TestPrefix, AddOneTestsFixture, Types);
</code></pre>
<p>And our type-parameterized tests are working!</p>
<pre><code class="language-console">$ ./tests
Running main() from /tmp/googletest-20201214-81667-fx54ix/googletest-release-1.10.0/googletest/src/gtest_main.cc
[==========] Running 4 tests from 4 test suites.
[----------] Global test environment set-up.
[----------] 1 test from TestPrefix/AddOneTestsFixture/0, where TypeParam = int
[ RUN      ] TestPrefix/AddOneTestsFixture/0.doAddOne
[       OK ] TestPrefix/AddOneTestsFixture/0.doAddOne (0 ms)
[----------] 1 test from TestPrefix/AddOneTestsFixture/0 (0 ms total)

[----------] 1 test from TestPrefix/AddOneTestsFixture/2, where TypeParam = long long
[ RUN      ] TestPrefix/AddOneTestsFixture/2.doAddOne
[       OK ] TestPrefix/AddOneTestsFixture/2.doAddOne (0 ms)
[----------] 1 test from TestPrefix/AddOneTestsFixture/2 (0 ms total)

[----------] 1 test from TestPrefix/AddOneTestsFixture/3, where TypeParam = unsigned long
[ RUN      ] TestPrefix/AddOneTestsFixture/3.doAddOne
[       OK ] TestPrefix/AddOneTestsFixture/3.doAddOne (0 ms)
[----------] 1 test from TestPrefix/AddOneTestsFixture/3 (0 ms total)

[----------] Global test environment tear-down
[==========] 4 tests from 4 test suites ran. (0 ms total)
[  PASSED  ] 4 tests.
</code></pre>
<h2 id="type-and-value-parameterized-tests"><a class="header" href="#type-and-value-parameterized-tests">Type <em><strong>and</strong></em> Value Parameterized Tests</a></h2>
<p>Now is the tricky part - GTest doesn’t expose an API for parameterizing tests
over values and types so we have to do some work ourselves.</p>
<p>First, let’s define the types and input data we’ll be parameterizing our tests over:</p>
<pre><code class="language-cpp">template &lt;typename T&gt;
using ParamT = std::vector&lt;std::tuple&lt;T, T&gt;&gt;;

static std::tuple&lt;ParamT&lt;int&gt;, ParamT&lt;long long&gt;, ParamT&lt;std::size_t&gt;&gt; allParams{
  { // Test cases for int
    std::make_tuple(1, 2),
    std::make_tuple(5, 6),
    std::make_tuple(9, 10),
  },
  { // Test cases for long long
    std::make_tuple(1, 2),
    std::make_tuple(5, 6),
    std::make_tuple(9, 10),
  },
  { // Test cases for size_t
    std::make_tuple(1, 2),
    std::make_tuple(5, 6),
    std::make_tuple(9, 10),
  },
};
</code></pre>
<p>This structure assumes you may want to add test inputs later on that may be
different depending on the type.
If this is not the case and you know your <code>make_tuple</code> calls are <code>static_cast</code>-able
into your parameter type, you may do the following to reduce code duplication:</p>
<pre><code class="language-cpp">#define ADDONE_TESTPARAMS                                                      \
  { std::make_tuple(1, 2), std::make_tuple(5, 6), std::make_tuple(9, 10), }
static std::tuple&lt;ParamT&lt;int&gt;, ParamT&lt;long long&gt;, ParamT&lt;std::size_t&gt;&gt; allParams{
        ADDONE_TESTPARAMS, ADDONE_TESTPARAMS, ADDONE_TESTPARAMS, };
</code></pre>
<p>Now, let’s refactor our fixture to take the types and values we just defined:</p>
<pre><code class="language-cpp">template &lt;typename T&gt;
struct AddOneTestsFixture : public testing::Test {
  AddOneTestsFixture() : params{std::get&lt;ParamT&lt;T&gt;&gt;(allParams)} {}
  ParamT&lt;T&gt; params;
};
</code></pre>
<p>You may notice we set <code>params</code> to <code>std::get&lt; ParamT&lt;T&gt; &gt;(allParams)</code> - this is how
we accomplish type <em>and</em> value parameterized tests.
We use the infrastructure of a type parameterized test, and leverage <code>std::tuple</code>
to do the value parameterization.</p>
<p>For the actual test code, we again reuse most of the test from our first type-
parameterized test, this time using the <code>params</code> field of our test fixture:</p>
<pre><code class="language-cpp">TYPED_TEST_P(AddOneTestsFixture, doAddOne) {

  // Iterate over the parameters configred by our fixture
  for(auto const&amp; [input, expect] : this-&gt;params) {

    // The assertions stay the same as in our original type-parameterized test
    ASSERT_EQ(addOne(input), expect)
      &lt;&lt; "addOne(" &lt;&lt; input &lt;&lt; ") != " &lt;&lt; expect &lt;&lt; "!";
  }
}
</code></pre>
<p>And voilà! our tests are parameterized over values and types:</p>
<pre><code class="language-console">$ ./tests
Running main() from /tmp/googletest-20201214-81667-fx54ix/googletest-release-1.10.0/googletest/src/gtest_main.cc
[==========] Running 3 tests from 3 test suites.
[----------] Global test environment set-up.
[----------] 1 test from TestPrefix/AddOneTestsFixture/0, where TypeParam = int
[ RUN      ] TestPrefix/AddOneTestsFixture/0.doAddOne
[       OK ] TestPrefix/AddOneTestsFixture/0.doAddOne (0 ms)
[----------] 1 test from TestPrefix/AddOneTestsFixture/0 (0 ms total)

[----------] 1 test from TestPrefix/AddOneTestsFixture/1, where TypeParam = long long
[ RUN      ] TestPrefix/AddOneTestsFixture/1.doAddOne
[       OK ] TestPrefix/AddOneTestsFixture/1.doAddOne (0 ms)
[----------] 1 test from TestPrefix/AddOneTestsFixture/1 (0 ms total)

[----------] 1 test from TestPrefix/AddOneTestsFixture/2, where TypeParam = unsigned long
[ RUN      ] TestPrefix/AddOneTestsFixture/2.doAddOne
[       OK ] TestPrefix/AddOneTestsFixture/2.doAddOne (0 ms)
[----------] 1 test from TestPrefix/AddOneTestsFixture/2 (0 ms total)

[----------] Global test environment tear-down
[==========] 3 tests from 3 test suites ran. (0 ms total)
[  PASSED  ] 3 tests.
</code></pre>
<h2 id="full-code-listing"><a class="header" href="#full-code-listing">Full Code Listing</a></h2>
<pre><code class="language-cpp">// addone.hpp
#pragma once
template&lt;typename T&gt;
auto addOne(T t) {
  return t + 1;
}
</code></pre>
<pre><code class="language-cpp">// addone_test.cpp
#include "addone.hpp"
#include &lt;gtest/gtest.h&gt;
#include &lt;tuple&gt;
#include &lt;vector&gt;

template &lt;typename T&gt;
using ParamT = std::vector&lt;std::tuple&lt;T, T&gt;&gt;;

static std::tuple&lt;ParamT&lt;int&gt;, ParamT&lt;long long&gt;, ParamT&lt;std::size_t&gt;&gt; allParams{
  {
    // Test cases for int
    std::make_tuple(1, 2),
    std::make_tuple(5, 6),
    std::make_tuple(9, 10),
  },
  {
    // Test cases for long long
    std::make_tuple(1, 2),
    std::make_tuple(5, 6),
    std::make_tuple(9, 10),
  },
  {
    // Test cases for size_t
    std::make_tuple(1, 2),
    std::make_tuple(5, 6),
    std::make_tuple(9, 10),
  },
};

template &lt;typename T&gt;
struct AddOneTestsFixture : public testing::Test {
  AddOneTestsFixture() : params{std::get&lt;ParamT&lt;T&gt;&gt;(allParams)} {}
  ParamT&lt;T&gt; params;
};

TYPED_TEST_SUITE_P(AddOneTestsFixture);

TYPED_TEST_P(AddOneTestsFixture, doAddOne) {
  for(auto const&amp; [input, expect] : this-&gt;params) {
    ASSERT_EQ(addOne(input), expect)
      &lt;&lt; "addOne(" &lt;&lt; input &lt;&lt; ") != " &lt;&lt; expect &lt;&lt; "!";
  }
}

REGISTER_TYPED_TEST_SUITE_P(AddOneTestsFixture, doAddOne);

using Types = testing::Types&lt;int, long long, std::size_t&gt;;
INSTANTIATE_TYPED_TEST_SUITE_P(TestPrefix, AddOneTestsFixture, Types);
</code></pre>
<p>Makefile used for this post:</p>
<pre><code class="language-make">CFLAGS := -I/usr/local/Cellar/googletest/1.10.0/include
CFLAGS += -L/usr/local/Cellar/googletest/1.10.0/lib -lgtest -lgtest_main
CFLAGS += -lpthread -std=c++17
CXX    =  clang++

all:
	$(CXX) addone_test.cpp $(CFLAGS) -o tests
</code></pre>
<p>{% include footer.html %}</p>
<h2 id="references-1"><a class="header" href="#references-1">References</a></h2>
<ul>
<li><a href="https://www.sandordargo.com/blog/2019/04/24/parameterized-testing-with-gtest">Blog Post: Parameterized testing with GTest</a></li>
<li><a href="https://stackoverflow.com/questions/8507385/google-test-is-there-a-way-to-combine-a-test-which-is-both-type-parameterized-a">SO Question: Is there a way to combine a test which is both type parameterized and value parameterized?</a></li>
<li><a href="https://github.com/google/googletest/blob/master/docs/advanced.md">GTest advanced docs</a></li>
<li><a href="http://llvmweekly.org/issue/376">LLVM Weekly mention</a></li>
<li><a href="https://github.com/llvm/llvm-project/tree/main/flang/unittests">Flang unittests</a></li>
</ul>
<!--
layout: post
title: Spack for Package Development Part 3
permalink: /spack3
cat: cs
-->
<p>Third in this series, this post focuses on <em>leveraging environments for debugging and reproducing errors</em>.</p>
<p>In the <a href="csblog//spack2">previous post about package development with Spack</a>, we discussed environment management with Spack, particularly integration with a private repository.
What are some of the benefits of this, other than onboarding new developers?</p>
<p>As we’ve developed our private spack repository, we’ve also added some spack environments along the way:</p>
<pre><code>
floodsimulationrepo
├── environments
│   ├── jupiter
│   │   └── env.yaml
│   ├── saturn
│   │   └── env.yaml
│   └── osx
│       └── env.yaml
├── packges
│   ├── ipopt
│   │   └── package.py
│   └── floodsimulation
│       └── package.py
└── repo.yaml

</code></pre>
<p>In this post, we’re going to look at using this configuration to reproduce errors and coordinate development and debugging within a large team.</p>
<h3 id="reproducing-finicky-errors"><a class="header" href="#reproducing-finicky-errors">Reproducing Finicky Errors</a></h3>
<p>Bugs relating to interfaces between libraries are sometimes the most difficult to track down.
To use an example from my experience, one of my teams was developing a library that builds on the optimization solver <a href="https://github.com/LLNL/hiop">HiOp</a>, which leverages CUDA, MAGMA, and many BLAS/LAPACK routines.
After developing some functionality tests to ensure our library was performing as expected, we noticed that on some platforms with certain CUDA devices and CUDA versions, our library was failing to converge within our expected tolerance.
For weeks we stepped through debuggers and discussed possibile issues with our codebase and the various libraries we depend on to no avail.</p>
<p>We eventually enlisted the help of collaborators from another laboratory to build and test our codebase under similar conditions on their platforms to ensure they were able to reproduce the bug.
In order to ensure our collaborators were able to accurately reproduce the environments in which we found the bug, we created and distributed spack environments specific to that development snapshot.</p>
<p>Continuing with our <code>FloodSimulation</code> example, let us imagine we found a bug when running with CUDA versions v11.0.104 through 11.1 and HiOp v0.3.0 on our imaginary cluster Jupiter, and would like other teammembers to reproduce the bug on differrent platforms but using the same stack.
We might create an environment file like so:</p>
<pre><code class="language-yaml">
# reproduce-error.yaml
spack:
  specs:
  - floodsimulation ^floodsimulationrepo.petsc ^floodsimulationrepo.hiop
    ^cuda@11.0.104 ^hiop@0.3.0
  view: true
  packages:
    cuda:
      versions: [11.0.104, 11.1.0]
    hiop:
      versions: [0.3.0]

</code></pre>
<p>You can see we’ve established the exact matrix of libraries in which our bug manifests.
We then asked our collaborators to install all versions of this spack environment and attempt to reproduce our bug, with all certainty that they will accurately reproduce the environment.</p>
<p>We also tend to track these environments in our repository like so:</p>
<pre><code>
floodsimulationrepo
├── environments
│   ├── jupiter
│   │   ├── env.yaml
│   │   └── reproduce-error.yaml &lt;---
│   ├── saturn
│   │   └── env.yaml
│   └── osx
│       └── env.yaml
├── packges
│   ├── ipopt
│   │   └── package.py
│   └── floodsimulation
│       └── package.py
└── repo.yaml

</code></pre>
<p>so that in future threads we are able to refer back to the exact configurations which caused bugs in the past.</p>
<p>With this strategy, we are able to maintain a reproducible and consistent software stack with robust coordination between teams.
Another potential use-case of this strategy is to coordinate profiling efforts - each month or so a spack environment which instantiates a development snapshot of the development stack may be distributed to profiling teams.
This way, the profiling team may work on a known working configuration of the software stack to identify performance bottlenecks while the core development team continues developing.</p>
<p><a href="csblog//spack2">Previous in this Series</a></p>
<p><a href="csblog//spack4">Next in this Series</a></p>
<p>{% include footer.html %}</p>
<!--
layout: post
title: Clang Tools for Checking Domain-Specific Errors
permalink: /clang-lambda
cat: cs
-->
<p>Compilers are extremely proficient at catching (and even suggesting fixes for) errors in your code.
What about cases that are not formally errors, but should not exist in your codebase?
This post explores using Clang tools to address this case.</p>
<h2 id="example-use-case"><a class="header" href="#example-use-case">Example Use Case</a></h2>
<p>When using portability libraries such as <a href="https://github.com/LLNL/RAJA/blob/main/docs/sphinx/user_guide/index.rst">RAJA</a> and
<a href="https://github.com/kokkos/kokkos">Kokkos</a>, the capture clauses of lambda statements are extremely important.
When developing the open source optimization engine <a href="https://github.com/LLNL/hiop">HiOp</a> to use the portability library RAJA
in its linear algebra library,
we ran into an issue where a RAJA <code>forall</code> statement would implicitly capture the <code>this</code> pointer in an instance method
which would cause memory access errors when running on a GPU device.</p>
<p>For example, let’s say we have the following Vector class:</p>
<pre><code class="language-cpp">#include &lt;RAJA/RAJA.hpp&gt;

struct Vector {
  using namespace RAJA;
  
  Vector(std::size_t sz, int* data/*=pointer to data on device*/)
    : sz(sz), data(data) {}
    
  void times_constant(int factor) {
  
    forall&lt;cuda_exec&lt;128&gt;&gt;(RangeSegment(0, sz),
      [=] (Index_type i) {
      
        // Here, `data` is captured implicitly via `this` pointer
        // even though `this` does not reside on the GPU
        data[i] *= factor;
      });
      
  }
private:
  std::size_t sz;
  int* data;
};
</code></pre>
<p>As described in the comments above, the data lives on the device, but is accessed via the <code>this</code> pointer which does not.
The solution to this memory access error is to create a local copy of the pointer outside the scope of the RAJA lambda:</p>
<pre><code class="language-cpp">  void times_constant(int factor) {
    auto* local_data = this-&gt;data;
    forall&lt;cuda_exec&lt;128&gt;&gt;(RangeSegment(0, sz),
      [=] (Index_type i) {
        // Here, `data` is no longer captured implicitly via `this` pointer
        local_data[i] *= factor;
      });
  }
</code></pre>
<p>Of course, this is not an error that will be captured by nvcc, hipcc or another host compiler (that I know of).
At first we just examined each kernel in our codebase to ensure we did not use the <code>this</code> pointer implicitly.
Without too much effort however, we were able to develop a small tool to search our codebase for this exact case.</p>
<h2 id="clang-tools"><a class="header" href="#clang-tools">Clang Tools</a></h2>
<p>The first step in creating this tool was to set up a CMake project to link against LLVM libraries.
The directory structure looked like this:</p>
<pre><code>
lambda-checker
├── CMakeLists.txt
└── src
    ├── CMakeLists.txt
    ├── driver.cpp
    └── actions.hpp

</code></pre>
<p>Quite simple, no?
<a href="https://clang.llvm.org/docs/RAVFrontendAction.html">The Clang documentation for frontend actions</a> walks through a similar task.</p>
<p><code>src/driver.cpp</code> contains all of the code to instantiate an AST Action with a clang compiler instance
(and any options you would like to give your driver), while <code>src/actions.hpp</code> contains the actual code to
traverse the AST.</p>
<h3 id="cmake"><a class="header" href="#cmake">CMake</a></h3>
<p>In the top-level CMakeLists.txt and after the usual CMake project preamble, we include relevant clang libraries:</p>
<pre><code class="language-cmake"># top-level CMakeLists.txt

find_package(Clang REQUIRED)

set(CMAKE_MODULE_PATH
  ${CMAKE_MODULE_PATH}
  "${LLVM_CMAKE_DIR}"
  )

include(AddLLVM)

include_directories(${LLVM_INCLUDE_DIRS})
include_directories(${CLANG_INCLUDE_DIRS})
add_definitions(${LLVM_DEFINITIONS})
add_definitions(${CLANG_DEFINITIONS})
</code></pre>
<p>Then, we added our plugin as a target:</p>
<pre><code class="language-cmake"># top-level CMakeLists.txt
add_executable(LambdaChecker src/driver.cpp)
target_link_libraries(LambdaChecker
  PRIVATE
  clangAST
  clangBasic
  clangFrontend
  clangSerialization
  clangTooling
  clangIndex
  clangRewrite
  clangTooling
  )
</code></pre>
<h3 id="actions"><a class="header" href="#actions">Actions</a></h3>
<p>Before we start with <code>src/actions.hpp</code>, let us first discuss strategy.
Finding a potentially dangerous lambda capture requires two predicates for each lambda function found in the AST:</p>
<ol>
<li>Does the lambda capture <code>this</code>?</li>
<li>Does the lambda dereference <code>this</code> to access a pointer or array-like member?</li>
</ol>
<p>I broke each of these steps into its own frontend action.
The first simple searches the AST for a lambda function and checks if it captures <code>this</code>:</p>
<h4 id="find-lambda-that-captures-this"><a class="header" href="#find-lambda-that-captures-this">Find Lambda that Captures <code>this</code></a></h4>
<pre><code class="language-cpp">// src/actions.hpp
class FindLambdaCaptureThis
  : public RecursiveASTVisitor&lt;FindLambdaCaptureThis&gt; {
public:
  explicit FindLambdaCaptureThis(ASTContext *Context)
    : Context(Context), MemberVisitor(Context) {}

  bool VisitLambdaExpr(LambdaExpr *Expr) {
    bool FoundThis = false;
    for (auto it = Expr-&gt;capture_begin(); it != Expr-&gt;capture_end(); it++) {
      if (it-&gt;capturesThis()) {
        FoundThis = true;
        break;
      }
    }

    /* If `this` is not captured, we don't care about it. */
    if (!FoundThis)
      return true;

    const CompoundStmt* LambdaBody = Expr-&gt;getBody();
    if (LambdaBody-&gt;body_empty())
      return true;

    for(auto Stmt : LambdaBody-&gt;body()) {
      MemberVisitor.Parent = Expr;
      MemberVisitor.TraverseStmt(Stmt);
    }

    return true;
  }

private:
  ASTContext *Context;
  FindLambdaCapturedFields MemberVisitor; // we'll come back to this
};
</code></pre>
<p>You may find that we define the function <code>VisitLambdaExpr</code> - because this is a special name registered within clang,
the compiler instance will run this function on any AST node that matches it: every lambda expression.</p>
<p>Walking through the class above, we first check if the lambda expression captures <code>this</code>:</p>
<pre><code class="language-cpp">    bool FoundThis = false;
    for (auto it = Expr-&gt;capture_begin(); it != Expr-&gt;capture_end(); it++) {
      if (it-&gt;capturesThis()) {
        FoundThis = true;
        break;
      }
    }
</code></pre>
<p>If the lambda does not capture <code>this</code>, we can continue traversing the AST:</p>
<pre><code class="language-cpp">    if (!FoundThis)
      return true;
</code></pre>
<p>Then we make another check to ensure the lambda body is not empty:</p>
<pre><code class="language-cpp">    const CompoundStmt* LambdaBody = Expr-&gt;getBody();
    if (LambdaBody-&gt;body_empty())
      return true;
</code></pre>
<p>If all the above conditions are met, we traverse the body of the lambda to find any pointer- or array-like
member variables accessed in the lambda:</p>
<pre><code class="language-cpp">    for(auto Stmt : LambdaBody-&gt;body()) {
      MemberVisitor.Parent = Expr;
      MemberVisitor.TraverseStmt(Stmt);
    }
</code></pre>
<p>Now that we have a higher-level AST traversal class to find lambdas that capture <code>this</code>, we can look at our next
AST traversal class which checks for problematic uses of member variables.
The member visitor will accept <em>all forms of expressions</em>, so we only run that visitor on the statements
in the body of the lambda.
You may also notice that we set the <code>Parent</code> field of our <code>MemberVisitor</code> - this is to improve the quality
of the diagnostics we are able to emit. We’ll expand on this later.</p>
<h4 id="member-visitor"><a class="header" href="#member-visitor">Member Visitor</a></h4>
<p>This AST visitor class ensures no pointer- or array-like member variables are accessed in the lambda</p>
<pre><code class="language-cpp">struct FindLambdaCapturedFields
  : public RecursiveASTVisitor&lt;FindLambdaCapturedFields&gt; {
public:
  explicit FindLambdaCapturedFields(ASTContext *Context)
    : Context(Context) {}

  bool VisitMemberExpr(MemberExpr *Expr) {
    auto MemberType = Expr-&gt;getType();

    /* Problematic use of member variable! Time to generate diagnostic
     * information. */
    if (MemberType-&gt;isArrayType() || MemberType-&gt;isPointerType()) {

      /* Report diagnostic information */
      clang::DiagnosticsEngine &amp;DE = Context-&gt;getDiagnostics();

      /* Error message describing the issue */
      auto ID = DE.getCustomDiagID(
          clang::DiagnosticsEngine::Error,
          "Found lambda capturing pointer-like member variable here.");
      DE.Report(Expr-&gt;getBeginLoc(), ID);

      /* Remark indicating which member variable triggered the error */
      ID = DE.getCustomDiagID(clang::DiagnosticsEngine::Note,
          "Member variable declared here:");
      DE.Report(Expr-&gt;getMemberDecl()-&gt;getBeginLoc(), ID);

      /* Remark with suggested change to mitigate the issue */
      ID = DE.getCustomDiagID(clang::DiagnosticsEngine::Remark,
          "Consider creating a local copy of the member variable in local scope"
          " just outside the lambda capture.");
      DE.Report(Parent-&gt;getBeginLoc(), ID);
    }
    return true;
  }

  ASTContext *Context;
  LambdaExpr *Parent=nullptr;
};
</code></pre>
<p>First, we check the type of the expression inside the lambda:</p>
<pre><code class="language-cpp">    auto MemberType = Expr-&gt;getType();
    /* Problematic use of member variable! Time to generate diagnostic
     * information. */
    if (MemberType-&gt;isArrayType() || MemberType-&gt;isPointerType()) {
</code></pre>
<p>If we enter this conditional, we’ve found a potential problem! Now what to do?</p>
<h4 id="diagnostics"><a class="header" href="#diagnostics">Diagnostics</a></h4>
<p>Clang diagnostics are again a very rich library which won’t be fully flushed out here - please
consult <a href="https://clang.llvm.org/doxygen/classclang_1_1DiagnosticsEngine.html">the documentation for the Clang Diagnostics Engine</a>.</p>
<p>First order of business in emmitting diagnositcs is to get a handle for a diagnositcs engine capable
of printing helpful messages to the user of our tool.</p>
<pre><code class="language-cpp">      clang::DiagnosticsEngine &amp;DE = Context-&gt;getDiagnostics();
</code></pre>
<p>Let’s think for a moment about the sort of diagnostic we would like to emit.
I think we should report three things to the user if a lambda expression meets our
critera for an error:</p>
<ol>
<li>Location in the lambda where the member variable is used via <code>this</code> pointer</li>
<li>Location of that member’s declaration</li>
<li>Suggestion for fixing the issue</li>
</ol>
<p>Let’s address these one-by-one: first, report the location where the member variable is
potentially erroneously used.</p>
<pre><code class="language-cpp">      auto ID = DE.getCustomDiagID(
          clang::DiagnosticsEngine::Error,
          "Found lambda capturing pointer-like member variable here.");
      DE.Report(Expr-&gt;getBeginLoc(), ID);
</code></pre>
<p>Then, where the member variable was declared:</p>
<pre><code class="language-cpp">      /* Remark indicating which member variable triggered the error */
      ID = DE.getCustomDiagID(clang::DiagnosticsEngine::Note,
          "Member variable declared here:");
      DE.Report(Expr-&gt;getMemberDecl()-&gt;getBeginLoc(), ID);
</code></pre>
<p>Finally, a suggestion for fixing the error:</p>
<pre><code class="language-cpp">      /* Remark with suggested change to mitigate the issue */
      ID = DE.getCustomDiagID(clang::DiagnosticsEngine::Remark,
          "Consider creating a local copy of the member variable in local scope"
          " just outside the lambda capture.");
      DE.Report(Parent-&gt;getBeginLoc(), ID);
</code></pre>
<p>At this point, we’re essentially done - all we need is a bit of boilerplate code to
connect our AST consumer classes up to a compiler instance:</p>
<pre><code class="language-cpp">class LambdaCaptureCheckerConsumer : public clang::ASTConsumer {
public:
  explicit LambdaCaptureCheckerConsumer(ASTContext *Context)
    : Visitor(Context) {}
  explicit LambdaCaptureCheckerConsumer(CompilerInstance&amp; CI)
    : Visitor(&amp;CI.getASTContext()) {}

  virtual void HandleTranslationUnit(clang::ASTContext &amp;Context) {
    Visitor.TraverseDecl(Context.getTranslationUnitDecl());
  }
private:
  FindLambdaCaptureThis Visitor;
};
</code></pre>
<p>Now we’re done with the file <code>src/actions.hpp</code>.</p>
<h3 id="driver"><a class="header" href="#driver">Driver</a></h3>
<p>In <code>src/driver.cpp</code> we create an AST frontend action to create and use the compiler action we defined in <code>src/actions.hpp</code>:</p>
<pre><code class="language-cpp">// src/driver.cpp
class LambdaCaptureCheckerAction : public clang::ASTFrontendAction {
public:
  virtual std::unique_ptr&lt;clang::ASTConsumer&gt; CreateASTConsumer(
    clang::CompilerInstance &amp;Compiler, llvm::StringRef InFile) {
    return std::unique_ptr&lt;clang::ASTConsumer&gt;(
        new LambdaCaptureCheckerConsumer(&amp;Compiler.getASTContext()));
  }
};
</code></pre>
<p>Here I omit any command line options.
<a href="https://llvm.org/docs/CommandLine.html">The documentation on this topic</a> is rich,
so if you would like to add command line options you shouldn’t have too much trouble.</p>
<pre><code class="language-cpp">// src/driver.cpp
static cl::OptionCategory LambdaCaptureCheckerCategory("LambdaChecker options");

int main(int argc, const char **argv) {
  CommonOptionsParser Op(argc, argv, LambdaCaptureCheckerCategory);

  /* Create a new Clang Tool instance (a LibTooling environment). */
  ClangTool Tool(Op.getCompilations(), Op.getSourcePathList());

  return Tool.run(newFrontendActionFactory&lt;LambdaCaptureCheckerAction&gt;().get());
}
</code></pre>
<h3 id="running"><a class="header" href="#running">Running</a></h3>
<p>At this point, you may also generate a clang plugin library to use our AST actions
which can be loaded via compiler invocation, however I opted to stick with a standalone executable.</p>
<p>In order to fully test our AST action, I also created a subdirectory for examples,
giving us the following directory structure:</p>
<pre><code>
lambda-checker
├── CMakeLists.txt
├── src
│   ├── CMakeLists.txt
│   ├── driver.cpp
│   └── actions.hpp
└── test
    └── capture-test.cpp

</code></pre>
<p>Where <code>capture-test.cpp</code> contains:</p>
<pre><code class="language-cpp">// capture-test.cpp
struct CaptureTest {

  /* Should not capture */
  int *i;

  /* Should not capture */
  int j[1];

  /* OK to capture */
  int k=0;

  /* Method which implicitly captures `this` pointer and modifies member
   * variable `i`. This is problematic when using portability libraries, as
   * member variables may not reside on the host. */
  void methodUsingBadCapturePointer() {
    auto throwaway = [=] () {
      *i = 1;
    };
  }

  /* Raw arrays should not be used either. */
  void methodUsingBadCaptureArray() {
    auto throwaway = [=] () {
      j[0] = 1;
    };
  }

  /* The preferred method to mitigate the issue outlined above is to create a
   * local copy of the pointer and modify the underlying data through the copy.
   */
  void methodUsingGoodCapture() {
    int* localCopy = i;
    auto throwaway = [=] () {
      *localCopy += 1;
    };
  }

  /* Methods which capture `this` variables which are not pointers should not
   * cause an issue. */
  void methodNotCapturingPointer() {
    auto throwaway = [=] () {
      k++;
    };
  }
};

int main() { return 0; }
</code></pre>
<p>I added this as a CMake target such that the compile commands database would be generated for
our test case (<a href="https://clang.llvm.org/docs/HowToSetupToolingForLLVM.html">additional documentation for compile-commands database</a>).
To do this, add the following to the top-level <code>CMakeLists.txt</code>:</p>
<pre><code class="language-cmake"># top-level CMakeLists.txt
set(CMAKE_EXPORT_COMPILE_COMMANDS ON)
add_executable(dummy-target test/capture-test.cpp)
</code></pre>
<p>This way, we are able to run our plugin driver directly on our test case.</p>
<pre><code class="language-console">
$ cd lambda-capture
$ mkdir build
$ cd build
$ cmake .. &amp;&amp; make

$ # At this point, the file `compile_commands.json` should exist in your CWD
$ # and you should be able to run the driver on our test case:
$ ./src/LambdaChecker ../test/capture-test.cpp
/path/to/lambda-capture/test/capture.cpp:17:8: error: Found lambda capturing pointer-like member variable here.

      *i = 1;
       ^
/path/to/lambda-capture/test/capture.cpp:4:3: note: Member variable declared here:
  int *i;
  ^
/path/to/lambda-capture/test/capture.cpp:16:22: remark: Consider creating a local copy of the member variable in local scope
just outside the lambda capture.
    auto throwaway = [=] () {

</code></pre>
<p>As you can see, our tool seems to be correctly identifying our domain-specific error!
After developing this tool and running it over all of our codebases which make heavy use of portability
libraries such as RAJA and Kokkos, we are confident that we have purged this error from our
codebase.</p>
<p>Hopefully this demonstration helps your team weed out nasty errors like these from your codebase as well.</p>
<p>The full code listings can be found in the <a href="https://github.com/ashermancinelli/lambda-capture-checker">repository linked here</a>.
The code snippets used here for example purposes will not map perfectly to the current repository, but should
serve as a concrete starting point.</p>
<p>{% include footer.html %}</p>
<h2 id="references-2"><a class="header" href="#references-2">References</a></h2>
<ol>
<li><a href="https://github.com/ashermancinelli/lambda-capture-checker">Lambda Capture tool</a></li>
<li><a href="https://github.com/LLNL/RAJA/blob/main/docs/sphinx/user_guide/index.rst">RAJA</a></li>
<li><a href="https://github.com/kokkos/kokkos">Kokkos</a></li>
<li><a href="https://clang.llvm.org/docs/JSONCompilationDatabase.html">Clang compile commands database spec</a></li>
<li><a href="https://clang.llvm.org/docs/HowToSetupToolingForLLVM.html">Clang compile commands tutorial</a></li>
<li><a href="https://github.com/LLNL/hiop">HiOp</a></li>
<li><a href="https://clang.llvm.org/docs/RAVFrontendAction.html">Clang AST Visitor documentation</a></li>
<li><a href="https://www.youtube.com/watch?v=E6i8jmiy8MY&amp;ab_channel=CppNow">Peter Goldsborough’s C++Now talk on Clang/LLVM tools</a></li>
</ol>
<!--
layout: post
title: Spack for Package Development Part 2
permalink: /spack2
cat: cs
-->
<p>Second in this series, this post focuses on <em>getting stared with environments</em>.</p>
<p>In the <a href="csblog//spack1">previous post about package development with Spack</a>, we discussed the following points:</p>
<ul>
<li>Creating a private spack repository</li>
<li>Maintaining private packages</li>
<li>Maintaining forks of upstream packages</li>
</ul>
<p>In the following post(s), we’ll discuss the following in greater detail:</p>
<ul>
<li>Managing environments for reproducibility</li>
<li>Using environments for development</li>
<li>Using environments for continuous integration</li>
<li>Managing configurations for distributed and multi-platform development</li>
</ul>
<h2 id="managing-environments"><a class="header" href="#managing-environments">Managing Environments</a></h2>
<p>One of the most useful features of Spack is it’s support for <a href="https://spack.readthedocs.io/en/latest/environments.html">environments</a>.
An environment is a file describing a package or set of packages you wish to install, along with all of the spack configuration options you wish to use (see documentation linked above for more information).
In the previous post, we used an example package <code>FloodSimulation</code> and a corresponding repository - we’ll continue with this example here.</p>
<p>When managing a complex and mutable set of dependencies, reproducibility is <em>extremely</em> important.
Spack environments allow the development team to document <em><strong>every single option</strong></em> they used to install a particular package or set of packages.
For example, let’s say <code>FloodSimulation</code> has a few more dependencies: <a href="https://github.com/LLNL/hiop">HiOp</a>, <a href="https://icl.cs.utk.edu/projectsfiles/magma/doxygen/index.html">MAGMA</a>, CUDA &gt; 10.1, and <a href="https://gitlab.com/petsc/petsc">PETSc</a>.
Each of these packages has many configuration and installation options, and you may even want to use versions of these packages installed by your system administrator.</p>
<p>Our <code>FloodSimulation</code> package may have a <code>package.py</code> file that looks like this:</p>
<pre><code class="language-python">
from spack import *
import os

class FloodSimulation(CMakePackage):
    homepage = "https://github.com/your-username/flood-simulation"
    git = "https://github.com/your-username/flood-simulation.git"
    version('1.0.0', tag='v1.0.0')
    depends_on('petsc')
    depends_on('hiop')
    depends_on('cuda@10.1:', when='+cuda')
    depends_on('magma', when='+cuda')

    def cmake_args(self):
        args = []
        if '+cuda' in self.spec:
            args.append('-DFLOODSIMULATION_ENABLE_CUDA=ON')
        return args

</code></pre>
<p>Let us also assume you <em>need</em> to install with your fork of the PETSc and HiOp spack packages, and that you’d like to use the MAGMA and CUDA installations provided by your system administrator or package manager.
In this case, you might have an environment file like this:</p>
<pre><code class="language-yaml">
# example-env.yaml
spack:
  specs:
  - floodsimulation ^floodsimulationrepo.petsc ^floodsimulationrepo.hiop
  view: true
  packages:
    magma:
      externals:
      - spec: magma@2.5.4
        prefix: /path/to/magma
      buildable: false
    cuda:
      externals:
      - spec: cuda@10.2.89
        prefix: /path/to/cuda
      buildable: false

</code></pre>
<p>Constructing your spack environment from this file is easy as:</p>
<pre><code>
$ spack env create my-environment ./example-env.yaml
$ spack install

$ # To locate your new installation:
$ spack location -i floodsimulation

</code></pre>
<p>This way, if another developer needs to reproduce your development environment, you may distribute the environment file to perfectly recreate your installation.
I reccommend tracking your environments in version control along with the rest of your private spack repository with the following example directory layout:</p>
<pre><code>
floodsimulationrepo
├── environments
│   └── example-env.yaml
├── packges
│   ├── ipopt
│   │   └── package.py
│   └── floodsimulation
│       └── package.py
└── repo.yaml

</code></pre>
<h2 id="using-spack-environments"><a class="header" href="#using-spack-environments">Using Spack Environments</a></h2>
<p>In my opinion, the three most significant use cases for spack environments are:</p>
<ol>
<li>Reproducible environments for development</li>
<li>Reproducing finicky errors</li>
<li>Continuous integration/testing</li>
</ol>
<h3 id="reproducible-environments-for-development"><a class="header" href="#reproducible-environments-for-development">Reproducible Environments for Development</a></h3>
<p>With a complex codebase, onboarding often requires significant resources for new developers.
Gettings started with a new codebase can be challanging, especially when building the software stack in the first place can take up to several days.
I have found distributing a spack environment which is known to instantiate your software stack on a particular development machine to be a mostly frictionless method of getting new developers started on a codebase.
With the example environment file above, we specify instructions to instatiate the <code>FloodSimulation</code> software stack on a particular machine with a couple pre-installed packages.
If you are developing on many platforms and you need developers up and running on all platforms with a short turnaround time, distributing spack environments will likely be a suitable solution.</p>
<p>Extending the example above, the following directory structure is a suitable way to maintain spack environments to instatiate the <code>FloodSimulation</code> stack on multiple platforms.
Let’s assume you want developers up and running on two clusters, Jupiter and Saturn, as well as OSX for local development:</p>
<pre><code>
floodsimulationrepo
├── environments
│   ├── jupiter
│   │   └── env.yaml
│   ├── saturn
│   │   └── env.yaml
│   └── osx
│       └── env.yaml
├── packges
│   ├── ipopt
│   │   └── package.py
│   └── floodsimulation
│       └── package.py
└── repo.yaml

</code></pre>
<p>In this way, you may simply instruct a new developer to run the following to get started with local development on a Mac:</p>
<pre><code class="language-console">
$ # in the directory `floodsimulationrepo`:
$ spack env create local-development ./environments/osx/env.yaml
$ spack install

$ # Any time the developer logs in to develop locally:
$ spack env activate local-development

</code></pre>
<p>And when they need to get started developing on the institutional cluster Jupiter:</p>
<pre><code class="language-console">
$ spack env create remote-development ./environments/jupiter/env.yaml
$ spack install

</code></pre>
<h4 id="an-aside-on-spack-views"><a class="header" href="#an-aside-on-spack-views">An Aside on Spack Views</a></h4>
<p>If you didn’t notice earlier, our environment file contains the line:</p>
<pre><code class="language-yaml">  view: true
</code></pre>
<p>This means that when you activate your spack environment (<code>spack env activate &lt;environment name&gt;</code>), spack will use a “view” by defaul.
A view is a single installation prefix into which all packages installed via the environment are symbolically linked.</p>
<p>By default, spack views are installed to:</p>
<pre><code>$SPACK_ROOT/var/spack/environments/&lt;environment name&gt;/.spack-env/
</code></pre>
<p>and the environment file is installed to</p>
<pre><code>$SPACK_ROOT/var/spack/environments/&lt;environment name&gt;/spack.yaml
</code></pre>
<p>If our new developer activates her spack environment like so:</p>
<pre><code class="language-console">
$ spack env activate local-development
$ ls $SPACK_ROOT/var/spack/environments/local-development/.spack-env/lib
libmagma.so libhiop.so ...

</code></pre>
<p>she will have access to all of her newly installed libraries in a single installation prefix, automatically added to her <code>LD_LIBRARY_PATH</code> and <code>DYLD_LIBRARY_PATH</code>.</p>
<p><a href="https://spack.readthedocs.io/en/latest/command_index.html?highlight=view#spack-view">The Spack command reference on views</a> contains further information on this topic.</p>
<h4 id="back-to-using-environments"><a class="header" href="#back-to-using-environments">Back to Using Environments</a></h4>
<p>Now that we have a feel for creating, using, and distributing spack environments, how do we develop with them?</p>
<p>As we saw in the aside on views above, activating a spack environment with the view option enabled (which is the default) adds the view to the user’s path, library path, etc.
Assuming <code>FloodSimulation</code> is a CMake project (as we specified in the example spack package above) and we have written clean enough CMake to find external libraries, the workflow for building <code>FloodSimulation</code> should be relatively straightforward:</p>
<pre><code class="language-console">
$ git clone https://github.com/your-username/flood-simulation.git
$ mkdir build install
$ cd build

$ # At this step, cmake should be able to find all of your libraries in the
$ # spack view:
$ cmake ../flood-simulation -DCMAKE_INSTALL_PREFIX=$PWD/../insatll

$ # If some libraries are not found, simply run `spack location -i package`
$ # to find the prefix for the package, and add it manually with ccmake:

$ # Assuming magma is the package cmake failed to find, run this command and
$ # copy the path:
$ spack location -i magma

$ # Manually add the path to the magma installtion to cmake:
$ ccmake

$ make install

</code></pre>
<p>From here, developers may continue the edit-build-test cycle for <code>FloodSimulation</code>, knowing they are using the correct set of dependencies.</p>
<p>Read on for discussion on additional uses for environments you should consider incorporating into your workflow.</p>
<p><a href="csblog//spack1">Previous in this Series</a></p>
<p><a href="csblog//spack3">Next in this Series</a></p>
<p>{% include footer.html %}</p>
<!--
layout: post
title: Spack for Package Development Part 1
permalink: /spack1
cat: cs
-->
<p><a href="https://spack.readthedocs.io/en/latest/">Spack</a> is typically used for package deployment, however this post will be about package <em>development</em> with Spack.
First in this series, this post focuses on <em>motivation and introduction</em>.</p>
<h2 id="intro"><a class="header" href="#intro">Intro</a></h2>
<p>Most upstream Spack packages are quite stable.
In my experience, the majority of Spack packages are based on packages that have already existed for a long time, and a member of the community created a Spack package to install it (eg OpenMPI).
In this case, the options and versions for the package are probably set-in-stone.
There are likely very few PRs submitted for these packages, and users can rely on the dependencies and installation process staying mostly the same.</p>
<p>For a package under extremely heavy development however, this is not the case.
To use a package manager <em>and</em> iterate rapidly on a package, I think there are roughly 3 criteria for that package manager:</p>
<ol>
<li>Adding a new <em>dependency</em> should be easy and fast</li>
<li>Adding new <em>versions</em> should be easy and fast</li>
<li>Adding new <em>options</em> should be easy and fast</li>
</ol>
<p>In my opinion, using the typical Spack workflow of submitting a pull request to the upstream Spack repository for every meaningful change meets none of these critera.</p>
<h2 id="configuring-spack-repositories"><a class="header" href="#configuring-spack-repositories">Configuring Spack Repositories</a></h2>
<p>An alternative strategy is to use Spack’s support for external repositories.
A repository is simply a directory which contains a <code>repo.yaml</code> file and a <code>packages/</code> directory
under which Spack packages reside.</p>
<p>For example, creating the file</p>
<pre><code class="language-yaml"># repo.yaml
repo:
  namespace: examplerepo
</code></pre>
<p>in a directory with a <code>packages</code> subdirectory like so:</p>
<pre><code>examplerepo
├── packges
│   └── examplepackage
│       └── package.py
└── repo.yaml
</code></pre>
<p>is a valid repository.</p>
<p>Running <code>spack repo add .</code> in this directory will add the path to that repository to your Spack configuration:</p>
<pre><code class="language-yaml"># ~/.spack/repos.yaml
repos:
  - $spack/var/spack/repos/builtin
  - /path/to/examplerepo
</code></pre>
<p>After configuring your example repository, you are able to install packages from it directly!
If your package conflicts with a builtin package, you may install it using the namespace set in <code>examplerepo/repo.yaml</code>:</p>
<pre><code class="language-console">
$ # If examplepackage is unique:
$ spack install examplepackage

$ # If examplepackage conflicts with a builtin package:
$ spack install examplerepo.examplepackage

</code></pre>
<h2 id="3rd-party-packages-in-your-spack-repo"><a class="header" href="#3rd-party-packages-in-your-spack-repo">3rd Party Packages in Your Spack Repo</a></h2>
<p>The most common case that I have found of a package conflicting with a builtin is when one of your packages relies on a fork of an upstream package, so you maintain a modified version of the upstream package (in <code>examplerepo/packages/forked-package/package.py</code>, for example).
This allows developers to iterate quickly and modify dependencies without attempting to maintain a fork of the entire spack repository.</p>
<p>For example, let’s say you’re developing a package FloodSimulation which relies on OpenMPI and Ipopt.
As you develop your software, you realize the Ipopt Spack package doesn’t expose all of Ipopt’s configuration options, and you need to make rather significant edits to the Ipopt Spack package.
You could go through the pull request process upstream, however if you have many similar edits to many other packages, you may want to maintain an Ipopt fork in your spack repository:</p>
<pre><code>floodsimulationrepo
├── packges
│   ├── ipopt
│   │   └── package.py
│   └── floodsimulation
│       └── package.py
└── repo.yaml
</code></pre>
<p>You may then install FloodSimulation with your fork of Ipopt like so:</p>
<pre><code class="language-console">
$ spack install floodsimulation ^floodsimulationrepo.ipopt

</code></pre>
<p>If you track your private spack repository with source control, it is quite easy to maintain your small package repository while your key packages are under heavy development.
Each release of your package may serve as a time to submit all of your modifications to forked packages as well as the spack package descriptions to upstream spack such that end users are able to fully take advantage of your configuration.</p>
<p>This strategy alone has the potential to save a significant amount of developer time when heavily developing a package.
The next post will go further into managing environments and multi-platform configurations.</p>
<p><a href="csblog//spack2">Next in this Series</a></p>
<p>{% include footer.html %}</p>
<!--
layout: post
title: A Look at std::mdspan
permalink: /std-mdspan-tensors
cat: cs
-->
<p>New library feature coming to C++23</p>
<h2 id="tensors"><a class="header" href="#tensors">Tensors</a></h2>
<p>A YouTuber by the name of <a href="https://www.youtube.com/c/ContextFree/videos">Context Free</a> posted a few videos about tensors in various programming languages, including C++ (<a href="https://youtu.be/WbpbEilgQBc">first video</a>, <a href="https://youtu.be/ICxxKeE4GuA">second video</a>).
I loved these videos, but I noticed ContextFree failed to mention <code>std::mdspan</code>, a new library feature coming to C++23.</p>
<h2 id="stdmdspan"><a class="header" href="#stdmdspan"><code>std::mdspan</code></a></h2>
<p><code>std::mdspan</code> (or <code>std::experimental::mdspan</code> until the proposal is accepted-I’ll be using <code>stdex::mdspan</code>, as <code>stdex</code> is a common alias for <code>std::experimental</code>) is an alias for a more complex type, but at it’s core it is a pointer plus some number of <em>extents</em>.
<em>Extents</em> are either sizes of a given dimension on an <code>mdspan</code>, or the sentinal <code>std::dynamic_extent</code>, which indicates the extent is <em>dynamic</em>, and doesn’t have to be supplied at compile-time.
These extents and the sentinal <code>dynamic_extent</code> can be mixed and matched.
This powerful capability allows users to work with data as if it were a complex matrix structure while the underlying data remain linear.</p>
<p>For example, given raw data, an <code>mdspan</code> may be constructed and passed to some library that expects a matrix with rank 3:</p>
<pre><code class="language-cpp">// https://godbolt.org/z/eWKev9nas
template&lt;T&gt;
using mat_3d = std::mdspan&lt;
                 T,
                 std::extents&lt;
                     std::dynamic_extent
                   , std::dynamic_extent
                   , std::dynamic_extent
                 &gt;
               &gt;;

template&lt;typename T&gt; void work(mat_3d&lt;T&gt; mat);

int main() {
  int raw[500] = {};

  work(stdex::mdspan(raw, 100, 5, 1));
  work(stdex::mdspan(raw, 4, 5, 5));
  work(stdex::mdspan(raw, 10, 1, 50));
}
</code></pre>
<p>Note that we will have to be more careful if we mix-and-match compile-time and run-time extents.</p>
<pre><code class="language-cpp">// https://godbolt.org/z/Phr1vh9zs
template&lt;typename T&gt; using mat_3d = stdex::mdspan&lt;
  T,
  stdex::extents&lt;
    stdex::dynamic_extent,
    3,
    stdex::dynamic_extent
  &gt;
&gt;;

template&lt;typename T&gt; void work(mat_3d&lt;T&gt; mat) {}

int main() {
  int raw[27] = {};
  work(mat_3d&lt;int&gt;(raw, 3, 3, 3));
  work(mat_3d&lt;int&gt;(raw, 9, 3, 0));
  // work(mat_3d&lt;int&gt;(raw, 3, 0, 9)) will fail bc extents don't match!
  return 0;
}
</code></pre>
<p>After supplying a dummy implementation of <code>work</code> to print the shape, we get</p>
<pre><code class="language-console">mat has shape [100, 5, 1]
mat has shape [4, 5, 5]
mat has shape [10, 1, 50]
</code></pre>
<p>In either case, the underlying data is the same, though it’s <em>viewed</em> differently in each of the invocations of <code>work</code>.
It’s no coincidence that <em>view</em> seems like a natural name for <code>mdspan</code> - <code>mdspan</code> was developed by the authors of the portable execution library Kokkos and inspired by the <code>Kokkos::View</code> type.</p>
<h2 id="subspans"><a class="header" href="#subspans">Subspans</a></h2>
<p>Just like <code>std::span</code>, <code>mdspan</code> has support for taking subspans of a given span.
This is more complicated with <code>mdspan</code> however, due to <code>mdspan</code>’s variadic extents.</p>
<p>There are three ways to take a slice of an <code>mdspan</code>:</p>
<ol>
<li>An integer index into the respective dimension</li>
<li>A <code>std::tuple</code> or <code>std::pair</code> begin/end pair of indices</li>
<li>The special value <code>std::full_extent</code> indicating all elements of the dimension should be selected in the subspan</li>
</ol>
<p>For example:</p>
<pre><code class="language-cpp">// https://godbolt.org/z/Wrr4dhEs8
int main() {
  int raw[27] = {};
  std::iota(raw, raw+27, 0);

  // full will have shape [3,3,3]
  auto full = stdex::mdspan(raw, 3, 3, 3);

  // sub will have shape [1,3] and values [18,19,20]
  auto sub = stdex::submdspan(full, 2, std::pair{0, 1}, stdex::full_extent);
}
</code></pre>
<p>Note that using a single value as an extent passed to <code>submdspan</code> squashes the dimension to 0, while passing a <code>pair</code> or <code>tuple</code> will keep the dimension around, even if that dimension is 1.
<code>pair</code>/<code>tuple</code> do not effect rank, while passing an index as an extent does.</p>
<p>I hope this article was enough to get you interested in <code>mdspan</code> and the future of C++!
Make sure to check out Daisy Hollman’s appearance on CppCast, Context Free’s YouTube channel, and the reference implementation of C++23’s <code>std::mdspan</code>.</p>
<p>{% include footer.html %}</p>
<h2 id="links"><a class="header" href="#links">Links</a></h2>
<ul>
<li><a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2021/p0009r14.html"><code>mdspan</code> paper, revision 14</a></li>
<li>Godbolt links:
<ul>
<li><a href="https://godbolt.org/z/eWKev9nas">Example 1</a></li>
<li><a href="https://godbolt.org/z/Phr1vh9zs">Example 2</a></li>
<li><a href="https://godbolt.org/z/Wrr4dhEs8">Example 3</a></li>
</ul>
</li>
<li><a href="https://www.youtube.com/c/ContextFree/videos">ContextFree’s YouTube channel</a></li>
<li><a href="https://github.com/kokkos/mdspan">Kokkos mdspan implementation</a></li>
<li><a href="https://github.com/kokkos/mdspan/wiki/A-Gentle-Introduction-to-mdspan">Intro to mdspan wiki article</a></li>
<li><a href="https://godbolt.org/z/KMT3G9Ese"><code>std::mdspan</code> on compiler explorer</a></li>
<li><a href="https://cppcast.com/too-cute-mdspan/">CppCast episode with Daisy Hollman</a></li>
</ul>
<!--
layout: post
title: Using the Most Popular Programming Languages of the '60s
permalink: /pop-langs-1960s
cat: cs
-->
<p>We use the 6 most popular programming languages of the 1960’s to solve a leetcode problem!</p>
<p>Most of these languages have changed a lot since the 1960s, so the way I’m using these languages won’t be quite the same as they were used back then.
For example, I couldn’t find a way to compile and/or run an ALGOL50 program, so I’ll have to use Algol68, a later standard of the language.
Similarly, the first APLs were intended for use on a blackboard, and the first actual implementations were all proprietary.
Many of the languages were originally written on punchcards and physically inserted into a punchard reader, and I don’t have access to any of that.
For the most part, I made some attempt to use an older version of each language to get a better feel for what it would be like to use the language back in the day.</p>
<p>I’ll be looking at the languages in ascending order based on their popularity in 1965.</p>
<p>Along with my solution for each language, I’ll give a little bit of history and a quote from Edsger Dijkstra (whether I agree with it or not :smile:).
His scathing remarks about almost every language on this list were too good to leave out.</p>
<a target="_blank" href="https://github.com/ashermancinelli/algorithm-testbed">
All these solutions and the build system needed to compile the examples can be found in this repository.
</a>
***NOTE: The repo linked above has been temporarily made private due to intellectual property questions and will be restored as soon as possible.***
<a href="https://youtu.be/lYfCNa9coC4" target="_blank">
See the youtube video version of this content linked here.
</a>
<h2 id="problem"><a class="header" href="#problem">Problem</a></h2>
<p><a href="https://leetcode.com/problems/find-peak-element/">Find the element that is greater than both neighbors, aka the peak element.
</a></p>
<h4 id="example-1"><a class="header" href="#example-1">Example 1:</a></h4>
<p><strong>Input</strong>: <code>nums = [1,2,3,1]</code></p>
<p><strong>Output</strong>: <code>2</code></p>
<p><strong>Explanation</strong>: <code>3</code> is a peak element and your function should return the index number <code>2</code>.</p>
<h4 id="example-2"><a class="header" href="#example-2">Example 2:</a></h4>
<p><strong>Input</strong>: <code>nums = [1,2,1,3,5,6,4]</code></p>
<p><strong>Output</strong>: <code>1</code> or <code>5</code></p>
<p><strong>Explanation</strong>: Your function can return either index number <code>1</code> where the peak element is <code>2</code>, or index number <code>5</code> where the peak element is <code>6</code>.</p>
<h4 id="constraints"><a class="header" href="#constraints">Constraints:</a></h4>
<ul>
<li><code>1 &lt;= nums.length &lt;= 1000</code></li>
<li><code>-231 &lt;= nums[i] &lt;= 231 - 1</code></li>
<li><code>nums[i] != nums[i + 1]</code> for all valid <code>i</code>.</li>
</ul>
<h2 id="content"><a class="header" href="#content">Content</a></h2>
<p>Here’s how the various languages stack up.
We’ll start at the bottom with APL and work our way up to Fortran.</p>
<ol>
<li><a href="csblog/2021-10-24-Popular-Languages-1965.html#fortran">Fortran</a></li>
<li><a href="csblog/2021-10-24-Popular-Languages-1965.html#cobol">COBOL</a></li>
<li><a href="csblog/2021-10-24-Popular-Languages-1965.html#algol">ALGOL</a></li>
<li><a href="csblog/2021-10-24-Popular-Languages-1965.html#basic">BASIC</a></li>
<li><a href="csblog/2021-10-24-Popular-Languages-1965.html#lisp">Lisp</a></li>
<li><a href="csblog/2021-10-24-Popular-Languages-1965.html#apl">APL</a></li>
</ol>
<h3 id="apl"><a class="header" href="#apl"><a href="csblog/2021-10-24-Popular-Languages-1965.html#content">APL</a></a></h3>
<blockquote>
<p>APL is a mistake, carried through to perfection. It is the language of the future for the programming techniques of the past: it creates a new generation of coding bums.</p>
<p>Edsger Dijkstra</p>
</blockquote>
<p>APL was originally designed by Ken Iverson in 1957 as a mathematical notation to be used on blackboards[<a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_hist_apl_computer_history">ref</a>].
Kev Iverson was hired by IBM in 1960 to further develop the notation, at that point still just a mathematical notation and not a programming language.
Iverson’s paper <em>A Programming Language</em> was published in 1962, and would be the basis for naming the language <em>APL</em>.
Finally in 1966 the IBM released APL\360 written in a bit under 40,000 lines of Basic Assembly Language 360.</p>
<p>Just before leaving IBM, in 1979 Iverson gave his famous <em>ACM Turing Award Lecture</em> titled <em>Notation as a tool of Thought</em> where he builds up algorithm intuition in the reader using the APL language[<a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_ntot">ref</a>].
In 1980, Iverson left IBM for I. P. Sharp Associates where he developed SHARP APL [<a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_wiki_iverson">ref</a>].
It was just after this in 1981 that Dyalog APL was born, potentially the most popular APL implementation tothis day and a significant force in the APL community[<a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_hist_dyalog">ref</a>].
Ken Iverson moved on from IPSharp in 1990 to JSoftware to write the J programming language along with Roger Hui, a colleague from I.P. SHARP, who sadly passed away earlier this month (October 2021).</p>
<p>This solution was given to me by <a href="https://aplwiki.com/wiki/Ad%C3%A1m_Brudzewsky">Adám Brudzewsky</a> on the APL Farm discord server (<a href="https://mlochbaum.github.io/BQN/index.html#where-can-i-find-bqn-users">more information on the discord here</a> and <a href="https://aplwiki.com/wiki/Forums">also here</a>).
This runs on APL\360 thanks to <a href="http://web.archive.org/web/20201111235017/http://members.aon.at/nkehrer/ibm_5110/emu5110.html" target="_blank">an IBM 5110 emulator (how cool is this!!!)</a>
I used <a href="http://www.bitsavers.org/pdf/ibm/apl/APL_360_Users_Manual_Aug68.pdf" target="_blank">this IBM APL\360 User’s Manual</a> to play around with Adám’s solution in the emulator.</p>
<p><em>NOTE: These solutions use base 1 indices</em></p>
<pre><code>     ∇ Z←F L
[1]    Z←((L&gt;¯1↓0,L)∧(L&gt;1↓L,0))⍳1
     ∇
     F 1 2 3 1
3
     F 2 1 2 3
1
</code></pre>
<p>Here’s a snippet from the user’s manual linked earlier:</p>
<center>
<img 
  src="/images/lc-peak-element/apl360-users-manual.png"
  alt="Here's a snippet from the user's manual linked earlier"
  width=600/>
</center>
<p>And two more solutions from Adám:</p>
<h4 id="second-solution"><a class="header" href="#second-solution">Second Solution</a></h4>
<pre><code>     ∇ Z←F L;A;B
[1]    A←L&gt;1↓L,0
[2]    B←L&gt;¯1↓0,L
[3]    Z←(A∧B)⍳1
     ∇
</code></pre>
<h4 id="third-solution"><a class="header" href="#third-solution">Third Solution</a></h4>
<pre><code>     ∇ Z←F L
[1]    Z←(L&gt;⌈⌿¯1 1⌽(2,⍴L)⍴L)⍳1
     ∇
</code></pre>
<p>I also solved it in BQN just for fun:</p>
<pre><code>   i0 ← 1‿2‿3‿1
   i1 ← 1‿2‿1‿3‿5‿6‿4
   i2 ← 2‿1‿2‿3‿1
   F ← ((¯∞⊸«˜&lt;⊢)∧(⊢&gt;¯∞⊸»))⊐(1˙)
   F ¨ i0‿i1‿i2
┌─
· ┌·    ┌·    ┌·
  · 2   · 1   · 0
      ┘     ┘     ┘
                    ┘
</code></pre>
<p>And here’s the image explanation of the solution.
These diagrams are meant to be read from top to bottom as the BQN program executes.
You can generate diagrams like these on your own by clicking the <em>Explain</em> button before running your code on the <a href="https://mlochbaum.github.io/BQN/try.html#code=ICAgaTAg4oaQIDHigL8y4oC/M+KAvzEKICAgaTEg4oaQIDHigL8y4oC/MeKAvzPigL814oC/NuKAvzQKICAgaTIg4oaQIDLigL8x4oC/MuKAvzPigL8xCgpGIOKGkCAoKMKv4oie4oq4wqvLnDziiqIp4oinKOKKoj7Cr+KInuKKuMK7KSniipAoMcuZKQoKRiDCqCBpMOKAv2kx4oC/aTI=" target="_blank">Try BQN page linked here.</a></p>
<center>
<img 
  src="/images/lc-peak-element/bqn2.png"
  alt="Here's an explanation of each part of this solution"
  width=600/>
</center>
<h3 id="lisp"><a class="header" href="#lisp"><a href="csblog/2021-10-24-Popular-Languages-1965.html#content">Lisp</a></a></h3>
<blockquote>
<p>LISP has been jokingly described as “the most intelligent way to misuse a computer”. I think that description a great compliment because it transmits the full flavor of liberation: it has assisted a number of our most gifted fellow humans in thinking previously impossible thoughts.</p>
<p>Edsger Dijkstra</p>
</blockquote>
<p>The 5th most popular programming language in 1965 was Lisp.</p>
<p>Lisp was invented by John McCarthy in 1958 at MIT with his paper <em>Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I</em>, paralleling Ken Iverson’s paper <em>A Programming Language</em>.[<a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_hist_scheme">ref</a>].</p>
<p>I used MIT Scheme for my Lisp since it seems like the oldest lisp implementation that I can still install.</p>
<p>Although Scheme is such an old language, it felt very futuristic and clean.
I’ve used other lisps before, but I’m nowhere near an expert.
Scheme felt like a wonderful and comprehensible tool.
I really loved using it and I think I’ll be spending some more quality time with Scheme in these videos.</p>
<p>If you have a better scheme solution, please let me know, I’d love to see it.</p>
<pre><code class="language-scheme">(define shl
  (lambda (v l)
    (reverse (cons v (reverse (cdr l))))))

(define shr
  (lambda (v l)
    (cons v (reverse (cdr (reverse l))))))

(define solve
  (lambda (input)
    (reduce max 0
            (map
              (lambda (a b)
                (if a b -1))
              (map &gt;
                   input
                   (map max
                        (shl -99999 input)
                        (shr -99999 input)))
              (iota (length input)))))))

(for-each
  (lambda (l)
    (newline)
    (display (solve l))
    (newline))
  (list
    '(1 2 3 1)
    '(1 2 1 3 5 6 4)
    '(2 1 2 3 2 1)))
</code></pre>
<p>I did find that the builtin functions for Scheme were a bit lacking.
For example I had to write my own functions to shift a value into a list.</p>
<pre><code class="language-scheme">(define shl
  (lambda (v l)
    (reverse (cons v (reverse (cdr l))))))

(define shr
  (lambda (v l)
    (cons v (reverse (cdr (reverse l))))))
</code></pre>
<p>Here’s what it looks like to use them.
Although I had to write my own, it did come easily.</p>
<pre><code class="language-scheme">1 ]=&gt; (shl 0 '(1 2 3))

;Value: (2 3 0)

1 ]=&gt; (shr 0 '(1 2 3))

;Value: (0 1 2)
</code></pre>
<p>Let’s walk through this solution inside-out.</p>
<pre><code class="language-scheme">(define solve
  (lambda (input)
    (reduce max 0
            (map
              (lambda (a b)
                (if a b -1))
              (map &gt;
                   input
                   (map max
                        (shl -99999 input)
                        (shr -99999 input)))
              (iota (length input))))))
</code></pre>
<p>For an input <code>(1 2 3 1)</code>, we’ll find the max of shifting left and right.
If a number is greater than the max of the left and right, we know it’s greater than both the left and the right value.</p>
<pre><code class="language-scheme">1 ]=&gt; (define input '(1 2 3 1))

;Value: a

1 ]=&gt; (map max
        (shl -99999 input)
        (shr -99999 input))

;Value: (2 3 2 3)
</code></pre>
<p>Now we just have to find the indices in the input where the input is greater than the last return value, the greater of either shift.</p>
<pre><code class="language-scheme">      (map &gt;
           input
           (map max
                (shl -99999 input)
                (shr -99999 input)))

;Value: (#f #f #t #f)
</code></pre>
<p>Then we can just take the index if the previous map gave us a <code>#t</code> true value, or -1 otherwise.
We then take the max of these values to find the peak element.</p>
<pre><code class="language-scheme">1 ]=&gt; (map
        (lambda (a b)
          (if a b -1))
        (map &gt;
             input
             (map max
                  (shl -99999 input)
                  (shr -99999 input)))
        (iota (length input)))

;Value: (-1 -1 2 -1)
</code></pre>
<p>Here’s the same code as before, we’ve just wrapped it in a max reduce to get our final answer.</p>
<pre><code class="language-scheme">1 ]=&gt; (reduce max 0
              (map
                (lambda (a b)
                  (if a b -1))
                (map &gt;
                     input
                     (map max
                          (shl -99999 input)
                          (shr -99999 input)))
                (iota (length input))))
;Value: 2
</code></pre>
<p>Of course now we can just wrap all that code in a function:</p>
<pre><code class="language-scheme">1 ]=&gt; (define solve
        (lambda (input)
          (reduce max 0
                  (map
                    (lambda (a b)
                      (if a b -1))
                    (map &gt;
                         input
                         (map max
                              (shl -99999 input)
                              (shr -99999 input)))
                    (iota (length input))))))
      
1 ]=&gt; (solve '(1 2 3 1))

;Value: 2
</code></pre>
<p>And we can run it on a few inputs to verify our solution:</p>
<pre><code class="language-scheme">1 ]=&gt; (for-each
        (lambda (l)
          (newline)
          (display (solve l))
        (list
          '(1 2 3 1)
          '(1 2 1 3 5 6 4)
          '(2 1 2 3 2 1)))

2
5
3
</code></pre>
<h3 id="basic"><a class="header" href="#basic"><a href="csblog/2021-10-24-Popular-Languages-1965.html#content">BASIC</a></a></h3>
<blockquote>
<p>It is practically impossible to teach good programming to students that have had a prior exposure to BASIC: as potential programmers they are mentally mutilated beyond hope of regeneration.</p>
<p>Edsger Dijkstra</p>
</blockquote>
<p>BASIC stands for <em>Beginner’s All-Purpose Symbolic Instruction Code</em>[<a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_time_basic">ref</a>].
BASIC was designed by two math professors at Dartmouth College in 1964.
John Kemeny, one of the co-creators of BASIC attended lectures from  John von Neumann and worked as Albert Einstein’s mathematical assistant for a time[<a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_time_basic">ref</a>], and Tom Kurtz, the other co-creator, first proposed the concept of time-sharing[<a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_early_timesharing">ref</a>].
These guys were clearly pretty bright.
BASIC was probably the first beginner-oriented language, with the goal of getting students started writing programs as quickly as possible.</p>
<blockquote>
<p>We needed a language that could be ‘taught’ to virtually all students (and faculty) without their having to take a course.</p>
<p>Thomas Kurtz, co-inventor of BASIC</p>
</blockquote>
<p>Visual Basic, a descendent of BASIC used in Excel and other Microsoft products, was actually one of the first languages I ever learned, writing Excel macros for the finance department of the company I worked for.</p>
<p>Although it was a programming on-ramp for me, I still have to side with Dijkstra on BASIC (although maybe not so harshly).
BASIC was the product of some brilliant folks and it had a huge impact on the history of programming, but I can’t say I recommend it today.</p>
<p>This solution is pretty much the same solution I used for the rest of the programming languages:
create a wrapper array so I can pretend that out of bounds in either direction is -∞.
I then check all the values to see if any element is greater than the elements to its left and right.</p>
<p>I used FreeBASIC to run this example:</p>
<pre><code class="language-basic">         dim i0(1 to 4) as integer
         i0(1) = 1
         i0(2) = 2
         i0(3) = 3
         i0(4) = 1

         dim i1(1 to 7) as integer
         i1(1) = 1
         i1(2) = 2
         i1(3) = 1
         i1(4) = 3
         i1(5) = 5
         i1(6) = 6
         i1(7) = 4

         function solve(prob() as integer) as integer
             dim vals(1 to ubound(prob)+2) as integer
             vals(1) = -9999999
             vals(ubound(prob)+1) = -9999999
             for i as integer = 1 to ubound(prob)
                 vals(i) = prob(i)
                 if (vals(i)&gt;vals(i+1) and vals(i)&gt;vals(i-1)) then solve=i-1
             next
         end function

         print solve(i0())
         print solve(i1())
</code></pre>
<pre><code class="language-console">$ ./src/freebasic/lc-peak-element-freebasic
 2
 5
</code></pre>
<h3 id="algol"><a class="header" href="#algol"><a href="csblog/2021-10-24-Popular-Languages-1965.html#content">ALGOL</a></a></h3>
<p>You may notice that Algol is the only language that does not have a scathing quote from Dijkstra.
This is probably in part because Dijkstra was a significant contributor to Algol![<a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_cwi_dijkstra">ref</a>]</p>
<blockquote>
<p>In 1958-1959, Dijkstra was involved in a number of meetings that culminated in the publication of the report defining the ALGOL 60 language. Ironically, Dijkstra’s name does not appear in the list of 13 authors of the final report: it seems he left the committee prematurely because he could not agree with the majority opinions.[<a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_cwi_dijkstra">ref</a>]</p>
</blockquote>
<p>Algol/Fortran family tree:</p>
<center>
<img 
  src="./images/lc-peak-element/algol-fortran-fam-tree.png"
  alt="Algol/Fortran Family Tree"
  width=600/>
</center>
<blockquote>
<p>Here is a language so far ahead of its time that it was not only an improvement on its predecessors but also on nearly all its successors.</p>
<p>Tony Hoare[<a href="https://en.wikipedia.org/wiki/ALGOL">ref</a>]</p>
</blockquote>
<p>I’m using the Algol68 Genie compiler-interpreter for this code.
I honestly found Algol pretty usable!
I saw some code that used function pointers, and it looked pretty clean.
It seems like Algol has some pretty modern first-class-function capabilities.
I can see why it was the language in which computer algorithms were published for many years[<a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_britannica_algol">ref</a>].
ALGOL was actually designed by an international committee of the ACM during 1958–60 for this purpose.</p>
<pre><code class="language-algol">PROC solve = ([]INT elements)INT: (
  INT found := -1;
  [1:(UPB elements)+1]INT vs;
  vs[1] := - 999 999 999;
  vs[UPB elements] := - 999 999 999;
  FOR i FROM LWB elements TO UPB elements DO vs[1+i] := elements[i] OD;
  FOR i FROM 2 TO UPB elements DO
    IF vs[i] &gt; vs[i+1] AND vs[i] &gt; vs[i-1] THEN found := i-1 FI
  OD;
  found
);

main:(
  []INT i0 = (1,2,3,1);
  []INT i1 = (1,2,1,3,5,6,4);

  print(("Input #0: ", solve(i0), new line));
  print(("Input #1: ", solve(i1), new line))
)
</code></pre>
<pre><code class="language-console">$ a68g ../src/algol68/lc-peak-element.al
Input #0:          +3
Input #1:          +6
</code></pre>
<p>I honestly wouldn’t mind writing more Algol down the line.</p>
<h3 id="cobol"><a class="header" href="#cobol"><a href="csblog/2021-10-24-Popular-Languages-1965.html#content">COBOL</a></a></h3>
<blockquote>
<p>The use of COBOL cripples the mind; its teaching should, therefore, be regarded as a criminal offense.</p>
<p>Edsger Dijkstra</p>
</blockquote>
<p>The history behind COBOL is extremely inspiring and exciting, however COBOL was <em>very</em> painful to use.
And I only learned the most shallow bit of COBOL - in order to read more like plain English, COBOL has <strong>over 300 keywords</strong>.
I can only imagine what it feels like to maintain a 500k line COBOL codebase.</p>
<p>I used the GNUCobol compiler for this example.
You’ll notice that everything is indented - COBOL, like several of the other languages I’m covering here, was originally used on a punchcard <a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_os_wac">as explained in this article from opensource.com</a>.
Each puncard represented <em>a single line of code</em>, and the first six and final eight columns of each card were reserved for sequence numbers and identifiers, which you’ll see here as an asterisk <code>*</code> for comments, and a dash <code>-</code> for line continuation.</p>
<pre><code class="language-cobol">       ID DIVISION.
       PROGRAM-ID. ARRAYTEST.
       ENVIRONMENT DIVISION.
       DATA DIVISION. 
       WORKING-STORAGE SECTION.
      * Store both problems and their sizes and answers in one
      * structure
       01 SIZES PIC 9(3) OCCURS 2 TIMES VALUE 0.
       01 OFFSETS PIC 9(3) OCCURS 2 TIMES VALUE 0.
       01 PROBLEMS PIC 9(3) OCCURS 12 TIMES VALUE 0.
       01 CURRENT-PROBLEM PIC 9(3) VALUE 0.
       01 TMP PIC 9(3) VALUE 0.
       01 IDX PIC 9(3) VALUE 1.
       01 NPROBLEMS PIC 9(3) VALUE 2.
       01 ANSWERS PIC S9(3) OCCURS 2 TIMES VALUE -1.
       01 VALS PIC S9(5) OCCURS 15 TIMES.
       PROCEDURE DIVISION.

       100-MAIN.

      *    Set up problem [1,2,3,1]
           MOVE 4 TO SIZES(1).
           MOVE 0 TO OFFSETS(1).

           MOVE 1 TO PROBLEMS(1).
           MOVE 2 TO PROBLEMS(2).
           MOVE 3 TO PROBLEMS(3).
           MOVE 1 TO PROBLEMS(4).

      *    Set up problem [1,2,1,3,5,6,4]
           MOVE 7 TO SIZES(2).
           MOVE 4 TO OFFSETS(2).

           MOVE 1 TO PROBLEMS(5).
           MOVE 2 TO PROBLEMS(6).
           MOVE 1 TO PROBLEMS(7).
           MOVE 3 TO PROBLEMS(8).
           MOVE 5 TO PROBLEMS(9).
           MOVE 6 TO PROBLEMS(10).
           MOVE 4 TO PROBLEMS(11).

      *    Run solve procedure on both problems
           PERFORM VARYING CURRENT-PROBLEM FROM 1 BY 1 UNTIL CURRENT-PRO
      -BLEM &gt; NPROBLEMS
             MOVE OFFSETS(CURRENT-PROBLEM) TO IDX
             PERFORM SOLVE
             DISPLAY ANSWERS(CURRENT-PROBLEM) END-DISPLAY
           END-PERFORM.

           STOP RUN.

       SOLVE.
           MOVE -99999 TO VALS(1).
           MOVE -99999 TO VALS(SIZES(CURRENT-PROBLEM)).
           PERFORM VARYING IDX FROM 1 BY 1 UNTIL IDX&gt;SIZES(CURRENT-PROBL
      -EM)
             COMPUTE TMP = IDX + OFFSETS(CURRENT-PROBLEM) END-COMPUTE
             MOVE PROBLEMS(TMP) TO VALS(1+IDX)
           END-PERFORM.

           PERFORM VARYING IDX FROM 2 BY 1 UNTIL IDX&gt;SIZES(CURRENT-PROBL
      -EM)
             
             COMPUTE TMP = IDX + OFFSETS(CURRENT-PROBLEM) END-COMPUTE
             IF PROBLEMS(TMP) &gt; PROBLEMS(TMP - 1)
      -AND PROBLEMS(TMP) &gt; PROBLEMS(TMP + 1)
               MOVE IDX TO ANSWERS(CURRENT-PROBLEM)
             END-IF
           END-PERFORM.

       PRINT-AR.
           DISPLAY "IDX=" IDX " VALUE=" PROBLEMS(IDX) END-DISPLAY.
</code></pre>
<p>Running gives:</p>
<pre><code>$ ./src/cobol/lc-peak-element-cobol
+003
+006
</code></pre>
<p>I certainly felt the weight of this when I tried to write this function:
Every time I had to change a condition that wrapped a line, I would join the lines together and figure out where the new line break should be, and make sure to get the <code>-</code> character in the 7th column.
I’m sure there are some more modern conventions around COBOL considering <em>5 billion lines of new COBOL code are written every year</em>[<a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_os_wac">ref</a>], but I’m pretty content not to write any COBOL for a while.</p>
<pre><code class="language-cobol">       SOLVE.
           MOVE -99999 TO VALS(1).
           MOVE -99999 TO VALS(SIZES(CURRENT-PROBLEM)).
           PERFORM VARYING IDX FROM 1 BY 1 UNTIL IDX&gt;SIZES(CURRENT-PROBL
      -EM)
             COMPUTE TMP = IDX + OFFSETS(CURRENT-PROBLEM) END-COMPUTE
             MOVE PROBLEMS(TMP) TO VALS(1+IDX)
           END-PERFORM.

           PERFORM VARYING IDX FROM 2 BY 1 UNTIL IDX&gt;SIZES(CURRENT-PROBL
      -EM)
             
             COMPUTE TMP = IDX + OFFSETS(CURRENT-PROBLEM) END-COMPUTE
             IF PROBLEMS(TMP) &gt; PROBLEMS(TMP - 1)
      -AND PROBLEMS(TMP) &gt; PROBLEMS(TMP + 1)
               MOVE IDX TO ANSWERS(CURRENT-PROBLEM)
             END-IF
           END-PERFORM.
</code></pre>
<p>Now on to the inspiring stuff: TLDR COBOL was designed by a consensus-driven committee with a huge focus on portability, and led by women.</p>
<p>In 1959 Grace Hopper, a retired Navy officer, organized a meeting of users and manufacturers to conceive of a programming language in response to Mary Hawes’s call for a portable programming language, a language that could be compiled and ran on computers from multiple manufacturers.
<a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_twitter_bryce_cobol">You can read more about the history of COBOL on this Twitter thread from Bryce Lelbach which you should <em>definitely</em> check out.</a></p>
<p>I think we have a lot to learn from COBOL, and a lot to be thankful for.
<a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_iso_homepage">The ISO</a> didn’t come along until 1988, long after Grace Hopper initiated the committee for the development of COBOL.
I’m sure we owe a huge debt to COBOL and the women-led consensus-driven community that gave us COBOL.
I want to learn all I can from that history.</p>
<p>I don’t want to write any more COBOL than I have to though :smile:.</p>
<h3 id="fortran"><a class="header" href="#fortran"><a href="csblog/2021-10-24-Popular-Languages-1965.html#content">Fortran</a></a></h3>
<blockquote>
<p>FORTRAN, ‘the infantile disorder’, by now nearly 20 years old, is hopelessly inadequate for whatever computer application you have in mind today: it is now too clumsy, too risky, and too expensive to use.</p>
<p>Edsger Dijkstra</p>
</blockquote>
<p>Fortran, the grand finale, #1 on our list.
I <em>completely</em> disagree with Dijkstra on this one - I love Fortran’s history and I occasionally write it professionally.</p>
<p>In 1953, John Backus submitted a proposal to his bosses at IBM to develop a more practical alternative to assembly language[<a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_ftn_start">ref</a>].
The first compiler didn’t come around until 1957.
The name “Fortran” is derived from <a href="csblog/2021-10-24-Popular-Languages-1965.html#ref_ftn_start">FORmula TRANslation</a>, and very quickly became the lingua franca for scientific computing.</p>
<p>Fortran still is one of the most used programming languages in high performance computing (HPC), and it’s not going away any time soon.
<a href="https://github.com/llvm/llvm-project/tree/main/flang#flang">The Flang project, part of the LLVM project,</a> is a modern Fortran compiler targeting the 2018 standard with support for OpenACC, OpenMP and other cool optimization stuff.
It’s written in wonderfully modern and well-maintained C++, and I use the C++ style guide from Flang for my technical teams at my day job.
Flang is definitely worth keeping an eye on, I think it will become a significant force in HPC in the coming years.</p>
<p>I tried using the g77 compiler from GCC 3.4.4 for this example to get a better feel for historical Fortran, but after removing some syntax I didn’t realize came with f90, I realized the 32b build of GCC would probably not be able to target my modern machine.</p>
<pre><code class="language-console">$ g77 ./src/fortran/lc-peak-element.f
/tmp/ccYc9ESc.s: Assembler messages:
/tmp/ccYc9ESc.s:39: Error: invalid instruction suffix for `push'
/tmp/ccYc9ESc.s:91: Error: invalid instruction suffix for `push'
</code></pre>
<p>This at least let me know my code was valid Fortran 77!
After removing any compilation errors with g77, I just built the example with gfortran from gcc 11.2.</p>
<pre><code class="language-fortran">c     Comments require a 'c'in the first column, just like the
c     punchcards!
      program main
        integer :: ret
        integer :: i0(4)
        integer :: i1(7)

c       First Problem
        i0(1) = 1
        i0(2) = 2
        i0(3) = 3
        i0(4) = 1

c       Second Problem
        i1(1) = 1
        i1(2) = 2
        i1(3) = 1
        i1(4) = 2
        i1(5) = 4
        i1(6) = 2
        i1(7) = 1

        ret = -1

        call solve(i0, 4, ret)
        print*,ret

        call solve(i1, 7, ret)
        print*,ret

      end program

      subroutine solve(input, len, ret)
        integer :: input(len)
        integer :: len
        integer :: ret

        integer :: vals(len+2)
        integer :: i

        vals(1) = -99999
        vals(len) = -99999

        do i = 2, len
          vals(i) = input(i+1)
        end do

        do i = 2, len
          if (vals(i) &gt; vals(i+1) .and. vals(i) &gt; vals(i-1)) then
            ret = i-1
            return
          endif
        enddo

      end subroutine
</code></pre>
<p>For Fortran 90 I can do array assignments like this, which I really like:</p>
<pre><code class="language-fortran">vals(2:len) = input
</code></pre>
<p>instead of this:</p>
<pre><code class="language-fortran">        do i = 2, len
          vals(i) = input(i+1)
        end do
</code></pre>
<p>and I don’t get to use the <code>intent</code> keyword, both of which were big drawbacks, but this really wasn’t too bad.</p>
<p>Looking to the future of Fortran, GCC’s GFortran is very actively maintained, <a href="https://developer.nvidia.com/hpc-sdk">the newest NVIDIA HPC SDK has fantastic Fortran support</a>, <a href="https://www.olcf.ornl.gov/frontier/">the new US Dept. of Energy Exascale supercomputer <em>Frontier</em></a> will use AMD GPUs which have <a href="https://github.com/ROCmSoftwarePlatform/hipfort">hipfort, a Fortran interface to AMD GPU libraries</a>, and <a href="https://www.intel.com/content/www/us/en/develop/documentation/get-started-with-cpp-fortran-compiler-openmp/top.html">Intel’s GPU platform and Fortran compiler are widely used as well</a>.
Fortran has a wonderfully rich history, and it’s certainly a part of our future.</p>
<blockquote>
<p>Much of my work has come from being lazy. I didn’t like writing programs, and so, when I was working on the IBM 701, writing programs for computing missile trajectories, I started work on a programming system to make it easier to write programs.</p>
<p>John Backus</p>
</blockquote>
<h2 id="conclusion-2"><a class="header" href="#conclusion-2">Conclusion</a></h2>
<p>I hope you all enjoyed foray into the history of programming languages (and computing in general)!</p>
<p>{% include footer.html %}</p>
<!---
## YouTube Description:

Solve a leetcode problem with the most popular programming languages of the '60s!

Timestamps:
0:00 Intro
0:38 Leetcode problem introduction
1:07 APL
3:34 Lisp
5:30 BASIC
7:10 ALGOL
8:05 COBOL
10:35 Fortran
12:58 Conclusion

You can find a longer version with more references here:
Blog Post: http://www.ashermancinelli.com/pop-langs-1960s

GitHub Repo for Examples: https://github.com/ashermancinelli/algorithm-testbed
LinkedIn: https://www.linkedin.com/in/asher-mancinelli-bb4a56144/
-->
<h2 id="references-3"><a class="header" href="#references-3">References</a></h2>
<ul>
<li><a target="_blank" name="ref_ftn_start" href="https://en.wikipedia.org/wiki/Fortran#History">Fortran history</a></li>
<li><a target="_blank" name="ref_pop_langs" href="https://statisticsanddata.org/most-popular-programming-languages/">Most Popular Programming Languages</a></li>
<li><a target="_blank" name="ref_hist_apl_computer_history" href="https://computerhistory.org/blog/the-apl-programming-language-source-code/">The Apl Programming Language Source Code</a></li>
<li><a target="_blank" name="ref_wiki_iverson"><a href="https://en.wikipedia.org/wiki/Kenneth_E._Iverson">Kenneth Iverson Wikipedia</a></a></li>
<li><a target="_blank" name="ref_ntot" href="https://www.jsoftware.com/papers/tot.htm"><em>Notation as a Tool of Thought</em>, Ken Iverson</a></li>
<li><a target="_blank" name="ref_hist_dyalog" href="https://www.dyalog.com/uploads/files/apl50/Dyalog%20APL%20A%20Personal%20History.pdf">History of Dyalog</a></li>
<li><a target="_blank" name="ref_gnuapl_stallman" href="https://en.wikipedia.org/wiki/APL_(programming_language)#GNU_APL">GNU APL</a></li>
<li><a target="_blank" name="ref_pnnl" href="https://www.pnnl.gov/">Pacific Northwest National Laboratory</a></li>
<li><a target="_blank" name="ref_bqn_hist" href="https://mlochbaum.github.io/BQN/commentary/history.html">BQN’s Development History</a></li>
<li><a target="_blank" name="ref_hist_scheme" href="https://en.wikipedia.org/wiki/History_of_the_Scheme_programming_language">History of the Scheme Programming Language</a></li>
<li><a target="_blank" name="ref_apl_wiki" href="https://aplwiki.com/wiki/Main_Page">APL Wiki</a></li>
<li><a target="_blank" name="ref_apl_wiki_dyalog" href="https://aplwiki.com/wiki/Dyalog_APL">APL Wiki: Dyalog</a></li>
<li><a target="_blank" name="ref_apl_wiki_logos" href="https://aplwiki.com/wiki/APL_logo">APL Wiki: Logos</a></li>
<li><a target="_blank" name="ref_alg_testbed_repo" href="https://github.com/ashermancinelli/algorithm-testbed">Repository for all the solutions</a></li>
<li><a target="_blank" name="ref_time_basic" href="https://time.com/69316/basic/">Fifty Years of BASIC, the Programming Language That Made Computers Personal</a></li>
<li><a target="_blank" name="ref_cwi_dijkstra" href="https://www.cwi.nl/about/history/e-w-dijkstra-brilliant-colourful-and-opinionated">Edsger W. Dijkstra: Brilliant, Colourful, and Opinionated</a></li>
<li><a target="_blank" name="ref_si_cobol" href="https://americanhistory.si.edu/cobol/introduction">National Museum of American History</a></li>
<li><a target="_blank" name="ref_wiki_cobol" href="https://en.wikipedia.org/wiki/COBOL#:~:text=To%20support%20this%20English%2Dlike,ARE%20%2C%20and%20VALUE%20and%20VALUES%20.">Wikipedia: COBOL</a></li>
<li><a target="_blank" name="ref_os_wac" href="https://opensource.com/article/17/8/what-about-cobol">opensource.com: Don’t hate COBOL until you’ve tried it</a></li>
<li><a target="_blank" name="ref_twitter_bryce_cobol" href="https://twitter.com/blelbach/status/1259313973318451200">Bryce Lelbach Twitter Thread on COBOL</a></li>
<li><a target="_blank" name="ref_iso_homepage" href="https://www.iso.org/technical-committees.html">International Organization for Standardization (ISO)</a></li>
<li><a target="_blank" name="ref_flang" href="https://github.com/llvm/llvm-project/tree/main/flang#flang">Flang Fortran Compiler</a></li>
<li><a target="_blank" name="ref_ecp_frontier" href="https://www.olcf.ornl.gov/frontier/">US DOE Frontier Supercomputer at ORNL</a></li>
<li><a target="_blank" name="ref_britannica_algol" href="https://www.britannica.com/technology/ALGOL-computer-language">Britannica: ALGOL computer language</a></li>
<li><a target="_blank" name="ref_early_timesharing" href="https://www.cs.cornell.edu/wya/AcademicComputing/text/earlytimesharing.html">Cornell University: Early Timesharing</a></li>
<li><a target="_blank" name="ref_apk_wiki_timeline" href="https://aplwiki.com/index.php?title=Timeline_of_influential_array_languages">APL Wiki Timeline</a></li>
<li><a target="_blank" name="ref_ibm_5110_emu" href="http://web.archive.org/web/20201111235017/http://members.aon.at/nkehrer/ibm_5110/emu5110.html">IBM 5110 Emulator</a></li>
<li><a target="_blank" name="ref_ibm_apl_bitsavers" href="http://www.bitsavers.org/pdf/ibm/apl/">Bit Savers: IBM APL References</a></li>
<li><a target="_blank" name="ref_ibm_apl_bitsavers_refcard" href="http://www.bitsavers.org/pdf/ibm/apl/S210-0007-0_APL_360_Reference_Card.pdf">APL\360 Reference Card</a></li>
<li><a target="_blank" name="ref_ibm_apl_bitsavers_68manual" href="http://www.bitsavers.org/pdf/ibm/apl/APL_360_Users_Manual_Aug68.pdf">APL\360 User Manual</a></li>
<li><a target="_blank" name="ref_tut_a68" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.4668&rep=rep1&type=pdf">Tutorial on Algol68</a></li>
<li><a target="_blank" name="ref_g77" href="http://www.kilmnj.com/g77/">GNU Fortran 77 (g77) Legacy Site</a></li>
<li><a target="_blank" name="ref_g77_legacy_tarball" href="https://gfortran.meteodat.ch/download/legacy_g77/">Legacy G77 tarball</a></li>
</ul>
<!--
layout: post
title: One Problem, Four Languages, Two Paradigms
permalink: /leetcode-distributed-computing
cat: cs
-->
<p>Solving a leetcode problem in four programming languages using two acceleration paradigms!</p>
<p><em>NOTE: This post is a transcript of <a href="https://youtu.be/Xk7-xjnEISE">the youtube video linked here</a>.</em></p>
<p>In addition to that, we’ll be using various combinations of two programming paradigms common in distributed computing: using a GPU to perform some calculations and MPI to distribute our calculation among multiple processes, potentially on multiple machines.</p>
<p>We’ll be looking at <a href="https://leetcode.com/problems/valid-sudoku/">this leetcode problem,</a> which is to determine if a 9 x 9 Sudoku board is valid, but not necessarily solvable.
Each row, column, and subbox of the grid must have the digits 1-9.</p>
<p>Let’s jump right in to our BQN solution.</p>
<h2 id="content-1"><a class="header" href="#content-1">Content</a></h2>
<ol>
<li><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#bqn">BQN</a></li>
<li><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#approach">Approach</a></li>
<li><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#python">Python</a></li>
<li><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#python-and-mpi">Python And MPI</a></li>
<li><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#c++">C++</a></li>
<li><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#c++-and-mpi">C++ And MPI</a></li>
<li><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#c++-and-cuda">C++ And CUDA</a></li>
<li><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#c++-and-cuda-and-mpi">C++ And CUDA And MPI</a></li>
<li><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#fortran">Fortran</a></li>
<li><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#fortran-and-mpi">Fortran And MPI</a></li>
<li><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#conclusion">Conclusion</a></li>
<li><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#youtube-description">YouTube Video Description</a></li>
</ol>
<h2 id="bqn"><a class="header" href="#bqn"><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#content">BQN</a></a></h2>
<p>This is what our sudoku boards will look like:</p>
<pre><code># two 8s in the first block
bad ← ⟨8, 3, 0, 0, 7, 0, 0, 0, 0
       6, 0, 0, 1, 9, 5, 0, 0, 0
       0, 9, 8, 0, 0, 0, 0, 6, 0
       8, 0, 0, 0, 6, 0, 0, 0, 3
       4, 0, 0, 8, 0, 3, 0, 0, 1
       7, 0, 0, 0, 2, 0, 0, 0, 6
       0, 6, 0, 0, 0, 0, 2, 8, 0
       0, 0, 0, 4, 1, 9, 0, 0, 5
       0, 0, 0, 0, 8, 0, 0, 7, 9⟩

# valid sudoku
good ← ⟨5, 3, 0, 0, 7, 0, 0, 0, 0
        6, 0, 0, 1, 9, 5, 0, 0, 0
        0, 9, 8, 0, 0, 0, 0, 6, 0
        8, 0, 0, 0, 6, 0, 0, 0, 3
        4, 0, 0, 8, 0, 3, 0, 0, 1
        7, 0, 0, 0, 2, 0, 0, 0, 6
        0, 6, 0, 0, 0, 0, 2, 8, 0
        0, 0, 0, 4, 1, 9, 0, 0, 5
        0, 0, 0, 0, 8, 0, 0, 7, 9⟩

</code></pre>
<p>And here is our full solution.
This solution will be the basis for all of our later solutions.</p>
<pre><code>F ← {𝕊𝕩:
  Fl ← 0⊸≠⊸/                       # Filter 0s out
  Dup ← (∨´∾´)¬∘∊¨                 # Are there any duplicates?

  rs ← Dup Fl¨(9/↕9)⊔𝕩             # Check rows
  cs ← Dup Fl¨(81⥊↕9)⊔𝕩            # Check columns

  bi ← 27⥊3/↕3
  bs ← Dup Fl¨(bi∾(3+bi)∾(6+bi))⊔𝕩 # Check blocks

  (bs ∨ rs ∨ cs)⊑"true"‿"false"
}
</code></pre>
<p>This first line is a function to filter out any 0s:</p>
<pre><code>   Fl ← 0⊸≠⊸/
   Fl ⟨5, 3, 0, 0, 7, 0, 0, 0, 0⟩
⟨ 5 3 7 ⟩
</code></pre>
<p>Here we have another utility function to return an integer indicating whether any duplicates were found in any sublists:</p>
<pre><code>   Dup ← (∨´∾´)¬∘∊¨
   Dup ⟨⟨5, 3, 7⟩, ⟨1, 2, 3⟩⟩
0
   Dup ⟨⟨5, 3, 7⟩, ⟨1, 2, 2⟩⟩
1
</code></pre>
<p>Next we check for duplicates in all the filtered rows and columns:</p>
<pre><code>   rs ← Dup Fl¨(9/↕9)⊔𝕩
   cs ← Dup Fl¨(81⥊↕9)⊔𝕩
</code></pre>
<p>These ranges are used to create indices for grouping the values in X.
I’ll show a trimmed down version of their output here to give you an idea:</p>
<pre><code>   3‿3⥊(3/↕3) # For the rows
┌─       
╵ 0 0 0  
  1 1 1  
  2 2 2  
        ┘
   3‿3⥊(9⥊↕3) # For the columns
┌─       
╵ 0 1 2  
  0 1 2  
  0 1 2  
        ┘
</code></pre>
<p>Next I do something similar to get the indices for the boxes.</p>
<pre><code>   bi ← 27⥊3/↕3
   3‿9⥊bi
┌─                   
╵ 0 0 0 1 1 1 2 2 2  
  0 0 0 1 1 1 2 2 2  
  0 0 0 1 1 1 2 2 2  
                    ┘
</code></pre>
<p>This creats indices for the first three boxes, and you can probably imagine how to extend this to get the indices for all the boxes. I just add three to the previous indices, and then add six, and then append them all together. Here’s the second layer:</p>
<pre><code>   3‿9⥊bi+3
┌─                   
╵ 3 3 3 4 4 4 5 5 5  
  3 3 3 4 4 4 5 5 5  
  3 3 3 4 4 4 5 5 5  
                    ┘
</code></pre>
<p>And the final layer:</p>
<pre><code>   3‿9⥊bi+6
┌─                   
╵ 6 6 6 7 7 7 8 8 8  
  6 6 6 7 7 7 8 8 8  
  6 6 6 7 7 7 8 8 8  
                    ┘
</code></pre>
<p>And all three layers of indices stacked on top of each other:</p>
<pre><code>   9‿9⥊(bi∾(3+bi)∾(6+bi))
┌─                   
╵ 0 0 0 1 1 1 2 2 2  
  0 0 0 1 1 1 2 2 2  
  0 0 0 1 1 1 2 2 2  
  3 3 3 4 4 4 5 5 5  
  3 3 3 4 4 4 5 5 5  
  3 3 3 4 4 4 5 5 5  
  6 6 6 7 7 7 8 8 8  
  6 6 6 7 7 7 8 8 8  
  6 6 6 7 7 7 8 8 8  
                    ┘
</code></pre>
<p>Using these indices, I group all the elements of the input, and then check all of them for duplicates:</p>
<pre><code>   bs ← Dup Fl¨(bi∾(3+bi)∾(6+bi))⊔𝕩 # Check blocks
</code></pre>
<p>And in the end I check if there were duplicates in the blocks, in the rows, or in the columns, and use that to index into our strings that indicate whether our sudoku board is valid or not.</p>
<pre><code>   (bs ∨ rs ∨ cs)⊑"true"‿"false"
</code></pre>
<h2 id="approach"><a class="header" href="#approach"><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#content">Approach</a></a></h2>
<p>Before we move on to the Python solution, I’d like to talk about our approach to this solution in the rest of the languages, because they will all be pretty similar.</p>
<p>Just like in the BQN solution, we have three collections which represent the validity of the rows, another for the columns, and a third for the blocks.</p>
<p>Here I have a subset of a sudoku board on the bottom.</p>
<p><img src="csblog//images/approach1.png" alt="Initial Row, Column, and Block Matrices" /></p>
<p>In our procedural languages, we’ll create an array thrice the size of the grid to hold these values.</p>
<p>Note that this is not as space (or time) efficient as many of the solutions that you can find on the discussion page for the leetcode problem, but it is much easier to parallelize and that’s really the point of this video.</p>
<p>Let’s now walk through a few steps of our algorithm starting at the second row and first column of our sudoku board, which relates to the second row of our “row matrix.”</p>
<p>Because we’re looking at our row matrix, we’ll take the row index in our sudoku board as the row for our row matrix, and we’ll take the value in the cell, in this case 6, as the column in our row matrix.
We’ll increment the value at this location in our row matrix, or in the first layer of our 3-d sum matrix that we’ll use to get our final answer.</p>
<p><img src="csblog//images/approach2.png" alt="Checking Rows" /></p>
<p>Let’s move on to check the first row and second column of our sudoku board for our column matrix.
Because we’re looking at our column matrix, or the second layer of our final sum array, we’ll use the column index as the row index in our column matrix, and the value in that cell for the column index in our column matrix.</p>
<p>We’ll increment the value at this location in our column matrix, or in the second layer of our 3-d sum matrix that we’ll use to get our final answer.</p>
<p><img src="csblog//images/approach3.png" alt="Checking Columns" /></p>
<p>Finally, let’s look at the first block in our sudoku board, which corresponds to the first row in our block matrix, and let’s look at the first cell in that block.
The value in the first cell in the first block is 8, so we’ll increment the first row and eighth column in our block matrix.</p>
<p><img src="csblog//images/approach4.png" alt="Checking Blocks" /></p>
<p>If we then perform these three operations for every cell in the sudoku board, we’ll have a final matrix that indicates all the row-column-block-value combinations that we have, and if any cell in that final matrix has a value greater than one, then our board is invalid.</p>
<p>If we were then to check the final cell in the first block of our board, we would find that the eighth element of the first row of our block matrix would be incremented again, which would mean we have an invalid board!</p>
<p><img src="csblog//images/approach5.png" alt="Checking Last Element of Block" /></p>
<p>If any value in our final array is greater than one, then we know we have at least one duplicate in at least one row, column, or block.</p>
<p>What’s neat about this solution is that no single operation depends on any other operation as long as we perform our operations atomically.
This way, our work can be performed on multiple machines or different devices, and as long as we synchronize at the end, our solution will be the same.</p>
<p>Now that we’ve talked strategy, let’s see what this looks like in our Python solution.</p>
<h2 id="python"><a class="header" href="#python"><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#content">Python</a></a></h2>
<p>Here’s our simple Python solution:</p>
<pre><code class="language-python">shape = 9
blksz = 3
def solve(board):
    ar = [[[0 for j in range(3)] for i in range(shape)] for k in range(shape)]
    for r in range(shape):
        for c in range(shape):
            v = board[r][c]
            if 0 == v:
                continue
            ar[r][v - 1][0] += 1
            ar[c][v - 1][1] += 1

            bi = (r // blksz) * blksz + (c // blksz)
            ar[bi][v - 1][2] += 1
    return max(max(i) for j in ar for i in j) &lt; 2
</code></pre>
<p>You can see here that we increment the value in the first layer of our full 3D matrix according to the row and the value in the cell currently being examined:</p>
<pre><code class="language-python">            ar[r][v - 1][0] += 1
</code></pre>
<p>We do the same for our column matrix:</p>
<pre><code class="language-python">            ar[c][v - 1][1] += 1
</code></pre>
<p>And finally for our block matrix, it just takes a little bit of math to figure out what our block index is.</p>
<pre><code class="language-python">            bx = r // blksz
            by = c // blksz
            bi = bx * blksz + by
            ar[bi][v - 1][2] += 1
</code></pre>
<p>I use this main function to run the python solution:</p>
<pre><code class="language-python">if __name__ == "__main__":
    for b in sudoku_9x9.boards():
        print(solve(b))
</code></pre>
<p>We run our example with two valid boards and two invalid boards and get the answers we expect:</p>
<pre><code class="language-console">$ python ./src/python/lc-valid-sudoku.py
True
True
False
False
</code></pre>
<h2 id="python-and-mpi"><a class="header" href="#python-and-mpi"><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#content">Python And MPI</a></a></h2>
<p>Now we’ll look at another python example, but this time one that uses MPI to distribute the calculations.</p>
<p>MPI provides a lot of infrastructure for distributed computing: using the <code>mpirun</code> command spawns N processes, each of which knows how many processes were spawned, what its unique process ID is, and some other relevant information.
These processes may be spawned on multiple machines even, and MPI gives us the tools to communicate between these processes.
We’ll take advantage of this infrastructure to perform our calculations on multiple processes.</p>
<pre><code class="language-python">import numpy as np
from mpi4py import MPI
shape = 9
blksz = 3
comm = MPI.COMM_WORLD
def solve(board, comm):
    ar = np.zeros((9, 9, 3), dtype=np.int64)
    chunk = (81 + comm.size - 1) // comm.size
    subscripts = (*itertools.product(range(9), range(9)),)
    for i in range(comm.rank * chunk, (comm.rank * chunk) + chunk):
        if i &gt;= 81:
            break
        r, c = subscripts[i]
        v = board[r][c]
        if 0 == v:
            continue
        ar[r][v - 1][0] += 1
        ar[c][v - 1][1] += 1
        bi = (r // blksz) * blksz + (c // blksz)
        ar[bi][v - 1][2] += 1
    gar = np.zeros((9 * 9 * 3,), dtype=np.int64)
    comm.Reduce([ar.flatten(), MPI.INT], [gar, MPI.INT], op=MPI.SUM, root=0)
    comm.Barrier()
    return max(gar.flatten()) &lt; 2 if 0 == comm.rank else False
</code></pre>
<p>This is what the setup looks like to get an MPI program running.</p>
<pre><code class="language-python">if __name__ == "__main__":
    if 0 == comm.rank:
        print("Running with size {0}".format(comm.size))

    for b in sudoku_9x9.boards():
        comm.Barrier()
        if 0 == comm.rank:
            ret = solve(b, comm)
            print(ret)
        else:
            solve(b, comm)
</code></pre>
<p>Here we chunk our work up based on how many processes we have:</p>
<pre><code class="language-python">    chunk = ((9 * 9) + comm.size - 1) // comm.size
</code></pre>
<p>Say we’re given 5 processes and we have 81 cells to check (because that’s the size of our sudoku board).
The calculation would look something like this:</p>
<p><code>chunk</code> is then the smallest amount of work for each process such that all the work that needs to be done is performed.
This is a common calculation that needs to be done in parallel computing.
Note that our final process may exit early if the work is not evenly divisible by the chunk size.</p>
<pre><code class="language-console">&gt;&gt;&gt; work = 81
&gt;&gt;&gt; size = 5
&gt;&gt;&gt; chunk = (work + size - 1) // size
&gt;&gt;&gt; chunk
17
&gt;&gt;&gt; chunk * size
85
</code></pre>
<p>We then generate all the possible combinations of rows and columns, and iterate over only the elements that fall within the chunk of work that belongs to our current MPI process.</p>
<pre><code class="language-python">    subscripts = (*itertools.product(range(9), range(9)),)
    for i in range(comm.rank * chunk, (comm.rank * chunk) + chunk):
        if i &gt;= work:
            break
        r, c = subscripts[i]
</code></pre>
<p>The rest of this code is exactly the same as our serial implementation:</p>
<pre><code class="language-python">        v = board[r][c]
        if 0 == v:
            continue
        ar[r][v - 1][0] += 1
        ar[c][v - 1][1] += 1
        bi = (r // blksz) * blksz + (c // blksz)
        ar[bi][v - 1][2] += 1
</code></pre>
<p>This next bit is more interesting.
We create a global array with the size we need to hold our final sum matrix, and we use the MPI function <code>Reduce</code>.
This function will perform the operation <code>op</code>, in this case <code>MPI.SUM</code>, to join the arrays <code>ar</code> and <code>gar</code> together on rank 0 specified by the <code>root</code> argument.
This means that our final summed matrix for all components of the solution is on the MPI process with rank 0.
We can then check if we have any cells with values greater than one, and return that value if we’re on rank 0.
Otherwise, we can just return false since no other rank has the final array.</p>
<pre><code class="language-python">    gar = np.zeros((9 * 9 * 3,), dtype=np.int64)
    comm.Reduce([ar.flatten(), MPI.INT], [gar, MPI.INT], op=MPI.SUM, root=0)
    comm.Barrier()
    return max(gar.flatten()) &lt; 2 if 0 == comm.rank else False
</code></pre>
<p>Here I run the example on 5 processes, and we see we get the same solution as with our serial example.</p>
<pre><code class="language-console">$ mpirun -n 5 python ./src/python/lc-valid-sudoku-mpi.py
Running with size 5
True
True
False
False
</code></pre>
<p>Now let’s move on to all our C++ solutions.</p>
<h2 id="c"><a class="header" href="#c"><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#content">C++</a></a></h2>
<p>All of our C++ solutions will use a board like this:</p>
<pre><code class="language-cpp">const auto board = std::array&lt;int, 81&gt;{
  5, 3, 0,  0, 7, 0,  0, 0, 0,
  6, 0, 0,  1, 9, 5,  0, 0, 0,
  0, 9, 8,  0, 0, 0,  0, 6, 0,
  
  8, 0, 0,  0, 6, 0,  0, 0, 3,
  4, 0, 0,  8, 0, 3,  0, 0, 1,
  7, 0, 0,  0, 2, 0,  0, 0, 6,
  
  0, 6, 0,  0, 0, 0,  2, 8, 0,
  0, 0, 0,  4, 1, 9,  0, 0, 5,
  0, 0, 0,  0, 8, 0,  0, 7, 9
};
</code></pre>
<p>Here’s our serial C++ solution.</p>
<pre><code class="language-cpp">auto isgood(const std::array&lt;int, 81&gt; &amp;board) -&gt; bool {
  auto ar = std::vector&lt;int&gt;(9 * 9 * 3, 0);
  for (int r = 0; r &lt; 9; r++)
    for (int c = 0; c &lt; 9; c++) {
      const auto v = board[idx2(r, c)];
      if (0 == v)
        continue;
      ar[idx3(r, v - 1, 0)] += 1;
      ar[idx3(c, v - 1, 1)] += 1;
      const auto bi = (r / blksz) * blksz + (c / blksz);
      ar[idx3(bi, v - 1, 2)] += 1;
    }
  const auto m =
    std::accumulate(ar.begin(), ar.end(), -1, max);
  return m &lt; 2;
}
</code></pre>
<p>You can see pretty much everything about our solution is the same so far.
You may notice the <code>idx2</code> and <code>idx3</code> functions - these just calculate the linear index from subscripts so we can almost use 2d and 3d subscripts while keeping our arrays totally linear.</p>
<p>Here’s our <code>idx2</code> function for example. I’ll be using these functions for the rest of my solutoins since they make the code much more readable.</p>
<pre><code class="language-cpp">int idx2(int r, int c) {
  return (r * 9) + c;
};
</code></pre>
<p>Here at the end I find the max value again and check to make sure it’s less than two:</p>
<pre><code class="language-cpp">  const auto m = 
    std::accumulate(ar.begin(), ar.end(), -1, max);
  return m &lt; 2;
</code></pre>
<p>Running our executable gives us the same answers as our previous implementations:</p>
<pre><code class="language-console">$ ./src/cpp/lc-valid-sudoku
true
true
false
false
</code></pre>
<h2 id="c-and-mpi"><a class="header" href="#c-and-mpi"><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#content">C++ And MPI</a></a></h2>
<p>Here we have our MPI distributed C++ solution, let’s walk through it in a few steps.</p>
<pre><code class="language-cpp">auto isgood(const std::array&lt;int, 81&gt; &amp;board, int rank, int size) -&gt; bool {
  const auto chunk = (81 + size - 1) / size;
  auto ar = std::vector&lt;int&gt;(81 * 3, 0);
  auto indices = std::vector&lt;std::pair&lt;int, int&gt;&gt;(81 * size, std::make_pair(-1, -1));
  for (int r = 0; r &lt; 9; r++)
    for (int c = 0; c &lt; 9; c++)
      indices[idx2(r, c)] = std::make_pair(r, c);
  for (std::size_t i = chunk * rank; i &lt; chunk + (chunk * rank); i++) {
    const auto &amp;[r, c] = indices[i];
    const auto v = board[idx2(r, c)];
    if (r &lt; 0 or 0 == v) continue;
    ar[idx3(r, v - 1, 0)] += 1;
    ar[idx3(c, v - 1, 1)] += 1;
    const auto bi = (r / blksz) * blksz + (c / blksz);
    ar[idx3(bi, v - 1, 2)] += 1;
  }
  std::vector&lt;int&gt; gar(9 * 9 * 3, 0);
  MPI_Reduce(ar.data(), gar.data(), gar.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
  return 0 == rank ? std::accumulate(gar.begin(), gar.end(), -1, std::max) &lt; 2
                   : false;
}
</code></pre>
<p>All the setup is the same between the last several solutions.</p>
<p>Astute viewers may recognize this as a cartesian product, but I couldn’t find a nice way to do this with the STL algorithms.
If any viewers know of a nicer way to generate the cartesian product of two containers, please let me know.</p>
<pre><code class="language-cpp">  for (int r = 0; r &lt; 9; r++)
    for (int c = 0; c &lt; 9; c++)
      indices[idx2(r, c)] = std::make_pair(r, c);
</code></pre>
<p>The core loop is much the same as our other solutions, aside from unpacking the row and column as a tuple.</p>
<pre><code class="language-cpp">  for (std::size_t i = chunk * rank; i &lt; chunk + (chunk * rank); i++) {
    const auto &amp;[r, c] = indices[i];
    const auto v = board[idx2(r, c)];
    if (r &lt; 0 or 0 == v)
      continue;
    ar[idx3(r, v - 1, 0)] += 1;
    ar[idx3(c, v - 1, 1)] += 1;
    const auto bi = (r / 3) * 3 + (c / 3);
    ar[idx3(bi, v - 1, 2)] += 1;
  }
</code></pre>
<p>This section is exactly equivilant to the Python version below.
This should give you an idea of what it’s like to use the raw C and Fortran interfaces to MPI.</p>
<pre><code class="language-cpp">  std::vector&lt;int&gt; gar(9 * 9 * 3, 0);
  MPI_Reduce(ar.data(), gar.data(), gar.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
  return 0 == rank ? std::accumulate(gar.begin(), gar.end(), -1, sb::max) &lt; 2
                   : false;
</code></pre>
<p>Python version:</p>
<pre><code class="language-python">    gar = np.zeros((9 * 9 * 3,), dtype=np.int64)
    comm.Reduce([ar.flatten(), MPI.INT], [gar, MPI.INT], op=MPI.SUM, root=0)
    comm.Barrier()
    return max(gar.flatten()) &lt; 2 if 0 == comm.rank else False
</code></pre>
<p>In my main function I iterate over the same boards and use some extra logic so we only see the results that rank 0 gave back:</p>
<pre><code class="language-cpp">int main(int argc, char **argv) {
  int size, rank;
  MPI_Init(&amp;argc, &amp;argv);
  MPI_Comm comm = MPI_COMM_WORLD;
  MPI_Comm_size(comm, &amp;size);
  MPI_Comm_rank(comm, &amp;rank);
  for (const auto &amp;board : all_boards) {
    bool ret;
    if (0 == rank) {
      ret = isgood(board, rank, size);
      std::cout &lt;&lt; bool2str(ret) &lt;&lt; "\n";
    } else isgood(board, rank, size);
    MPI_Barrier(comm);
  }
  MPI_Finalize();
  return 0;
}
</code></pre>
<p>Running this works just as all our previous solutions did:</p>
<pre><code class="language-console">$ mpirun -n 5 ./src/cpp/lc-valid-sudoku-mpi
true
true
false
false
</code></pre>
<p>We’ll now take a look at our CUDA-enabled solution.</p>
<h2 id="c-and-cuda"><a class="header" href="#c-and-cuda"><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#content">C++ And CUDA</a></a></h2>
<p>Here’s our single-process CUDA implementation.
I for the most part am using raw CUDA, but I use a few helper methods from Thrust as well, such as the type-safe device malloc and free and some pointer-casting methods.
For those that are unfamiliar, the funny-looking function calls with the triple braces are how you launch a raw cuda kernel.
These allow you to pass arguments to the CUDA runtime to let it know how you’d like your CUDA kernel to be launched.</p>
<pre><code class="language-cpp">auto isgood(const Board &amp;board) -&gt; bool {
  auto d_ar = device_malloc&lt;int&gt;(81 * 3);
  setar&lt;&lt;&lt;1, dim3(9, 9)&gt;&gt;&gt;(
      raw_pointer_cast(d_ar), 
      raw_pointer_cast((thrust::device_vector&lt;int&gt;(board.begin(), board.end())).data()));
  cudaDeviceSynchronize();
  const auto m = thrust::reduce(d_ar, d_ar+(81*3), -1, thrust::maximum&lt;int&gt;());
  device_free(d_ar);
  return m &lt; 2;
}
</code></pre>
<p>I have the following <code>using</code> statements to make the code a little more readable hopefully.</p>
<pre><code class="language-cpp">using thrust::device_malloc;
using thrust::device_free;
using thrust::device_vector;
using thrust::host_vector;
using thrust::raw_pointer_cast;
</code></pre>
<p>Along with the previous code that should look pretty familiar at this point, I define two other CUDA kernels.
The first is this short <code>setrc</code> kernel, which sets rows and columns based on the kernel launch parameters I pass.
This is a shortcut for a cartesian product of the rows and columns that runs on the GPU.</p>
<pre><code class="language-cpp">__global__ void setrc(int *rows, int *cols) {
  const int r = threadIdx.x, c = threadIdx.y;
  rows[idx2(r, c)] = r;
  cols[idx2(r, c)] = c;
}
</code></pre>
<p>The other kernel is this <code>setar</code> function, which is the same core kernel that’s been at the heart of all of our solutions so far.</p>
<pre><code class="language-cpp">__global__ void setar(int *ar, const int *board) {
  const auto row = threadIdx.x, col = threadIdx.y;
  const int value = board[idx2(row, col)];
  if (0 == value) return;
  atomicAdd(&amp;ar[idx3(row, value, 0)], 1);
  atomicAdd(&amp;ar[idx3(col, value, 1)], 1);
  const int bi = (row / blksz) * blksz + (col / blksz);
  atomicAdd(&amp;ar[idx3(bi, value, 2)], 1);
}
</code></pre>
<p>Outside of those two kernels, the solution should look pretty familiar at this point.
We allocate our final array and pass it to our cuda kernel, along with the sudoku board after copying it to the GPU.</p>
<pre><code class="language-cpp">  auto d_ar = device_malloc&lt;int&gt;(81 * 3);
  setar&lt;&lt;&lt;1, dim3(9, 9)&gt;&gt;&gt;(
    raw_pointer_cast(d_ar),
    raw_pointer_cast(
      (device_vector&lt;int&gt;(board.begin(), board.end())).data()
    )
  );
</code></pre>
<p>We then syncronize with our GPU to make sure the kernel finishes before reducing to find the maximum value with <code>thrust::reduce</code>, freeing our device memory, and returning whether all values fell below two.</p>
<pre><code class="language-cpp">  cudaDeviceSynchronize();
  const auto m = thrust::reduce(d_ar, d_ar+(81*3), -1, thrust::maximum&lt;int&gt;());
  device_free(d_ar);
  return m &lt; 2;
</code></pre>
<p>Let’s move on to our most complex example, the C++ CUDA-enabled, MPI-distributed implementation.</p>
<h2 id="c-and-cuda-and-mpi"><a class="header" href="#c-and-cuda-and-mpi"><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#content">C++ And CUDA And MPI</a></a></h2>
<p>Now that we’re using two extra paradigms, CUDA GPU device offloading and MPI distributed computing, our code is looking more noisy.
It’s still pretty much the same solution as our non-distributed CUDA solution though.</p>
<pre><code class="language-cpp">auto isgood(const Board &amp;board, int rank, int size) -&gt; bool {
  const auto chunk = (81 + size - 1) / size;
  const auto rows = device_malloc&lt;int&gt;(chunk * size),
             cols = device_malloc&lt;int&gt;(chunk * size);
  thrust::fill(rows, rows + (chunk * size), -1);
  setrc&lt;&lt;&lt;1, dim3(9, 9)&gt;&gt;&gt;(raw_pointer_cast(rows), raw_pointer_cast(cols));
  auto d_ar = device_malloc&lt;int&gt;(81 * 3);
  thrust::fill(d_ar, d_ar + (81 * 3), 0);
  setar&lt;&lt;&lt;1, chunk&gt;&gt;&gt;(
      raw_pointer_cast(d_ar),
      raw_pointer_cast((device_vector&lt;int&gt;(board.begin(), board.end())).data()),
      raw_pointer_cast(rows), raw_pointer_cast(cols), chunk * rank);
  cudaDeviceSynchronize();
  auto h_ar = host_vector&lt;int&gt;(d_ar, d_ar + (81 * 3));
  auto gar = host_vector&lt;int&gt;(81 * 3, 0);
  MPI_Reduce(h_ar.data(), gar.data(), gar.size(), MPI_INT, MPI_SUM, 0,
             MPI_COMM_WORLD);
  device_free(rows); device_free(cols); device_free(d_ar);
  if (rank &gt; 0)
    return false;
  const auto m = thrust::reduce(thrust::host, gar.begin(), gar.end(), -1,
                                thrust::maximum&lt;int&gt;());
  return m &lt; 2;
}
</code></pre>
<p>The <code>setar</code> kernel is a bit different from our non distributed CUDA solution since we’re only operating on a subset of our sudoku board.
We set the values in our final sum matrix for the row, column and block submatrices just like before.
This time however, we’re given this <code>offset</code> parameter.
This is because we’re not just running CUDA kernels, we’re running CUDA kernels on multiple processes and potentially multiple machines, so we’re only performing a subset of the full set of operations.
This offset parameter tells us where we should start relative to the entire set of operations.
We’re also not using the builtin <code>threadIdx.y</code> value since we’re launching our kernel in a 1D grid with precalculated row and column indices instead of a 2D grid.</p>
<pre><code class="language-cpp">__global__ void setar(int *ar, const int *board, const int *rows,
                      const int *cols, const int offset) {
  const auto i = offset + threadIdx.x;
  const int r = rows[i], c = cols[i];
  const int value = board[idx2(r, c)];
  if (r &lt; 0 || 0 == value)
    return;
  atomicAdd(&amp;ar[idx3(r, value, 0)], 1);
  atomicAdd(&amp;ar[idx3(c, value, 1)], 1);
  const int bi = (r / blksz) * blksz + (c / blksz);
  atomicAdd(&amp;ar[idx3(bi, value, 2)], 1);
}
</code></pre>
<p>If we return to the start of our top-level function, you’ll see that we calculate the work that should be performed on this MPI process.
We also set up our row and column indices using our cartesian product kernel.</p>
<pre><code class="language-cpp">  const auto chunk = (81 + size - 1) / size;
  const auto rows = device_malloc&lt;int&gt;(chunk * size),
             cols = device_malloc&lt;int&gt;(chunk * size);
  thrust::fill(rows, rows + (chunk * size), -1);
  setrc&lt;&lt;&lt;1, dim3(9, 9)&gt;&gt;&gt;(raw_pointer_cast(rows), raw_pointer_cast(cols));
</code></pre>
<p>We then set up our final sum matrix on the device:</p>
<pre><code class="language-cpp">  auto d_ar = device_malloc&lt;int&gt;(81 * 3);
  thrust::fill(d_ar, d_ar + (81 * 3), 0);
</code></pre>
<p>And then launch our core kernel to perform the operations assigned to the current rank:</p>
<pre><code class="language-cpp">  setar&lt;&lt;&lt;1, chunk&gt;&gt;&gt;(
      raw_pointer_cast(d_ar),
      raw_pointer_cast((device_vector&lt;int&gt;(board.begin(), board.end())).data()),
      raw_pointer_cast(rows), raw_pointer_cast(cols), chunk * rank);
</code></pre>
<p>We syncronize with our GPU device and copy the data to a host vector before reducing the final sum array across all of our ranks using MPI.
Note that if we used a GPU-enabled MPI provider we could send the data on the device directly to another system’s GPU without copying the memory to the host, but this has other complications so I kept it simple for this example.</p>
<pre><code class="language-cpp">  cudaDeviceSynchronize();
  auto h_ar = host_vector&lt;int&gt;(d_ar, d_ar + (81 * 3));
  auto gar = host_vector&lt;int&gt;(81 * 3, 0);
  MPI_Reduce(h_ar.data(), gar.data(), gar.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
</code></pre>
<p>And then we perform our final reduction on our root rank to see if we have any cells with values greater than 1.
We could perform this reduction on the device, but it’s probably not worth it to copy the data back to the device for just one operation.</p>
<pre><code class="language-cpp">  if (rank &gt; 0)
    return false;
  const auto m = thrust::reduce(thrust::host, gar.begin(), gar.end(), -1,
                                thrust::maximum&lt;int&gt;());
  return m &lt; 2;
</code></pre>
<p>And there we have it, our sudoku validator is running on multiple processes and using GPUs.</p>
<pre><code class="language-console">$ mpirun -n 7 ./src/thrust/lc-valid-sudoku-mpi-thrust
true
true
false
false
</code></pre>
<p>Now let’s move on to Fortran.</p>
<h2 id="fortran-1"><a class="header" href="#fortran-1"><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#content">Fortran</a></a></h2>
<p>You’re likely not surprised that this looks a lot like our previous solutions.</p>
<pre><code class="language-fortran">subroutine isgood(board, ret)
  implicit none
  integer, dimension(0:(shape*shape)-1), intent(in) :: board
  logical, intent(out) :: ret
  integer, dimension(0:(shape * shape * 3)-1) :: ar
  integer :: v, row, col, i, bx, by
  ar = 0
  do row = 0, shape-1
    do col = 0, shape-1
      v = board(idx2(row, col))
      if (v .eq. 0) cycle
      ar(idx3(row, v-1, 0)) = ar(idx3(row, v-1, 0)) + 1      
      ar(idx3(col, v-1, 1)) = ar(idx3(col, v-1, 1)) + 1
      ar(idx3(bi(row, col), v-1, 2)) = ar(idx3(bi(row, col), v-1, 2)) + 1
    end do
  end do
  v = maxval(ar) - 1
  ret = (v .lt. 1)
end subroutine isgood
</code></pre>
<p>If I clear away the declarations and initializations, this looks fairly readable.
You may notice that I have to repeat myself a few times because there’s not a really nice way to incremenet a value in fortran.</p>
<pre><code class="language-fortran">subroutine isgood(board, ret)
  do row = 0, shape-1
    do col = 0, shape-1
      v = board(idx2(row, col))
      if (v .eq. 0) cycle
      ar(idx3(row, v-1, 0)) = ar(idx3(row, v-1, 0)) + 1      
      ar(idx3(col, v-1, 1)) = ar(idx3(col, v-1, 1)) + 1
      ar(idx3(bi(row, col), v-1, 2)) = ar(idx3(bi(row, col), v-1, 2)) + 1
    end do
  end do
  v = maxval(ar) - 1
  ret = (v .lt. 1)
end subroutine isgood
</code></pre>
<p>Now we move on to the MPI-distributed Fortran implementation.
This solution is pretty long so I’ll break the function into a few slides like a sliding window.</p>
<h2 id="fortran-and-mpi"><a class="header" href="#fortran-and-mpi"><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#content">Fortran And MPI</a></a></h2>
<p>Here is the full solution.</p>
<pre><code class="language-fortran">subroutine isgood(board, ret)
  use mpi
  implicit none
  integer, dimension(shape*shape), intent(in) :: board
  logical, intent(out) :: ret
  integer, dimension(shape * shape * 3) :: ar, gar
  integer, dimension(shape * shape) :: rows, cols
  integer :: v, row, col, i, chunk, rank, size, ierr

  ar = 0
  gar = 0

  call MPI_Comm_rank(MPI_COMM_WORLD, rank, ierr)
  call MPI_Comm_size(MPI_COMM_WORLD, size, ierr)

  do row = 0, shape-1
    do col = 0, shape-1
      rows(1+idx2(row, col)) = row
      cols(1+idx2(row, col)) = col
    end do
  end do

  chunk = ((shape*shape) + size - 1) / size

  do i = 1+(rank*chunk), (rank*chunk)+chunk
    if (i .gt. (shape*shape)) exit
    row = rows(i)
    col = cols(i)
    v = board(1+idx2(row, col))
    if (v .eq. 0) cycle
    ar(idx3(row, v-1, 0)+1) = ar(idx3(row, v-1, 0)+1) + 1      
    ar(idx3(col, v-1, 1)+1) = ar(idx3(col, v-1, 1)+1) + 1
    ar(idx3(bi(row, col), v-1, 2)+1) = ar(idx3(bi(row, col), v-1, 2)+1) + 1
  end do
  
  call MPI_Reduce(ar, gar, 3*shape*shape, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD, ierr)
  call MPI_Barrier(MPI_COMM_WORLD, ierr)

  if (0 .eq. rank) then
    v = maxval(gar) - 1
    ret = (v .lt. 1)
  else
    ret = .false.
  end if

end subroutine isgood
</code></pre>
<p>Let’s trim away the declarations and initializations again:</p>
<pre><code class="language-fortran">subroutine isgood(board, ret)
  do row = 0, 8
    do col = 0, 8
      rows(1+idx2(row, col)) = row
      cols(1+idx2(row, col)) = col
    end do
  end do
  chunk = (81 + size - 1) / size
  do i = 1+(rank*chunk), (rank*chunk)+chunk
    if (i .gt. 81) exit
    row = rows(i)
    col = cols(i)
    v = board(1+idx2(row, col))
    if (v .eq. 0) return
    ar(idx3(row, v-1, 0)+1) = ar(idx3(row, v-1, 0)+1) + 1      
    ar(idx3(col, v-1, 1)+1) = ar(idx3(col, v-1, 1)+1) + 1
    ar(idx3(bi(row, col), v-1, 2)+1) = ar(idx3(bi(row, col), v-1, 2)+1) + 1
  end do
  call MPI_Reduce(ar, gar, 3*81, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD, ierr)
  call MPI_Barrier(MPI_COMM_WORLD, ierr)
  if (0 .eq. rank) then
    v = maxval(gar) - 1
    ret = (v .lt. 1)
  else
    ret = .false.
  end if
end subroutine isgood
</code></pre>
<p>You’ll notice that I create row and column arrays again because this makes distributing the processes much simpler.</p>
<pre><code class="language-fortran">  do row = 0, 8
    do col = 0, 8
      rows(1+idx2(row, col)) = row
      cols(1+idx2(row, col)) = col
    end do
  end do
</code></pre>
<p>The core loop is the same as the other distributed solutions.
I work only on the rows and columns assigned to the current rank.</p>
<pre><code class="language-fortran">  do i = 1+(rank*chunk), (rank*chunk)+chunk
    if (i .gt. 81) exit
    row = rows(i)
    col = cols(i)
    v = board(1+idx2(row, col))
    if (v .eq. 0) return
    ar(idx3(row, v-1, 0)+1) = ar(idx3(row, v-1, 0)+1) + 1      
    ar(idx3(col, v-1, 1)+1) = ar(idx3(col, v-1, 1)+1) + 1
    ar(idx3(bi(row, col), v-1, 2)+1) = ar(idx3(bi(row, col), v-1, 2)+1) + 1
  end do
</code></pre>
<p>We reduce the solution across all of our ranks to get the full array on rank 0.
We then perform our max reduce to get our answer and we return!</p>
<pre><code class="language-fortran">  call MPI_Reduce(ar, gar, 3*81, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD, ierr)
  call MPI_Barrier(MPI_COMM_WORLD, ierr)
  if (0 .eq. rank) then
    v = maxval(gar)
    ret = (v .lt. 2)
  else
    ret = .false.
  end if
</code></pre>
<p>Running this gives us the answers we expect.</p>
<pre><code class="language-console">$ mpirun -n 7 ./src/fortran/lc-valid-sudoku-ftn-mpi
 Running with world size of           7
 T
 T
 F
 F
</code></pre>
<h2 id="conclusion-3"><a class="header" href="#conclusion-3"><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#content">Conclusion</a></a></h2>
<p>I hope you’ve all enjoyed this video and the foray into distributed computing in a few different programming languages.</p>
<p>{% include footer.html %}</p>
<h2 id="youtube-description"><a class="header" href="#youtube-description"><a href="csblog/2021-10-19-Leetcode-And-Distributed-Computing.html#content">YouTube Description</a></a></h2>
<p>We solve a Leetcode problem in four languages using various combinations of MPI and CUDA!</p>
<ul>
<li>
<p>0:00 Problem Introduction</p>
</li>
<li>
<p>0:36 BQN Solution</p>
</li>
<li>
<p>2:07 Solution Strategy</p>
</li>
<li>
<p>4:54 Python Solution</p>
</li>
<li>
<p>5:42 Python &amp; MPI Solution</p>
</li>
<li>
<p>8:01 C++ Solution</p>
</li>
<li>
<p>8:55 C++ &amp; MPI Solution</p>
</li>
<li>
<p>9:58 C++ &amp; CUDA Solution</p>
</li>
<li>
<p>11:24 C++ &amp; MPI &amp; CUDA Solution</p>
</li>
<li>
<p>13:31 Fortran Solution</p>
</li>
<li>
<p>13:55 Fortran &amp; MPI Solution</p>
</li>
<li>
<p>14:38 Conclusion</p>
</li>
<li>
<p>Written version: http://www.ashermancinelli.com/leetcode-distributed-computing</p>
</li>
<li>
<p>LinkedIn: https://www.linkedin.com/in/asher-mancinelli-bb4a56144/</p>
</li>
<li>
<p>GitHub Repo for Examples: https://github.com/ashermancinelli/algorithm-testbed</p>
</li>
</ul>
<!--
layout: post
title: BQN and CUDA C++ LeetCode Solutions
permalink: /bqn-cuda-cpp-lc-longest-valid-parens
category: bqn, c++, cuda, leetcode
wip: false
cat: cs
-->
<p>Solving a hard leetcode problem in the BQN APL dialect and CUDA C++!</p>
<p><em>NOTE: This post is a transcript of <a href="https://youtu.be/3D7sfXzBBXE">the youtube video linked here</a>.</em></p>
<h2 id="problem-1"><a class="header" href="#problem-1">Problem</a></h2>
<p>Hello everyone, today I’d like to go through two solutions to a LeetCode problem.
We’ll first look at the solution with the BQN array language, and then we’ll look at a GPU-capable solution in CUDA that uses the Thrust template library.</p>
<p><a href="https://leetcode.com/problems/longest-valid-parentheses/">Link here</a>.</p>
<p>Given a string containing just the characters ‘(’ and ‘)’, find the length of the longest valid parentheses substring.</p>
<p>For example, for the string <code>")()())"</code> the expected answer is 4, and for this string the expected answer is two: <code>"())"</code>. Of course, for an empty string the answer is 0.</p>
<p>We’ll be looking at the solution in BQN first.</p>
<h2 id="bqnapl-solution"><a class="header" href="#bqnapl-solution">BQN/APL Solution</a></h2>
<p>Here is the full solution:</p>
<pre><code>   F ← {0⌈1+⌈´⌈´¨∾¨1↓¨⊔¨0=+`¨1-˜¨2×¨↓")("⊸⊐ 𝕩}
   F ")()())"
4
   F "(()"
2
   F ""
0
</code></pre>
<p>I take the index into the string <code>")("</code> to convert to integers and I take all the prefixes of that array.</p>
<pre><code>   {↓")("⊸⊐ 𝕩} "(()"
⟨ ⟨ 1 1 0 ⟩ ⟨ 1 0 ⟩ ⟨ 0 ⟩ ⟨⟩ ⟩
</code></pre>
<p>I then multiply by two and subtract one so each index represents the change in the level of nesting at that index.</p>
<pre><code>   {1-˜¨2×¨↓")("⊸⊐ 𝕩} "(()"
⟨ ⟨ 1 1 ¯1 ⟩ ⟨ 1 ¯1 ⟩ ⟨ ¯1 ⟩ ⟨⟩ ⟩
</code></pre>
<p>I then plus-scan to find the cumulative level of nesting up to that index for each prefix of the array:</p>
<pre><code>   {+`¨1-˜¨2×¨↓")("⊸⊐ 𝕩} "(()"
⟨ ⟨ 1 2 1 ⟩ ⟨ 1 0 ⟩ ⟨ ¯1 ⟩ ⟨⟩ ⟩
</code></pre>
<p>Find the zeros in each prefix, since these are the locations where the substring is balanced:</p>
<pre><code>   {0=+`¨1-˜¨2×¨↓")("⊸⊐ 𝕩} "(()"
⟨ ⟨ 0 0 0 ⟩ ⟨ 0 1 ⟩ ⟨ 0 ⟩ ⟨⟩ ⟩
</code></pre>
<p>We can then group the results to find the indices which are nonbalanced and balanced for each prefix:</p>
<pre><code>   {⊔¨0=+`¨1-˜¨2×¨↓")("⊸⊐ 𝕩} "(()"
┌─                                            
· ⟨ ⟨ 0 1 2 ⟩ ⟩ ⟨ ⟨ 0 ⟩ ⟨ 1 ⟩ ⟩ ⟨ ⟨ 0 ⟩ ⟩ ⟨⟩  
                                             ┘
   {⊔¨0=+`¨1-˜¨2×¨↓")("⊸⊐ 𝕩} ")()())" # Longer problem
┌─                                                                                                              
· ⟨ ⟨ 0 2 4 5 ⟩ ⟨ 1 3 ⟩ ⟩ ⟨ ⟨ 0 2 4 ⟩ ⟨ 1 3 ⟩ ⟩ ⟨ ⟨ 0 2 3 ⟩ ⟨ 1 ⟩ ⟩ ⟨ ⟨ 0 2 ⟩ ⟨ 1 ⟩ ⟩ ⟨ ⟨ 0 1 ⟩ ⟩ ⟨ ⟨ 0 ⟩ ⟩ ⟨⟩  
                                                                                                               ┘  
</code></pre>
<p>We can then of course drop the first list in each prefix so we only have the balanced indices. I’ll switch to the longer problem here so it’s a little easier to see what’s happening:</p>
<pre><code>   {1↓¨⊔¨0=+`¨1-˜¨2×¨↓")("⊸⊐ 𝕩} ")()())"
┌─                                                      
· ⟨ ⟨ 1 3 ⟩ ⟩ ⟨ ⟨ 1 3 ⟩ ⟩ ⟨ ⟨ 1 ⟩ ⟩ ⟨ ⟨ 1 ⟩ ⟩ ⟨⟩ ⟨⟩ ⟨⟩  
                                                       ┘
</code></pre>
<p>We can then flatten the sublists together and find the largest element, which represents the index in a given prefix with the longest valid substring:</p>
<pre><code>   {⌈´⌈´¨∾¨1↓¨⊔¨0=+`¨1-˜¨2×¨↓")("⊸⊐ 𝕩} ")()())"
3
</code></pre>
<p>Because we are using 0-based indices as God intended, we’ll have to add one to the result.
We’ll also take the maximum of our result and 0 in case no balanced substrings were found, which would otherwise give us <code>¯∞</code>:</p>
<pre><code>   {0⌈1+⌈´⌈´¨∾¨1↓¨⊔¨0=+`¨1-˜¨2×¨↓")("⊸⊐ 𝕩} ")()())"
4
</code></pre>
<p>Finally, let’s look at all the test cases:</p>
<pre><code>   F ← {0⌈1+⌈´⌈´¨∾¨1↓¨⊔¨0=+`¨1-˜¨2×¨↓")("⊸⊐ 𝕩}
   F ")()())"
4
   F "(()"
2
   F ""
0
</code></pre>
<p>Now that we’ve gone through the BQN solution, let’s take a look at the CUDA and Thrust solution</p>
<h2 id="cudathrust-solution"><a class="header" href="#cudathrust-solution">CUDA/Thrust Solution</a></h2>
<p>Here is the full solution, minus some includes and using statements:</p>
<pre><code class="language-cpp">auto solve(const string&amp; problem) -&gt; int {
  const int N = problem.size();
  if (0 == N)
    return 0;

  host_vector&lt;int&gt; mapping;
  mapping.reserve(N);
  std::transform(problem.begin(), problem.end(), std::back_inserter(mapping),
                 [=](const char &amp;c) { return c == '(' ? 1 : -1; });
  device_vector&lt;int&gt; d_mapping = mapping;

  vector&lt;int&gt; starts(N - 1);
  std::iota(starts.begin(), starts.end(), 0);

  int max_len = std::accumulate(
      starts.begin(), starts.end(), 0,
      [&amp;d_mapping, N](int max_so_far, int i) {
        device_vector&lt;int&gt; prefix(N-i);
        thrust::inclusive_scan(d_mapping.begin()+i, d_mapping.end(), prefix.begin());

        device_vector&lt;int&gt; indices(N - i);
        thrust::sequence(indices.begin(), indices.end(), 0);

        auto zip_start = thrust::make_zip_iterator(
            thrust::make_tuple(prefix.begin(), indices.begin()));
        auto zip_end = thrust::make_zip_iterator(
            thrust::make_tuple(prefix.end(), indices.end()));

        int max_for_prefix = thrust::transform_reduce(
            zip_start, zip_end,
            [=] __device__(const auto &amp;tup) -&gt; int {
              return thrust::get&lt;0&gt;(tup) == 0 ? 1 + thrust::get&lt;1&gt;(tup) : 0;
            },
            0, thrust::maximum&lt;int&gt;());

        return std::max(max_so_far, max_for_prefix);
      });

  return max_len;
}

int main() {
  for (const string &amp;problem : { ")()())", "(()", "" })
    std::cout &lt;&lt; solve(problem) &lt;&lt; "\n";
  return 0;
}
</code></pre>
<p>This is quite a lot to take in, so let’s break it down.</p>
<p>First I grab the problem size so I don’t have to keep repeating myself, and I check to make sure our problem size is greater than zero.
I then transform the string into integers and copy the data to the GPU device.
This step is just like the bqn solution up until this point.</p>
<pre><code class="language-cpp">  const int N = problem.size();
  if (0 == N)
    return 0;

  host_vector&lt;int&gt; mapping;
  mapping.reserve(N);
  std::transform(problem.begin(), problem.end(), std::back_inserter(mapping),
                 [=](const char &amp;c) { return c == '(' ? 1 : -1; });
  device_vector&lt;int&gt; d_mapping = mapping;
</code></pre>
<p>In BQN:</p>
<pre><code>{1-˜¨2×¨↓")("⊸⊐ 𝕩}
</code></pre>
<p>I then create an STL vector to hold the starting positions for each prefix.
I’m using the STL here instead of Thrust because I’ll otherwise have to nest my CUDA calls, and not all of the Thrust API is callable on the GPU device.
Ideally, we fit as much of our algorithm onto the GPU device to minimize any data transfer between memory spaces, but I still ended up using a mixture of the STL and Thrust.</p>
<pre><code class="language-cpp">  vector&lt;int&gt; starts(N - 1);
  std::iota(starts.begin(), starts.end(), 0);
</code></pre>
<p>Because of how the stl algorithms are used, we have to now go to the end of our
BQN solution. This call to accumulate corresponds to our outter reduction in our BQN solution here:</p>
<pre><code class="language-cpp">  // BQN) F ← {0⌈1+⌈´⌈´¨∾¨1↓¨⊔¨0=+`¨1-˜¨2×¨↓")("⊸⊐ 𝕩}
  //               ^
  //              here

  int max_len = std::accumulate(
      starts.begin(), starts.end(), 0,
      [&amp;d_mapping, N](int max_so_far, int i) {
        // ...
      });
</code></pre>
<p>We’re reducing over the maximum balanced substring for each prefix of the input string.</p>
<p>Next I create a device vector for the given prefix, and take the prefix sum of the current prefix.</p>
<pre><code class="language-cpp">  // BQN) +`
  int max_len = std::accumulate(...
        device_vector&lt;int&gt; prefix(N-i);
        thrust::inclusive_scan(d_mapping.begin()+i, d_mapping.end(), prefix.begin());
</code></pre>
<p>I then create an <em>iota</em> to zip with our prefix-summed substring (or a <em>range</em> in BQN parlance, or a <em>sequence</em> in Thrust parlance (can’t we all just agree on a term here…)):</p>
<pre><code class="language-cpp">        device_vector&lt;int&gt; indices(N - i);
        thrust::sequence(indices.begin(), indices.end(), 0);

        auto zip_start = thrust::make_zip_iterator(
            thrust::make_tuple(prefix.begin(), indices.begin()));
        auto zip_end = thrust::make_zip_iterator(
            thrust::make_tuple(prefix.end(), indices.end()));        
</code></pre>
<p>This corresponds to the <em>couple</em> dyad in BQN or the <em>zip</em> function in Python and lots of functional languages.</p>
<p>I then perform two algorithms in this one step. If the given position in the prefix-summed substring is zero, that means it’s balanced and I want to keep the index.
Otherwise, I can just throw it out.
After performing this transform or map algorithm, I take the max reduction of the substring to find the greatest index at which the substring is balanced.
If there are multiple points in the substring where the parens are balanced, this will find the greatest one.</p>
<pre><code class="language-cpp">        int max_for_prefix = thrust::transform_reduce(
            zip_start, zip_end,
            [=] __device__(const auto &amp;tup) -&gt; int {
              return thrust::get&lt;0&gt;(tup) == 0 ? 1 + thrust::get&lt;1&gt;(tup) : 0;
            },
            0, thrust::maximum&lt;int&gt;());
</code></pre>
<p>I then return the maximum balanced substring for the current prefix, which is then folded in the outter <code>std::accumulate</code> to find the greatest balanced substring for all prefixes in the original string.</p>
<pre><code class="language-cpp">  int max_len = std::accumulate(...
        [...](int max_so_far, int i) {
            int max_for_prefix = ...
            return std::max(max_so_far, max_for_prefix);
        });
</code></pre>
<p>I then return the maximum length I found, and we have our answer!</p>
<pre><code class="language-cpp">auto solve(const string&amp; problem) -&gt; int {
  ...
  int max_len = std::accumulate(...);
  return max_len;
}
</code></pre>
<p>I ran this with the same test cases like so:</p>
<pre><code class="language-cpp">int main() {
  for (const string &amp;problem : { ")()())", "(()", "" })
    std::cout &lt;&lt; solve(problem) &lt;&lt; "\n";
  return 0;
}
</code></pre>
<p>And running gave me:</p>
<pre><code class="language-console">$ ./src/thrust/lc-longest-valid-parens
4
2
0
</code></pre>
<p>Just like we expected!</p>
<h2 id="youtube-video-description"><a class="header" href="#youtube-video-description">YouTube Video Description</a></h2>
<p>We solve a hard leetcode problem in both BQN and CUDA C++ with the Thrust library.</p>
<h2 id="conclusion-4"><a class="header" href="#conclusion-4">Conclusion</a></h2>
<p>Thanks for tuning in and I hope you enjoyed this example program.
You can find all the GPU examples I used in the links below.
Connor Hoekstra, if you’re reading or watching this, I hope to see you out-do my BQN and CUDA solutions in another video :).</p>
<p>{% include footer.html %}</p>
<h2 id="links-1"><a class="header" href="#links-1">Links</a></h2>
<ul>
<li><a href="https://github.com/ashermancinelli/portable-alg-testbed">Repo for GPU Examples</a></li>
<li><a href="https://github.com/ashermancinelli/apl-snippets">Repo for BQN/APL Examples</a></li>
<li><a href="https://www.youtube.com/channel/UCZ5sL4E662VP1ZwC4h85ttQ">YouTube Channel</a></li>
<li><a href="http://www.ashermancinelli.com/">Blog</a></li>
<li><a href="https://www.linkedin.com/in/asher-mancinelli-bb4a56144/">LinkedIn</a></li>
<li><a href="https://leetcode.com/problems/longest-valid-parentheses/">LeetCode Problem</a></li>
</ul>
<!--
layout: post
title: Hunt for the Best Espresso in Portland
permalink: /espresso-pdx
cat: coffee
tags: 
-->
<p>Finding the best espresso in one of the homes of 3rd-wave coffee.</p>
<h2 id="the-plan"><a class="header" href="#the-plan">The Plan</a></h2>
<p>In the spring and summer of 2023 I started a list of coffee shops to evaluate.
By the end of the summer I hope to have established a rough ranking of the best shops,
along with some honorable mentions for great shops to work from.</p>
<p>Here are some of the early contenders:</p>
<center>
  <img
    src="/images/coffee/courier.png"
    />
</center>
<p>The second shop to get a 10/10 on my arbitrary rating system blew me away.
I’m not usually a huge fan of fruity espresso, but the double-fermented Las Frutas roasted in-house was the best espresso I’ve had to date.
The columbian beans were sealed in water bags for 72 hours before another 30 hour wet-tank soak were incredible.
The roaster and barista recomended Sterling Coffee Roasters for my next espresso, so I’m hopeful that Sterling will rate pretty highly as well.</p>
<center>
  <img
    src="/images/coffee/ovation.png"
    />
</center>
<p>Ovation Coffee and Tea was the first shop to be awarded a perfect 10/10.
The location in PSU’s campus right off the North-South streetcar line has plenty of space to work and REALLY cute porcelein cups.</p>
<h2 id="behind-the-museum"><a class="header" href="#behind-the-museum">Behind the Museum</a></h2>
<p>The Behind the Museum cafe’s beautiful floor-to-ceiling windows and Japanese pottery along the walls make for an
especially nice work environment, especially when it’s raining -
the rain rolling off those massive windows immediately puts you at ease.</p>
<!--
where: Portland, OR
layout: post
title: Sterling - 10/10
permalink: /sterling
cat: coffee
...

This place is something special. *Please* don't leave Portland without trying their espresso.

-->
<p><em>6/21/23 update:</em></p>
<p>In my second visit, I had another Etheopian roast, the <em>Sidama Setamo</em>, an Etheopian washed-process SO Heirloom roast with bergamot and wild cherries as flavor comps.
The barista compared the roast to Fruit Loops, which felt far closer to my experience than the listed comps; I also got notes of green apple.</p>
<p>I’m determined to try every roast this place puts on espresso;
if I ever move out of Portland I’m going to kick myself for not having as much of this espresso as I can.
This may be consistently the best espresso I’ll ever have.</p>
<p>I got to sit with Eric (the owner) for a minute today and ask a few questions about Sterling.
He started the operation in 2005 (under a different name I believe) and now has a team working at the roaster while he spends most of his time at the cafe interacting with customers and tasting roasts.</p>
<p>He spoke really highly of Keeper cafe (which I had never heard of) as well as Barista (which I tried later that day) as well as Courier, which is currently somewhere in my top 5 cafes &amp; roasters in Portland.
He also referred to Vivace in Seattle as the Mecca of the 90s and 2000s coffee culture, but lamented the lack of innovation he’s seen in Seattle’s coffee scene in the years since; he used to take new employees up to thier roaster to learn the trade.
Same with Slate roasters, who have since closed their doors.</p>
<p>I hope to set up a more formal interview with Eric sometime to tell Sterling’s story.</p>
<p>–&gt;
<em>6/14/23 (original review):</em></p>
<p>Prior to trying Sterling’s espresso, the #1 on my list was from Courier Coffee.
When I told the barista at Courier how much I enjoyed their espresso, he made sure to recommend Sterling for my next stop.</p>
<blockquote>
<p>Oh, you like espresso? Yeah, you should definitely check out Sterling</p>
</blockquote>
<p>He backed that comment up with a few more remarks about how special their espresso is, and it went to the top of my list of places to try next.
It did not dissapoint.</p>
<p>–&gt;</p>
<p>The barista was excited to welcome me in and tell me all about the roast they had ready for espresso, which happened to be the <em>Nigusse Lemma</em>, a natural-process small-batch Ethiopian roast with Etheopian Heriloom varietals.
I got the same feelings I get trying a new whiskey at the Scotch Lounge on the East side of town - she talked through the roasting process at their East-side roastery and the origins of the beans.</p>
<p>After pulling the shot, she put together a board with a small glass of sparkling water as a pallate clenser (as is usual in espresso-focused cafes).</p>
<p>Adding to the Scotch Lounge feelings, the espresso was served in a glencairn-style whiskey serving glass along with a third glass with a drip brew so as to try the roast from another perspective.</p>
<p>Both coffees were smooth and creamy - I jotted down <em>notes of strawberries and cream</em> before I even saw <em>“Raspberry &amp; Vanilla Cream”</em> listed as flavor comps on the bag of coffee they used for my espresso.</p>
<p>–&gt;</p>
<p>This espresso struck me as <em><strong>perfectly balanced</strong></em> - without hesitation, I wanted to share it with every one of my friends still not won over by espresso.
In my experience, when people try espresso and decide they hate it, 95% of the time it’s due to the intensity of the flavors, especially with sour and bitter notes.
When I try an intense espresso, something that punches you in the mouth with sourness or fruit or bitterness or chocolate or nuttiness, I can enjoy the intensity (if it’s done well!) - but that seems to be the attribute of espresso that puts people off of it.</p>
<p>This espresso was not intense in the usual way, but not too mild either.
It was full of flavor with delicious notes of citrus, cream, a balance of bitterness to round out the flavor profile, yet not too intense to be someone’s first espresso.</p>
<p>This espresso, along with their drip brew, the knowledgable baristas, and a cozy cafe seating area nestled into NW Portland with indoor trees and fast wifi brings this to the top of my list.
The barista who served me happened to recommend <em>Barista</em> for one of my next espresso destinations which I haven’t had the opportunity to try yet - I look forward to posting a review given that the current champion of my list thinks I should give it a try.</p>
<p>As I work out an espresso tour of Portland, I <em>have</em> to pencil Sterling in as the final destination.
I would want any espresso afficionados visiting the city to finish their time here with a Sterling espresso.</p>
<p>–&gt;</p>
<h3>
    <center>
    <a href="https://www.sterling.coffee/" target="blank">
    Sterling Coffee homepage
    </a>
    </center>
</h3>
<!--
where: Portland, OR
layout: post
title: DEADSTOCK Coffee - 9.5/10
permalink: /deadstock
cat: coffee
-->
<p><em><strong>“Coffee should be dope”</strong></em></p>
<p>–&gt;</p>
<p>Nestled in central Chinatown, this cafe front for a roastery has their own streetwear line and feels like a trap-house recording studio.
Self-termed <em><strong>Portland’s Hype Barista</strong></em>, Nike Jordan 1s are hung up everywhere and trap plays over the speakers all day.</p>
<p>The <em>BREEZY Don Enrique</em> Colombian single-origin medium roast was <em>seriously</em> good.
As soon as I finish making this espresso tour, I plan on coming back here to buy a bag or two of the BREEZY - it was one of the best-smelling roasts I’ve come across in the last year.</p>
<p>I haven’t seen DEADSTOCK mentioned on many blogs, which feels like an injustice.
Definitely worth stopping by.</p>
<p>–&gt;</p>
<h3>
    <center>
    <a href="https://deadstockcoffee.com" target="blank">
    Deadstock Coffee homepage
    </a>
    </center>
</h3>
<!--
layout: post
title: Barista - 9/10
permalink: /barista
where: Portland, OR
cat: coffee
-->
<p>Solid option in NW Portland with a variety of local roasts, not just their own.</p>
<p>–&gt;</p>
<p>I had the spirit lifter blend from Puff roasters, which was unfortunately out of stock in the consumer-sized bags, so I was unable to see the actual roast level, flavor comps, etc.
It tasted like a medium roast with some very mild sweetness, almost like a vanilla tea; very clear, very light body.</p>
<p>They had two other roasts on espresso; this is a thoughtful cafe with interesting roasts.
I plan on returning to rate all three roasts they have available, and I’ll keep my eyes on what new roasts they bring into the shop.</p>
<p>Although they roast their own coffee, they also bring in beans from other local roasters.
Sourcing and serving the best local beans is their mission statement, as listed on their website.</p>
<p>–&gt;</p>
<h3>
    <center>
    <a href="https://baristapdx.com/" target="blank">
    Barista homepage
    </a>
    </center>
</h3>
<!--
where: Portland, OR
layout: post
title: Never Coffee - 5/10
permalink: /never-coffee
cat: coffee
-->
<p>Not my favorite, but it might be yours.</p>
<p>The Never Coffee cafe on SW 12th and Alder is one of the nicest cafe environments I’ve found so far, with a modern feel and pastel color palette.
They got a low score from me simply because their espresso was not my favorite - it felt a little bland, not overly fruity or dark and rich, somewhere in the middle with less flavor than I was hoping for.</p>
<p>I had their Bangarang roast, a blend of washed Kenyan, washed Peruvian, and honey-process Honduran beans, which the barista was more than happy to tell me about - they clearly care about coffee, roast in-house, and take creative steps with their drinks, but their espresso was not for me and that’s my measuring stick.</p>
<p>Other reviews of Never Coffee call out their lattes as very above-average, which seems to be the case; almost every single person I saw in the cafe in the morning I spent there chose one of their 5 unique latte flavors, and I was the only one with a doppio.</p>
<p>If you stop by, maybe try one of their lattes.</p>
<p><a href="https://nevercoffeelab.com/">Never Coffee homepage</a></p>
<!--
layout: post
title: Upper Left Roasters - 9.5/10
permalink: /upperleft
cat: coffee
where: Portland, OR
-->
<p>Unique energy, passionate about espresso.</p>
<p>–&gt;</p>
<p>Named for it’s location relative to the Ladd’s Edition neighborhood in SE Portland, this roaster and cafe front has <em>several</em> roasts available for espresso at any given time (and one or two more specifically for drip and pour-over).</p>
<p>The flavor comps for the roast I tried were absolutely spot-on: I got strong notes of sweetness and white chocolate from the <em>90h Tabi</em>.
These Tabi beans from Tolima, Colombia underwent a 90-hour submerged fermentation process.
I hadn’t heard of the Tabi variety before: it’s a hybrid of bourbon, typica, and timor varietials.
<strong>
Upper Left is <em>clearly</em> passionate about espresso, but has something for everyone: more than half of the patrons were sporting iced lattes on the Friday afternoon I stopped in.
</strong></p>
<p>The venue is very unique as well: the roasting machine is separated from the cafe by only a bar seating area; you can sit right next to the roasting machine if you like.
The white walls, light wooden furnishing, and ample indoor and outdoor seating made for a wonderful environment.</p>
<p>Speaking of the outdoor seating, there was a DJ playing lowkey club mixes over industrial-sized speakers in the outdoor seating area which happened to be almost entirely full; the neighborhood clearly takes full advantage of having this cafe so accessible.</p>
<p>Upper Left is a must-try.</p>
<p>–&gt;</p>
<h3>
    <center>
        <a href="https://upperleftroasters.com/" target="blank">
        Upper Left Roasters homepage
        </a>
    </center>
</h3>
<!--
where: Portland, OR
layout: post
title: Abba - 7/10
permalink: /abba-roasters
cat: coffee
-->
<p>Enjoyable, but not my first recomendation.</p>
<p>–&gt;</p>
<p>The Etheopian and Indonesian in-house medium roast (the <em>Collective Blend</em>) was subtle and tasty - I wasn’t smacked across the face with fruit or punched in the nose with dark-chocolately bitterness.
The notes on the package mentioned dark cherry and pear; I can see where they got <em>pear</em> but <em>grapefruit</em> is the predominant flavor that came to mind.</p>
<p>It was enjoyable! I would have it again, but I’m not rushing back or sending out recomendations to all my friends.</p>
<p>The <em>sea-salt iced matcha latte</em> the person in front of me ordered looked beautiful, and I heard several people mention how much they liked it - if you’re a matcha latte kinda person, this might be a fun stop.</p>
<p>–&gt;</p>
<h3>
    <center>
    <a href="https://www.abbacoffeeroasters.com" target="blank">
    Abba Coffee Roasters homepage
    </a>
    </center>
</h3>
<!--
layout: post
title: Rose City Coffee - 7/10
permalink: /rose-city
where: Portland, OR
cat: coffee
-->
<p>Do you like blonde roasts or need a beautiful space to get some work done?</p>
<p>–&gt;</p>
<p>The first thing to draw me in to a coffee shop is usually the smells - pastries, coffee, and maybe the nutty wooden smell of roasting if I’m lucky enough to be in a roastery.</p>
<p>Walking into Rose City Coffee however, I was struck by the dark cherry-wood and soft-touch-textured metal furnishing, the brick walls, and the wide-open cafe space - there are no walls or furniture to obstruct your view of one side of the cafe from the other, combining with the floor-to-celeing windows to make the space feel larger than it is.</p>
<p>–&gt;</p>
<p>I had the <em>Floribunda</em>, a medium blend with chocolate and floral flavor comps… and that’s about all I could find about the roast between asking in the shop and looking online.
This is obviously not an espresso specialty shop, but my doppio was still alright.</p>
<p>I got quite a bit of sourness up front, without much darkness or bitterness to balance it out.
If you’re a fiend for blonde roasts, this would be a pretty fun place to stop - but the menu seems to emphasize lattes, so I don’t know that I would recommend this spot to a huge fan of blonde <em>espresso</em>.</p>
<p><em>Personally</em>, I would have loved a darker roast and a <em>LOT</em> more information about the beans, but espresso isn’t every shop’s emphasis, and that’s alright too.
Their food options looked (and smelled) amazing as well.</p>
<p>Rose City will live in my head as the place to go if I need to stay put somewhere for all day - great food, decent coffee, and a free-feeling space that I could spend a whole workday at.</p>
<p>–&gt;</p>
<p>On top of the food, coffee, and environment, the friend that recommended this place to me remarked that they have live music on Saturdays!
The community events add to Rose City’s vibe, which leaves it feeling remarkably like the Flying M coffee shop in the Boise, Idaho area, my all-around favorite shop from my time living there.</p>
<p>The Flying M will always have a special place in my heart, so I can’t help but feel sentimental cracking open my laptop to get some work done in the refreshing Rose City seating area.
It’s worth stopping by if you’re in the SE part of the city (especially on a Saturday 😊).</p>
<p>–&gt;</p>
<h3>
    <center>
        <a href="https://www.rosecitycoffeecompany.com/" target="blank">
        Rose City Coffee homepage
        </a>
        <hr>
        <a href="https://www.flyingmcoffee.com/" target="blank">
        The Flying M homepage
        </a>
    </center>
</h3>
<!--
where: Portland, OR
layout: post
title: Sterling - 10/10
permalink: /sterling
cat: coffee
-->
<p>This place is something special. <em>Please</em> don’t leave Portland without trying their espresso.</p>
<p>–&gt;
<em>6/21/23 update:</em></p>
<p>In my second visit, I had another Etheopian roast, the <em>Sidama Setamo</em>, an Etheopian washed-process SO Heirloom roast with bergamot and wild cherries as flavor comps.
The barista compared the roast to Fruit Loops, which felt far closer to my experience than the listed comps; I also got notes of green apple.</p>
<p>I’m determined to try every roast this place puts on espresso.
If I ever move from Portland, I’m going to kick myself for not having as much of this espresso as I can.
This may be consistently the best espresso I’ll ever have.</p>
<p>–&gt;
<em>6/14/23 (original review):</em></p>
<p>Prior to trying Sterling’s espresso, the #1 on my list was from Courier Coffee.
When I told the barista at Courier how much I enjoyed their espresso, he made sure to recommend Sterling for my next stop.</p>
<blockquote>
<p>Oh, you like espresso? Yeah, you should definitely check out Sterling</p>
</blockquote>
<p>He backed that comment up with a few more remarks about how special their espresso is, and it went to the top of my list of places to try next.
It did not dissapoint.</p>
<p>–&gt;</p>
<p>The barista was excited to welcome me in and tell me all about the roast they had ready for espresso, which happened to be the <em>Nigusse Lemma</em>, a natural-process small-batch Ethiopian roast with Etheopian Heriloom varietals.
I got the same feelings I get trying a new whiskey at the Scotch Lounge on the East side of town - she talked through the roasting process at their East-side roastery and the origins of the beans.</p>
<p>After pulling the shot, she put together a board with a small glass of sparkling water as a pallate clenser (as is usual in espresso-focused cafes).</p>
<p>Adding to the Scotch Lounge feelings, the espresso was served in a glencairn-style whiskey serving glass along with a third glass with a drip brew so as to try the roast from another perspective.</p>
<p>Both coffees were smooth and creamy - I jotted down <em>notes of strawberries and cream</em> before I even saw <em>“Raspberry &amp; Vanilla Cream”</em> listed as flavor comps on the bag of coffee they used for my espresso.</p>
<p>–&gt;</p>
<p>This espresso struck me as <em><strong>perfectly balanced</strong></em> - without hesitation, I wanted to share it with every one of my friends still not won over by espresso.
In my experience, when people try espresso and decide they hate it, 95% of the time it’s due to the intensity of the flavors, especially with sour and bitter notes.
When I try an intense espresso, something that punches you in the mouth with sourness or fruit or bitterness or chocolate or nuttiness, I can enjoy the intensity (if it’s done well!) - but that seems to be the attribute of espresso that puts people off of it.</p>
<p>This espresso was not intense in the usual way, but not too mild either.
It was full of flavor with delicious notes of citrus, cream, a balance of bitterness to round out the flavor profile, yet not too intense to be someone’s first espresso.</p>
<p>This espresso, along with their drip brew, the knowledgable baristas, and a cozy cafe seating area nestled into NW Portland with indoor trees and fast wifi brings this to the top of my list.
The barista who served me happened to recommend <em>Barista</em> for one of my next espresso destinations which I haven’t had the opportunity to try yet - I look forward to posting a review given that the current champion of my list thinks I should give it a try.</p>
<p>As I work out an espresso tour of Portland, I <em>have</em> to pencil Sterling in as the final destination.
I would want any espresso afficionados visiting the city to finish their time here with a Sterling espresso.</p>
<p>–&gt;</p>
<h3>
    <center>
    <a href="https://www.sterling.coffee/" target="blank">
    Sterling Coffee homepage
    </a>
    </center>
</h3>
<!--
layout: post
title: Superjoy Coffee Lab & Roasters - 10/10
permalink: /superjoy
where: Portland, OR
cat: coffee
-->
<p>There’s too much to say. You should really just go try their espresso.</p>
<p>–&gt;</p>
<h5>
    Superjoy attempts to bring a blend of artful appreciation of coffee and chinese culture to the Portland coffee scene.
</h5>
<p>–&gt;</p>
<p>I tried not to eavesdrop, but while I drank my coffee and enjoyed the ambiance of the shop, I couldn’t help but overhear Christopher (who seems to be the primary roaster) training up a new employee.
I overheard him gently preaching the value of <em>consistency</em> in a barista; how to consistently and diligently clean your workspace in between every shot, how to measure all the observable input variables, how to deliver the same shot over and over again.</p>
<p>He <em>clearly</em> cares about his coffee and the presentation of his roasts, which he seems to put 100% effort into.</p>
<p>–&gt;</p>
<p>My espresso tasted perfectly balanced with a mild finish; on the darker side of fruity with notes of cherry and pomegranate and a bit of nuttiness.
I had their medium roast blend of natural-process Ethiopian and washed-process Guatamalan beans</p>
<p>Christopher was excited to tell me about ever aspect of his shop, from the new design and branding language he recently brought to the bags to the procurement of the different beans he roasts in-house.</p>
<p>While I listened to Chris passionately give me every detail about his shop, I noticed some <em>very</em> interesting specialty drinks on the seasonal menu: a chinese pepper mocha, a rose honey latte, and a maple oat latte.
This heavily reminded me of Never Coffee’s creative seasonal specialty lattes.</p>
<p>–&gt;</p>
<p>Chris also mentioned that they usually have some chinese beans in house, but happened to be out that day; I had never (to my knowledge) tried chinese-grown beans before, so I plan to visit next week when they (hopefully) have them back in stock.</p>
<!--
- perfectly balanced
- fruity, but a little on the darker side, like a cherry or pomegranate.

- they usually roast chinese beans, but they were out when I went by.
- maybe a cupping next week when their chinese beans are back in stock
- Blend of natural-process Ethiopian and washed-process Guatamalan beans
    - chocolate and nutty notes
    - medium roast

- Chinese pepper mocha, rose honey latte, maple oat latte
- Chinese single origin coffee of the typica variety
-->
<p>–&gt;</p>
<h3>
    <center>
    <a href="https://www.superjoycoffeelab.com" target="blank">
    Superjoy homepage
    </a>
    </center>
</h3>
<!--
layout: post
title: Beginner's Guide to Espresso
permalink: /beginners-guide-espresso
cat: coffee
-->
<p>Espresso should be fun! Don’t be too intimidated by all the lingo.</p>
<p>–&gt;</p>
<p>These are my espresso cliff notes, things I keep referring back to when I get confused about some aspect of espresso.</p>
<em>
NOTE: This is not about how to <strong>make</strong> espresso - just getting a feel for the lingo and culture, hopefully increasing your enjoyment of the drink and specialty shops that serve it.
</em>
<h2 id="what-is-espresso"><a class="header" href="#what-is-espresso">What is Espresso?</a></h2>
<p>https://home.lamarzoccousa.com/the-beginners-guide-to-espresso-drinks-2/</p>
<blockquote>
<p>Espresso is a coffee brewing method. It’s a way of making coffee where a small amount of near boiling water is forced through finely ground coffee, under pressure.</p>
</blockquote>
<blockquote>
<p>The space above the flat espresso puck fills with water and the espresso machine applies 9 bars of pressure to force water through the coffee.</p>
</blockquote>
<blockquote>
<p>The (General) Recipe: To pull a double shot, grind 18 grams of finely ground coffee in your portafilter. Aim for about 28-36 grams of espresso in about 25-30 seconds.</p>
</blockquote>
<h3 id="processes"><a class="header" href="#processes">Processes</a></h3>
<p><a href="https://perfectdailygrind.com/2016/07/washed-natural-honey-coffee-processing-101/">Perfectdailygrind article</a></p>
<blockquote>
<p><em><strong>Washed coffees</strong></em> focus solely on the bean. They let you taste you what’s on the inside, not the outside.</p>
</blockquote>
<blockquote>
<p>Washed coffees depend almost 100% on the bean having absorbed enough natural sugars and nutrients during its growing cycle. This means the varietal, soil, weather, ripeness, fermentation, washing, and drying are key.</p>
</blockquote>
<blockquote>
<p>This means that the washed process highlights the true character of a single origin bean like no other process. It’s why so many specialty coffees are washed.</p>
</blockquote>
<p>–&gt;</p>
<blockquote>
<p>The <em><strong>natural process</strong></em>, also known as the dry process, is a back-to-basics approach that stems from Ethiopia. The fruit remains on the bean, and dries undisturbed.
… This inconsistency is often the result of unripe fruit drying and turning brown alongside ripe fruits.
However, there are many who believe this process actually has the potential to create the most flavourful coffees.</p>
</blockquote>
<p>–&gt;</p>
<blockquote>
<p>When done right, <em><strong>honey processed</strong></em> coffee can literally taste like someone has put honey and brown sugar in your cup of coffee – although the name actually comes from how sticky the beans get during processing.
The honey process is strongly associated with Costa Rica and, in recent years, subcategories have developed: yellow, red, golden, black, and white honey.</p>
</blockquote>
<p><a href="https://coffeeaffection.com/what-is-honey-processed-coffee/">Coffee Affection article</a></p>
<blockquote>
<p>Honey processing leaves a sticky inner coating called mucilage on the coffee bean. This “honey” ferments, giving the bean a sweet, fruity flavor.</p>
</blockquote>
<h2 id="varietals"><a class="header" href="#varietals">Varietals</a></h2>
<p>Within each <em>species</em> of coffe bean (Arabica, Robusta, Liberica, and Excelsa), there are many, many <em>varieties</em>.</p>
<h3 id="species-arabica"><a class="header" href="#species-arabica">Species: Arabica</a></h3>
<p><a href="https://homecoffeeexpert.com/types-of-coffee-beans/">Home Coffee Expert article</a></p>
<blockquote>
<p>Arabica coffee is the most popular coffee bean in the world, accounting for around 64% of global coffee production.</p>
</blockquote>
<blockquote>
<p>As the Arabica coffee plant originated in the highland areas of Ethiopia it requires high altitudes and is quite sensitive to high temperatures. It can also only be grown in what is known as the “Coffee Belt” – the equatorial regions between the Tropic of Cancer and the Tropic of Capricorn.</p>
</blockquote>
<blockquote>
<p>High-quality Arabica coffee beans should be sweet with chocolate, caramel, fruit, nut, and floral notes.</p>
</blockquote>
<p>The <em>varieties</em> of arabica can be listed under four main <em>groups</em>, or <em>collections of varieties</em>:</p>
<ul>
<li>Ethiopian landrace
<ul>
<li>associated with incredibly high-quality beans and low yields.</li>
<li>huge number of “heirloom” varieties grown in Ethiopia</li>
</ul>
</li>
<li>Bourbon and Typica group
<ul>
<li>huge amount of Arabica coffee grown today descended from a small number taken from Yemen in the late 17th century. The path taken determines their designation as either Bourbon or Typica.</li>
</ul>
</li>
<li>Introgressed</li>
<li>F1 hybrid
<ul>
<li>Hybrids are created when two genetically different “parent” plants are bred to create a new cultivar. The “F1” refers to the fact these are the first generation offspring.</li>
<li>broken down into two categories: Introgressed and Not Introgressed. Introgressed F1 Hybrids have characteristics from two distinct species like Robusta and Arabica. Wheras Not Introgressed F1 Hybrids are made by crossing only varieties of Arabica coffee beans.</li>
</ul>
</li>
</ul>
<h2 id="references-4"><a class="header" href="#references-4">References</a></h2>
<ol>
<li><a href="https://varieties.worldcoffeeresearch.org/arabica/varieties">World Coffee Research</a></li>
<li><a href="https://brewlogy.com/beans/tabi/">Brewology</a></li>
<li><a href="https://acquiredcoffee.com/espresso-101-beginners-guide-espresso/">Acquired Coffee’s beginner’s guide</a></li>
<li><a href="https://perfectdailygrind.com/2021/04/aroma-body-flavour-finish-a-beginners-guide-to-tasting-espresso/">PerfectDailyGrind.com’s beginner’s guide to espresso tasting</a></li>
<li><a href="https://yescoffeeroasters.com/all-about-espresso/">YES Coffee Roasters beginner’s guide</a></li>
<li><a href="https://espresso-works.com/blogs/coffee-life/coffee-tasting">Espresso-works coffee tasting guide</a></li>
<li><a href="https://dailycoffeenews.com/2019/02/07/the-coffee-roasters-complete-guide-to-coffee-varieties-and-cultivars/">The Coffee Roaster’s Complete Guide to Coffee Varieties and Cultivars</a></li>
<li><a href="https://homecoffeeexpert.com/types-of-coffee-beans/">TYPES OF COFFEE BEANS: LIST OF VARIETALS</a></li>
</ol>
<!--
layout: post
title: Seattle Trip Report (8 Reviews)
permalink: /seattle-trip-report
cat: coffee
where: Seattle, WA
-->
<p>All the shops and roasters I tried while visiting my friend in Seattle!</p>
<p>–&gt;</p>
<h2 id="lets-get-the-ratings-out-of-the-way"><a class="header" href="#lets-get-the-ratings-out-of-the-way">Let’s Get the Ratings Out of the Way</a></h2>
<ol>
<li>Cafe Allegro: 8.5/10</li>
<li>Cafe Vita: 8/10</li>
<li>Cafe Ladro: 8.5/10</li>
<li>Zeitgeist: 6.5/10</li>
<li>Good Weather Bike &amp; Coffee: 7.5/10</li>
<li>Lighthouse Roasters: 7/10</li>
<li>Monorail Espresso: 3/10</li>
<li>Fonté: 2/10</li>
</ol>
<h2 id="overall-impressions"><a class="header" href="#overall-impressions">Overall Impressions</a></h2>
<p>I was struck by how much more prevalent darker roasts were in Seattle - it was my impression that most specialty espresso was in the lighter range of roasts to bring out more of the differences between varietals.
Almost every shop I visited served a pretty dark and heavy-bodied roast for their espresso, and few places changed their roasts out with the seasons.</p>
<p>I spoke with Collin from Cafe Ladro about the coffee culture in Seattle:</p>
<blockquote>
<p>We were really ahead of our time in the 90s, but most places haven’t been updated since then.
You would probably find fewer shops serving lighter (and better) roasts in a newer place like Portland.
Things are even worse in San Fran - they were killing the game 30 years ago, but if you go now it’s mostly the same stuff.</p>
</blockquote>
<p>That being said, there were still some <em>great</em> spots roasting beans in-house.
<em><strong>Cafe Ladro</strong></em> (where I met Collin) and <em><strong>Cafe Vita</strong></em> were wonderful.
<em><strong>Cafe Allegro</strong></em> was a hidden gem I would <em>absolutely</em> swing by again, especially given how close it is to the 1 metro line; I had two coffees with an old friend there before hopping on the 1 to catch my Amtrack back to Portland, but I could have stayed much longer.
<em><strong>The Good Weather Bike &amp; Coffee</strong></em> cafe-and-bike-repair-shop-combo served a really nice Herkimer espresso blend, who roasts their beans a few neighborhoods away.</p>
<p>Collin mentioned Superjoy Roasters in Portland, and though they are on my list I have not stopped by there yet.</p>
<!---
- milstead in freemont
- were ahead of the curve in the 90s haven't really updated 
- san fran is worse than seattle
- collin 
- superjoy
- kurasu roasters from japan
-->
<h2 id="cafe-allegro-8510"><a class="header" href="#cafe-allegro-8510">Cafe Allegro: 8.5/10</a></h2>
<p>Hidden in an alleyway in between a used bookstore and a pizza joint, Cafe Allegro had the most enjoyable espresso of any of the cafes I visited in Seattle.
The barista had a t-shirt reading <em>Where the hell is Allegro?</em> paying homage to the cafe’s obscure location.</p>
<p>I felt a sense of relief when I took my first sip of Allegro’s espresso and felt familiar waves of fruit wash over me.
For most of my coffee-drinking life I considered myself more of a dark-roast kinda guy, but since getting more into espresso and going back to darker roasts, I feel like my taste has developed to like well-done lighter roasts far more.</p>
<p>I got notes of berries (strawberries?) and dried fruit.
The listed flavor comps were stone fruit (?) and chocolate.
I didn’t get much of either of those, but I still loved it (and went back for a second).</p>
<p>The barista mentioned a cafe in northern San Fransisco named after the Italian city <em>Trieste</em> after which Cafe Allegro was designed; maybe if my espresso journey takes me that far I’ll give it a try!
Definitely stop by if you have some time near the 1 line in the University District.</p>
<h2 id="cafe-vita-810"><a class="header" href="#cafe-vita-810">Cafe Vita: 8/10</a></h2>
<p>I had the <em>Cafe del Sol</em>, a house-roasted medium blend.
It was pretty good - the first &gt;=8 score of this Seattle trip.
I experienced medium body, balanced flavor (not nearly as dark as my previous shots) on the nutty side of sweet.
The flavor comps on the bag read <em>milk chocolate, caramel, dark cherry</em>; I mostly got the caramel.</p>
<p>Nothing mind-blowing, but I would have it again.</p>
<h2 id="cafe-ladro-8510"><a class="header" href="#cafe-ladro-8510">Cafe Ladro: 8.5/10</a></h2>
<p>This was the first specialty espresso place I got to visit.
I had the Ladro house medium blend with milk chocolate, dark cherry and caramel flavor comps, but the strongest flavors I got were nuttiness/hazelnut and vanilla.
I found a really mild finish, I wasn’t punched in the nose by bitterness or dryness like many of the other shops.</p>
<p>This was a really nice espresso! If you’re in the West Queen Anne area you might want to stop by.</p>
<h2 id="zeitgeist-6510"><a class="header" href="#zeitgeist-6510">Zeitgeist: 6.5/10</a></h2>
<p>This smaller German coffeehouse with aged brick walls was a nice environment and okayish coffee, but wasn’t overly impressive to me.
Unfortunately, they didn’t seem all that busy either - I could see myself coming back if I needed a place to work or if I knew there would be a more busy or upbeat energy, but this was not my favorite place.
I had the Jackson Street Blues roast from Fulcrum Coffee Roasters.</p>
<h2 id="good-weather-bike--coffee-7510"><a class="header" href="#good-weather-bike--coffee-7510">Good Weather Bike &amp; Coffee: 7.5/10</a></h2>
<p>Good Weather was interesting - the cafe environment was serene, tucked into an alleyway in Capitol Hill near a secondhand sportswear shop.
I was excited to see them serving Herkimer’s espresso blend; I wanted to try these beans from a Herkimer cafe front, but I didn’t get the chance.</p>
<p>I got a lighter body, almost like a black tea and a <em>much</em> lighter roast than some of the other (read: worse) shots I tried this weekend.
I tasted soem chamomile and dried fruits, maybe raisins/craisins.</p>
<h2 id="lighthouse-roasters-710"><a class="header" href="#lighthouse-roasters-710">Lighthouse Roasters: 7/10</a></h2>
<p>This neighborhood roaster was <em>filled</em> with dogs, and the cafe bar shelved probably a dozen kinds of dog treats.
The roast was too dark for specialty espresso, but I would absolutely have an americano or latte of their house roast.
I get the sense the neighborhood patrons stop by for a coffee and grab a bag of their house roast for their drip machine at home.
An extremely small and cozy shop with maybe 4 small tables right next to the roasters and at least half a dozen bags of green coffee beansready to be processed.</p>
<p>If I lived next to this cafe, I’d probably be drinking far more americano for how enjoyable their cafe environment was.</p>
<h2 id="monorail-espresso-310"><a class="header" href="#monorail-espresso-310">Monorail Espresso: 3/10</a></h2>
<p>The cute window-only cafe front was unfortunately not very good…
Very dry, very dark, HEAVY body.
The smells from the cafe and from my espresso were wonderful (I still <em>love</em> a dark-roast) but my espresso was not very good.
My friend’s vanilla latte was great though!</p>
<h2 id="fonté-210"><a class="header" href="#fonté-210">Fonté: 2/10</a></h2>
<p>Fonté was unfortunately the first espresso in this entire journey that I could not even finish.
Extremely dark and heavy-bodied, my mouth puckered up with bitterness with both of the sips I could get down.
Very unfortunate.</p>
<p>–&gt;</p>
<p>Thanks to my friend Emma that stuck with me and my annoying coffee flavor charts and endless cafes :)</p>
<!--
where: Portland, OR
layout: post
title: Adapt Cafe - 9.5/10
permalink: /adapt-cafe
cat: coffee
-->
<p>Pleasant, mild, unique, imported roasts from Sweden - try some beans you’ll likely not find anywhere else!</p>
<p>–&gt;</p>
<p>The first thing I look for when I walk into a new cafe in the Portland area is the coffee bags the shop is selling:</p>
<blockquote>
<p>Are they using Coava or Proud Mary beans? Do they roast their own? Am I trying something new today, or maybe an old favorite?</p>
</blockquote>
<p>Walking into Adapt Cafe in SE, I was surprised to see bags I’d not seen before - light and modern packaging around small, 250 gram bags from <em>Morgon Coffee Roasters</em> from Gothenburg, Sweden.</p>
<p>The swedish beans bled out into the design language of the shop - smallish seating area with huge windows and modern-feeling all-black furnishing against white walls.
The barista was happy to tell me about the imported roasts from the <em>very</em> young 5-year-old roasting company they buy from, and the rapid pace at which they have been winning awards:</p>
<blockquote>
<p>Our combined resume contains… competing in most things coffee.
Some of those competitions resulted in a <em><strong>bronze in roasting, two silvers in brewing and two gold medals in coffee tasting</strong></em>.</p>
<p>In 2020 we had our first prize spotlight for Morgon when we were <em><strong>finalists in the 12th annual international Sprudgie’s award as “Most notable roaster in 2020”</strong></em>.</p>
<ul>
<li>Morgon Coffee Roasters’ “About Us” page, see links below</li>
</ul>
</blockquote>
<p>–&gt;</p>
<p>I found the espresso to be very pleasant, on the mild side - the primary flavor comp I experienced was <em>plum</em>; a touch of sweetness and sourness, but not really any citrus <em>per se</em>, and certainly not much bitterness.
After the first sip, I couldn’t tell if it was more sour or bitter.
It wasn’t until the second and third sip that I made up my mind; that’s how subtle it was.
The other flavor comps were structured, chocolate, and bisuit, but I could only really pick up notes of plum.</p>
<p>I had their honey process Costa Rican beans of the <a href="https://varieties.worldcoffeeresearch.org/varieties/catuai">Catuai variety of Arabica</a> from the Montero family - at first I thought <em>Montero</em> might have been referring to a coffee <em>variety</em> I’d not yet heard of, but no: these beans were <em>grown</em> by the Montero family, and <a href="https://www.morgoncoffeeroasters.com/products/carlos-montero-costa-rica-tarrazu">Morgon’s website has a whole page about their relationship with the Montero family</a>.
Carlos Montero even visited Morgon in Gothenburg to see their roasting operation.</p>
<p>For being a dark-roast kinda guy, I <em>really</em> enjoyed their espresso.
It’s obvious they care about coffee, their baristas are knoledgable, and they get their beans from a <em>serious</em> roasting company.
I would love to see more Morgon beans around town, maybe even in a second Adapt Cafe location.
I think they would get a lot of business in a more central location, though I do enjoy how many local roasters get the spotlight in cafes downtown.</p>
<p>–&gt;</p>
<h3>
    <center>
        <a href="https://www.instagram.com/adaptcafe/" target="blank">
        Adapt Cafe insta
        </a>
        <hr>
        <a href="https://www.morgoncoffeeroasters.com/pages/about-us" target="blank">
        Morgon Coffee Roasters homepage
        </a>
    </center>
</h3>
<!--
where: Portland, OR
layout: post
title: Coava - 10/10
permalink: /coava
cat: coffee
-->
<p>They say you never forget your first love…</p>
<p>–&gt;</p>
<p><em><strong>…and Coava was mine.</strong></em></p>
<p>They are the first Portland-local roaster and cafe I fell in love with.</p>
<p>When I first moved to Portland, there was a Coava cafe in the same building as my apartment, on the lobby floor.
Nearly all 5 work-days, nearly every week since I moved here, I worked from that cafe in the upstairs seating area surrounded by Monstera Deliciosas, PSU students, and speakers playing indi tracks I’d never heard before.</p>
<p>I’ve consumed more Coava espresso than any other form of espresso in my life by a <em><strong>long shot</strong></em>.
It’s no surprise that they earn a perfect 10/10; fruity but not too blond, bitter but not too dark.</p>
<p>I’ll forever mourn their west-side location, but their headquarters shop on SE Main and Grand right off the 6 bus line is a much nicer cafe anyways.
The seating area is home to beautiful woodworking projects from <em>Bamboo Revolution</em> with whom Coava shares a space, along with antique roasting equipment and open-air garage doors and fans (at least in the summer time).</p>
<p>For this review I had a doppio of the <em>Java Papatong</em>, a washed-process from Indonesia, but I’ve loved many of their other roasts:</p>
<ul>
<li>the San Marcos,</li>
<li>the SO Blend from the Americas and East Africa,</li>
<li>the Darfusu washed process,</li>
<li>the Fazenda Serra do Boné from Brazil,</li>
</ul>
<p>and plenty of others. Their roasts are all seasonal, so try whatever they have.</p>
<p>–&gt;</p>
<center>
    <h3>
       <a href="https://coavacoffee.com" target="blank">Coava Homepage</a> 
    </h3>
</center>
<!--
layout: post
title: Portland Espresso Research Master List
permalink: /pdx-espresso-research
cat: coffee
-->
<p>All the Portland espresso recomendations I could find online, and the shops that come up again and again.</p>
<p>–&gt;</p>
<h1 id="my-takeaways"><a class="header" href="#my-takeaways">My Takeaways</a></h1>
<p>The most common names in these blogs are probably:</p>
<ol>
<li>Coava</li>
<li>Proud Mary</li>
<li>Case Study</li>
<li>Heart</li>
<li>Stumptown</li>
</ol>
<p>If you visit any given cafe in Portland, chances are high their beans come from either Heart, Coava or Proud Mary, and for good reason.
All three of them are local and put out some <em>solid</em> roasts… but I still recommend finding some smaller shops that still roast in-house.</p>
<p>In my opinion, the most <em>underrated</em> cafes and roasters are:</p>
<ol>
<li>Sterling</li>
<li>Deadstock</li>
<li>Courier</li>
</ol>
<p>and the most <em>overrated</em> are:</p>
<ol>
<li>Stumptown</li>
<li>Case Study</li>
<li>Never Coffee (unless you really like lattes)</li>
<li>PUSH X PULL</li>
</ol>
<h1 id="lists-from-other-blogs"><a class="header" href="#lists-from-other-blogs">Lists From Other Blogs</a></h1>
<p><em>As of Jun 15 2023</em></p>
<h2 id="pdx-monthly"><a class="header" href="#pdx-monthly"><a href="https://www.pdxmonthly.com/eat-and-drink/best-coffee-shops-cafes-portland">PDX Monthly</a></a></h2>
<p>Perhaps the most solid collection of Portland roasters and cafes, I found several shops I’d never heard of while it doesn’t miss the usual suspects.</p>
<ol>
<li>Abba</li>
<li>Albina Press</li>
<li>Carnelian Coffee</li>
<li>Case Study</li>
<li>Cathedral Coffee</li>
<li>Courier Coffee</li>
<li>Deadstock</li>
<li>Either/Or</li>
<li>Electrica</li>
<li>The Fresh Pot</li>
<li>Futura Coffee Roasters</li>
<li>Good Coffee</li>
<li>Guilder</li>
<li>Heart</li>
<li>In J/Super Joy</li>
<li>J Vein Caffe</li>
<li>Kalesa</li>
<li>Keeper Coffee</li>
<li>Less and More</li>
<li>Never Coffee</li>
<li>Portland Cà Phê</li>
<li>Prince Coffee</li>
<li>PUSH X PULL</li>
<li>Roseline</li>
<li>Soro Soro</li>
<li>Sterling Coffee</li>
<li>Tōv Coffee</li>
<li>Upper Left Roasters</li>
</ol>
<h2 id="oregon-obsessed"><a class="header" href="#oregon-obsessed"><a href="https://oregonobsessed.com/best-coffee-shops-in-portland/">Oregon Obsessed</a></a></h2>
<p>I’ve tried (and liked) almost all of <em>Oregon Obsessed</em>’s recomendations, though a few are overrated in my opinion:</p>
<ol>
<li>Coava</li>
<li>Good Coffee</li>
<li>Stumptown</li>
<li>Case Study</li>
<li>Nossa Familia</li>
<li>Proud Mary</li>
<li>Deadstock</li>
<li>Never Coffee</li>
<li>Ovation</li>
<li>Portland Coffee Roasters</li>
</ol>
<h2 id="daily-hive"><a class="header" href="#daily-hive"><a href="https://dailyhive.com/portland/best-coffee-shops-portland">Daily Hive</a></a></h2>
<p>Good to see <em>Coava</em> consistently topping these lists - hadn’t heard of Pájaro either.</p>
<ol>
<li>Coava</li>
<li>Pájaro</li>
<li>PUSH X PULL</li>
<li>Heart Coffee</li>
<li>Stumptown</li>
<li>Upper Left</li>
<li>Deadstock</li>
<li>Good Coffee</li>
</ol>
<h2 id="coffee-affection"><a class="header" href="#coffee-affection"><a href="https://coffeeaffection.com/best-coffee-shops-in-portland/">Coffee Affection</a></a></h2>
<p>Pretty much just hits the usual suspects.
Not much is new here.</p>
<ol>
<li>Coava</li>
<li>Good Coffee</li>
<li>Stumptown</li>
<li>Case Study</li>
<li>Nossa Familia</li>
<li>Proud Mary</li>
<li>Deadstock</li>
<li>Never Coffee</li>
<li>Ovation</li>
<li>Portland Coffee Roasters</li>
</ol>
<h2 id="foursquare"><a class="header" href="#foursquare"><a href="https://foursquare.com/top-places/portland/best-places-espresso">FourSquare</a></a></h2>
<ol>
<li>Barista</li>
<li>Coava</li>
<li>Stumptown</li>
<li>Case Study</li>
<li>Heart</li>
<li>Courier</li>
<li>Spella</li>
<li>Sterling</li>
<li>Water Avenue</li>
<li>Good Coffee</li>
<li>Common Grounds</li>
<li>Nossa Familia</li>
</ol>
<h2 id="hopculture"><a class="header" href="#hopculture"><a href="https://www.hopculture.com/best-coffee-portland-oregon/">HopCulture</a></a></h2>
<p>I hadn’t seen Kiosko before:</p>
<ol>
<li>Proud Mary</li>
<li>Kiosko</li>
<li>Heart</li>
<li>Stumptown</li>
<li>Never Coffee Lab</li>
<li>Good Coffee</li>
<li>Coava</li>
</ol>
<h2 id="boam"><a class="header" href="#boam"><a href="https://boam.com/best-espresso-shops-in-portland-or/">Boam</a></a></h2>
<p><em>Fairlane</em> is the only name on this list new to me.
Pretty solid list.</p>
<ol>
<li>Good Coffee</li>
<li>Nossa Familia</li>
<li>PUSH X PULL</li>
<li>Sunny Day Coffee</li>
<li>Ole Latte Coffee</li>
<li>Ovation Coffee &amp; Tea</li>
<li>Fairlane Coffee</li>
<li>Sisters Coffee Company in The Pearl</li>
<li>Coava</li>
<li>Grendel’s Coffee House</li>
</ol>
<h2 id="respresso-subreddit"><a class="header" href="#respresso-subreddit"><a href="https://www.reddit.com/r/espresso/comments/12jbj4m/best_espresso_in_portland_or/">R/Espresso Subreddit</a></a></h2>
<p>As of the time of writing, many subreddits are protesting a recent change to reddit’s 3rd party API access rules, so I can’t view the listright now.</p>
<h2 id="bean-box"><a class="header" href="#bean-box"><a href="https://beanbox.com/blog/best-coffee-in-portland">Bean Box</a></a></h2>
<ol>
<li>Roseline Coffee</li>
<li>Water Avenue Coffee Company</li>
<li>Coava</li>
<li>Good Coffee</li>
<li>Ovation</li>
<li>Proud Mary</li>
<li>Prince</li>
<li>Either/Or</li>
<li>Heart Cofffee Roasters</li>
<li>Nossa Familia Coffee</li>
<li>Stumptown</li>
</ol>
<h2 id="trip-advisor"><a class="header" href="#trip-advisor"><a href="https://www.tripadvisor.com/Restaurants-g52024-c8-Portland_Oregon.html">Trip Advisor</a></a></h2>
<p>This list is focused on <em>cafes</em> and not really <em>coffee</em>… there were some names on this list I hadn’t seen before, so I’ll keep it around… but I wouldn’t put much weight on these names (hence why it comes last in my research page).</p>
<ol>
<li>Chery’s on 12th</li>
<li>St Honore</li>
<li>Tin Shed Garden Cafe</li>
<li>Jam on Hawthorne</li>
<li>Island Cafe</li>
<li>Ken’s Artisan Bakery</li>
<li>Stumptown</li>
<li>The Waffle Window</li>
<li>Case Study</li>
<li>Cadillac Cafe</li>
<li>Milo’s City Cafe</li>
<li>Broder</li>
<li>Lovejoy</li>
<li>Gravy</li>
<li>Dragonfly Coffee House</li>
<li>Zell’s</li>
<li>Bleu Door Bakery</li>
<li>Petunia’s Pies &amp; Pastries</li>
<li>Cameo Cafe</li>
<li>Public Domain Coffee</li>
<li>Stepping Stone Cafe</li>
<li>The Daily Feast PDX</li>
<li>Barista</li>
<li>Dulin’s Cafe</li>
<li>Heart Coffee</li>
<li>Babica Hen Cafe</li>
<li>Prasad</li>
</ol>
<!--
where: Portland, OR
layout: post
title: Nossa Familia Coffee - 9/10
permalink: /nossa
cat: coffee
-->
<p>Delicious, well-balanced, predominantly Brazilian roasts and espresso.</p>
<p>–&gt;</p>
<p>I’ve had espresso from Nossa before at their northern location in the Pearl where I had one of their in-house roasts, but when I stopped by for this review they had a roast from Keia &amp; Martyn’s Coffee roasters (also based out of Portland).
This black-owned roaster was featured in the SE Nossa cafe I visited for the month of June (I’ll just have to revisit when they have their own roasts on espresso😉).</p>
<p>The Colombian Yeny Capiz roast from Keia &amp; Martyn’s <em>radical</em> line had flavor comps of molasses, sweet red tea, citrus, and candied hazelnut, though I didn’t get much of the citrus.
My coffee tasted balanced, nutty, and a bit sweet; the molasses and candied hazelnut came through pretty clearly to me.</p>
<p>Augusto Carneiro started Nossa to bring his family’s coffee to Portland from Brazil, and the shop continues to pride itself on the farmer-roaster relationships they have with all of their growers, which now come from Guatemala, Nicaragua, Peru, Ethiopia as well.</p>
<p>My coffee was solid: well-rounded, tasty, not overpowering.
It didn’t show up Courier or Sterling, but it was also clearly in the top echelon of coffees I’ve had in the city.</p>
<p>–&gt;</p>
<h3>
    <center>
        <a href="https://www.nossacoffee.com/" target="blank">
        Nossa Familia Coffee homepage
        </a>
        <hr>
        <a href="https://keiaandmartynscoffee.com/" target="blank">
        Keia & Martyn's Coffee Roasters homepage
        </a>
    </center>
</h3>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
